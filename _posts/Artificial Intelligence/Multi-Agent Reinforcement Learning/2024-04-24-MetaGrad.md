---
title: MetaGrad in LIO
date: 2024-04-24 14:40:00 +0800
categories: [Artificial Intelligence, Multi-Agent Reinforcement Learning]
tags: [Tech, AI, Multi Agents, RL, Framework]
math: True
---

> Yang, Jiachen, et al. "Adaptive incentive design with multi-agent meta-gradient reinforcement learning." arXiv preprint arXiv:2112.10859 (2021).

Jiachen学长的LIO的后续，我们叫LIO2，或者是Adaptive LIO，他们这个叫incentivization design，也缩写叫id

LIO和这篇都用了online cross validation的更新思路，区别是LIO在里面把上层incentivize designer的梯度的每一项都计算出来，这篇是用了PPO的方法（Appendix A.1. 应该就是直接替换参数的思路），作了更新上的改进，感觉从 LIO 改到 adaptive LIO 像是从 REINFORCE 改到 PPO

但是这篇里更新的那个PPO的probability ratio项里仍然需要求hypergradient，如果换到“发信号”的场景依然存在之前讨论的那个问题

不过还有一个重要的点我认为是，这篇的方法相比LIO更general。
LIO中的梯度（也就是这篇中的4式），是只考虑了二阶的影响的（也就是eta如何影响theta的更新），因为“给奖励”的奖励只在别人更新的时候用到，在生成轨迹样本的时候没有影响。但LIO没考虑一阶的影响，这个在发信息的情况中很重要，因为发信息eta一变，那么下层agent的策略的环境就变了，因为发信息的信息的分布变了。因此在发信息的环境中（包括reputation也是一种发信息），只像LIO那样列出4式是不够的

而这篇式子5的formulaiton够general（没有继续往下具体写），是可以包括了考虑上层优化中对下层策略的一阶影响的