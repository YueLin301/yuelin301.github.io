---
title: Recurrent Neural Network (RNN)
date: 2023-06-15 14:40:00 +0800
categories: [Machine Learning Basics]
tags: [index]
math: True
---

## Index of an Awesome Website
> Check this awesome website: [DIVE INTO DEEP LEARNING](https://d2l.ai/chapter_preface/index.html)
{: .prompt-info }

- [RNN = Recurrent Neural Network](https://d2l.ai/chapter_recurrent-neural-networks/rnn.html)
  - [BPTT = Backpropagation Through Time](https://d2l.ai/chapter_recurrent-neural-networks/bptt.html)
- [Modern Recurrent Neural Networks (Index)](https://d2l.ai/chapter_recurrent-modern/index.html)
  - [LSTM = Long Short-Term Memory](https://d2l.ai/chapter_recurrent-modern/lstm.html)
  - [GRU = Gated Recurrent Units](https://d2l.ai/chapter_recurrent-modern/gru.html)
  - [Encoder-Decoder](https://d2l.ai/chapter_recurrent-modern/encoder-decoder.html)
  - [Seq2Seq](https://d2l.ai/chapter_recurrent-modern/seq2seq.html)
- [Attention and Transformers](https://d2l.ai/chapter_attention-mechanisms-and-transformers/index.html)


## RNN

$$
\begin{aligned}
    \begin{cases}
      h_t = f(x_t, h_{t-1}, w_h) \\
      o_t = g(h_t, w_o) \\
    \end{cases}
\end{aligned}
$$

```python
import torch
rnn = torch.nn.RNN(10, 20, 2)   # input_size, hidden_size, num_layers
input = torch.randn(5, 3, 10)   # sequence_length, batch_size
h0 = torch.randn(2, 3, 20)      # num_layers, batch_size, hidden_size
output, hn = rnn(input, h0)
print(output.size(), hn.size())

# torch.Size([5, 3, 20]) torch.Size([2, 3, 20])
# output size: sequence_length, batch_size, hidden_size
# hidden size: num_layers, batch_size, hidden_size
```

## BPTT
$$
L = \frac{1}{T}\sum\limits_{t} l(o_t,y_t)
$$  
$$
\begin{aligned}
    \frac{\partial L}{\partial w_o} =& \frac{1}{T}\sum\limits_{t} \frac{\partial l(y_t,o_t)}{\partial w_o} \\
    =& \frac{1}{T}\sum\limits_{t}
    \frac{\partial l(y_t,o_t)}{\partial o_t}\cdot
     \frac{\partial o_t}{\partial w_o}
\end{aligned}
$$  
$$
\begin{aligned}
    \frac{\partial L}{\partial w_h} =& \frac{1}{T}\sum\limits_{t} \frac{\partial l(y_t,o_t)}{\partial w_h} \\
    =& \frac{1}{T}\sum\limits_{t} \frac{\partial l(y_t,o_t)}{\partial o_t} \cdot
     \frac{\partial o_t}{\partial h_t} \cdot
      \textcolor{red}{\frac{\partial h_t}{\partial w_h}}
\end{aligned}
$$  

$$
\begin{aligned}
    \textcolor{red}{\frac{\partial h_t}{\partial w_h}} =&
    \frac{\partial f(x_t, h_{t-1}, w_h)}{\partial w_h} + \frac{\partial f(x_t, h_{t-1}, w_h)}{\partial h_{t-1}} \cdot 
    \textcolor{red}{\frac{\partial h_{t-1}}{\partial w_h}}
\end{aligned}
$$

$$
\begin{cases}
      z_0 = a_0 = 0 \\
      z_k = a_k + b_k \cdot z_{k-1} \\
\end{cases}
$$

$$
\begin{aligned}
    z_k = a_k + \sum\limits_{i=0}^{k-1}
        \left(\prod\limits_{j=i+1}^k b_j \right) \cdot a_i
\end{aligned}
$$

## LSTM
- To learn long-term dependencies (owing to vanishing and exploding gradients).
- Each ordinary recurrent node is replaced by a memory cell.

![LSTM](https://d2l.ai/_images/lstm-3.svg){: width="300" height="300" }
_An illustration of LSTM (from the book "DIVE INTO DEEL LEARNING".)_

## GRU