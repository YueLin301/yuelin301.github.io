[
  
  {
    "title": "Tools of Visual Studio Code",
    "url": "/posts/VSCode/",
    "categories": "Efficiency, Tools",
    "tags": "tech, efficiency, tools, markdown",
    "date": "2023-12-24 18:40:00 +0000",
    





    
    "snippet": "  This note will be consistently updated.ShortcutsCommand + k Command + s: Keyboard Shortcuts  Command + p: Go to file  Command + Shift + o: Go to symbol in editor  Command + t: Go to symbol in wor...",
    "content": "  This note will be consistently updated.ShortcutsCommand + k Command + s: Keyboard Shortcuts  Command + p: Go to file  Command + Shift + o: Go to symbol in editor  Command + t: Go to symbol in workspace  Command + Shift + f: Search in workspace  Command + Shift + p: input command  Command + Shift + n: New window  Command + o: Open  Command + ,: Settings  Control + `: Terminal  F12: Go to definition  Shift + F12: Go to references  Control + -: Go back  Control + Shift + -: Go forward  Control + w: Switch windowRemote ConnectionRemote - SSH这玩意可以直接用vs code打开服务器上的文件夹。Remote - SSH 扩展使得开发者可以直接通过 SSH 协议连接到远程服务器。这意味着您可以在本地的 VS Code 中编辑远程服务器上的文件，运行代码，甚至调试程序。这对于那些需要处理大量数据、运行大型应用或者需要特定服务器环境的开发者特别有用。您只需在本地计算机上安装 VS Code 和必要的扩展，服务器端则无需安装 VS Code。  打开远程窗口：点击 VS Code 左下角的绿色按钮（远程窗口指示器），或按 Ctrl+Shift+P 打开命令面板，然后输入Remote-SSH: Add new ssh Host。  添加 SSH 主机：输入您的 SSH 连接命令，比如 ssh username@hostname。如果之前已添加过，可以直接从列表中选择。  选择配置文件：如果是首次添加，系统会询问您保存配置的位置，通常保存在用户目录的 .ssh/config 文件中。如果我想用我的电脑连接远程服务器，我应该选择的是本机的ssh configuration file.  连接和输入凭据：选择您的 SSH 主机后，VS Code 会尝试建立连接。根据配置，您可能需要输入密码或使用 SSH 密钥认证。  打开项目：连接成功后，您可以通过 “File -&gt; Open Folder” 打开远程服务器上的项目文件夹。SFTP“SFTP” 是 Visual Studio Code (VS Code) 的一个流行插件，它允许用户通过 SFTP (Secure File Transfer Protocol) 或 FTP (File Transfer Protocol) 来上传、下载和同步文件。这个插件特别适合在本地和远程服务器之间同步文件，对于需要频繁更新服务器上文件的开发者来说非常有用。  下载和上传，文件同步  下载下来改，改完上传，然后在远程运行Usage      In VS Code, open a local directory you wish to sync to the remote server (or create an empty directorythat you wish to first download the contents of a remote server folder in order to edit locally).    Ctrl+Shift+P on Windows/Linux or Cmd+Shift+P on Mac open command palette, run SFTP: config command.    A basic configuration file will appear named sftp.json under the .vscode directory, open and edit the configuration parameters with your remote server information.    For instance:  {    \"name\": \"Profile Name\",    \"host\": \"name_of_remote_host\",    \"protocol\": \"ftp\",    \"port\": 21,    \"secure\": true,    \"username\": \"username\",    \"remotePath\": \"/public_html/project\",    \"password\": \"password\",    \"uploadOnSave\": false}    The password parameter in sftp.json is optional, if left out you will be prompted for a password on sync.Note： backslashes and other special characters must be escaped with a backslash.      Save and close the sftp.json file.    Ctrl+Shift+P on Windows/Linux or Cmd+Shift+P on Mac open command palette.    Type sftp and you’ll now see a number of other commands. You can also access many of the commands from the project’s file explorer context menus.    A good one to start with if you want to sync with a remote folder is SFTP: Download Project.  This will download the directory shown in the remotePath setting in sftp.json to your local open directory.    Done - you can now edit locally and after each save it will upload to sync your remote file with the local copy.    Enjoy!    For detailed explanations please go to wiki.ExtensionsDendron Paste ImageCommand + Option + V: put the copied image in a new created file (named the same as the current file) and paste it here.Markdown All in OneRun command “Create Table of Contents”: Create Table of Contents.Outline MapYou can find it in the sidebar.TODO TreeYou can find it in the sidebar.Local HistoryYou can find it in the file explorer.vscode-iconsSkin.MacOS =&gt; Code &gt; Preferences &gt; File Icon Theme &gt; VSCode Icons.Rainbow BracketsPassive skill.Error LensPassive skill.Better AlignChoose texts and Option + APath IntellisensePassive skill.IntelliCodePassive skill.Code Spell CheckerPassive skill.One Dark ProSkin.Power ModeSkin.Passive skill.Show some special effects while you type."
  },
  
  {
    "title": "Interesting Facts",
    "url": "/posts/Interesting-Facts/",
    "categories": "Misc Notes",
    "tags": "life, misc note, writing buffer",
    "date": "2023-12-06 18:40:00 +0000",
    





    
    "snippet": "  This note will be consistently updated.Society  The Rules for Rulers | CGP Grey (YouTube)Animals  Octopus vs Underwater Maze | Mark Rober (YouTube) or 【Mark Rober 实验【中文配音】 | 章鱼到底有多聪明？(BiliBili)】I...",
    "content": "  This note will be consistently updated.Society  The Rules for Rulers | CGP Grey (YouTube)Animals  Octopus vs Underwater Maze | Mark Rober (YouTube) or 【Mark Rober 实验【中文配音】 | 章鱼到底有多聪明？(BiliBili)】Information Technology  Why can’t robots check the box that says ‘I’m not a robot’? | WTFAQ | ABC TV + iview (YouTube)Psychology  INTJ vs INFJ"
  },
  
  {
    "title": "Information Design in Multi-Agent Reinforcement Learning",
    "url": "/posts/IDMARL/",
    "categories": "Artificial Intelligence, Multi-Agent Reinforcement Learning",
    "tags": "tech, information design, sequential social dilemma, multi agents, reinforcement learning, my paper",
    "date": "2023-11-23 18:40:00 +0000",
    





    
    "snippet": "Key Points  Mixed-motive between two agents. Some other MARL papers use the term “mixed-motive” to describe scenarios involving two groups of agents, where agents within the same group cooperate bu...",
    "content": "Key Points  Mixed-motive between two agents. Some other MARL papers use the term “mixed-motive” to describe scenarios involving two groups of agents, where agents within the same group cooperate but have conflicts with agents from the other group.  Non-stationarity.          Signals should not be viewed as actions.      Signals affects the others’ “environment”.        Signaling Gradient is not only suitable for mixed-motive communication, but is also suitable for fully-cooperative communication.  Information design is far more difficult than incentive design.          Signal can be ignored. Incentive is compulsory. In the training beginning, the signaling scheme is almost random, the receiver will easily learn to ignore the messages, and this case is a strong equilibrium.      Signals immediately changes transitions (it affects both the sampling phase and the update phase of RL). Reward does not affect trajectory (it only affects the update phase of RL). So the hyper-gradient method used in LIO is not applicable here (the first-order gradient from the sampling phase is dominant).      The sender cannot take environmental actions. It only can get feedback from the receiver’s actions.        The sender and the receiver are rational (in a sense of RL), self-interested, and risk-neutral.  The receiver’s policy is Markovian, not history-dependent. At every timestep, it takes actions based on the current estimation of the future payoffs. So the obedience constraint can be easily extended in MSGs.  In the learning scenario, we can cancel the commitment assumption and the analysis of the revelation principle.Interesting Experimental Results  The DIAL sender does not concern about itself at all.  Symmetricity. Emergent Languages.  Honesty of the sender can be manipuated by the hyperparameters of the Lagrangian method, as shown in the heatmap in Appendix H.6.  The more the receiver can see, the less the informational adavantage the sender has, the less the sender can manipulate, as shown in the Appendix H.7.  The following part has not been finished yet."
  },
  
  {
    "title": "Policy Distillation",
    "url": "/posts/Policy-Distillation/",
    "categories": "Artificial Intelligence, Reinforcement Learning",
    "tags": "tech, policy distillation",
    "date": "2023-11-15 18:40:00 +0000",
    





    
    "snippet": "Introduction[Paper]: Policy DistillationThe following statements from the paper are key to understand this technique:  Distillation is a method to transfer knowledge from a teacher model $T$ to a s...",
    "content": "Introduction[Paper]: Policy DistillationThe following statements from the paper are key to understand this technique:  Distillation is a method to transfer knowledge from a teacher model $T$ to a student model $S$.  Goals:          It is “used to extract the policy of a reinforcement learning agent and train a new network that performs at the expert level while being dramatically smaller and more efficient.”      It is “used to consolidate multiple task-specific policies into a single policy.”        The following part has not been finished yet.Single-Game Policy DistillationMulti-Task Policy Distillation"
  },
  
  {
    "title": "Rational Emotive Behavior Therapy (REBT)",
    "url": "/posts/Rational-Emotive-Behavior-Therapy/",
    "categories": "Efficiency, Psychology",
    "tags": "life, efficiency, psychology, personality",
    "date": "2023-11-13 18:40:00 +0000",
    





    
    "snippet": "Introduction  Rational Emotive Behavior Therapy is a form of psychotherapy that helps you identify self-defeating thoughts and feelings, challenge the nature of irrational and unproductive feelings...",
    "content": "Introduction  Rational Emotive Behavior Therapy is a form of psychotherapy that helps you identify self-defeating thoughts and feelings, challenge the nature of irrational and unproductive feelings, and replace them with healthier, more productive beliefs.— From the website “Psychology Today”.Theoretical Assumptions  The REBT framework posits that humans have both innate rational (meaning self-helping, socially helping, and constructive) and irrational (meaning self-defeating, socially defeating, and unhelpful) tendencies and leanings.REBT claims that people to a large degree consciously and unconsciously construct emotional difficulties such as self-blame, self-pity, clinical anger, hurt, guilt, shame, depression and anxiety, and behavior tendencies like procrastination, compulsiveness, avoidance, addiction and withdrawal by the means of their irrational and self-defeating thinking, emoting and behaving.  REBT is then applied as an educational process in which the therapist often active-directively teaches the client how to identify irrational and self-defeating beliefs and philosophies which in nature are rigid, extreme, unrealistic, illogical and absolutist, and then to forcefully and actively question and dispute them and replace them with more rational and self-helping ones.— From Wikipedia.A-B-C-D-E-F ModelIllustration from Wikipedia.  The Bs, irrational beliefs that are most important in the A-B-C model are the explicit and implicit philosophical meanings and assumptions about events, personal desires, and preferences. The Bs, beliefs that are most significant are highly evaluative and consist of interrelated and integrated cognitive, emotional and behavioral aspects and dimensions. According to REBT, if a person’s evaluative B, belief about the A, activating event is rigid, absolutistic, fictional and dysfunctional, the C, the emotional and behavioral consequence, is likely to be self-defeating and destructive. Alternatively, if a person’s belief is preferential, flexible, and constructive, the C, the emotional and behavioral consequence is likely to be self-helping and constructive.  Through REBT, by understanding the role of their mediating, evaluative and philosophically based illogical, unrealistic and self-defeating meanings, interpretations and assumptions in disturbance, individuals can learn to identify them, then go to D, disputing and questioning the evidence for them. At E, effective new philosophy, they can recognize and reinforce the notion no evidence exists for any psychopathological must, ought or should and distinguish them from healthy constructs, and subscribe to more constructive and self-helping philosophies. This new reasonable perspective leads to F, new feelings and behaviors appropriate to the A they are addressing in the exercise.— From Wikipedia.My UnderstandingWe cannot ignore our feelings. Since the adversity has come true, the best thing we can do now is to adjust what we think to have positive feelings, and move on."
  },
  
  {
    "title": "HyperNetworks",
    "url": "/posts/HyperNetworks/",
    "categories": "Artificial Intelligence, Machine Learning Basics",
    "tags": "tech, HyperNetworks",
    "date": "2023-11-13 18:40:00 +0000",
    





    
    "snippet": "Introduction[Paper]: HyperNetworks  The following part has not been finished yet.Application in QMIXIllustration from the corresponding paper.The following statements from the paper are key to unde...",
    "content": "Introduction[Paper]: HyperNetworks  The following part has not been finished yet.Application in QMIXIllustration from the corresponding paper.The following statements from the paper are key to understand this application:  “The weights of the mixing network are produced by separate hypernetworks. Each hypernetwork takes the state s as input and generates the weights of one layer of the mixing network. Each hypernetwork consists of a single linear layer, followed by an absolute activation function, to ensure that the mixing network weights are non-negative. The output of the hypernetwork is then a vector, which is reshaped into a matrix of appropriate size. The biases are produced in the same manner but are not restricted to being non-negative. The final bias is produced by a 2 layer hypernetwork with a ReLU non-linearity.”  “QMIX relies on a neural network to transform the centralised state into the weights of another neural network, in a manner reminiscent of hypernetworks (Ha et al., 2017). This second neural network is constrained to be monotonic with respect to its inputs by keeping its weights positive.”  “The state is used by the hypernetworks rather than being passed directly into the mixing network because Qtot is allowed to depend on the extra state information in non-monotonic ways. Thus, it would be overly constraining to pass some function of s through the monotonic network alongside the per-agent values. Instead, the use of hypernetworks makes it possible to condition the weights of the monotonic network on s in an arbitrary way, thus integrating the full state s into the joint action-value estimates as flexibly as possible.”So in my understanding, the map of $Q^i,\\forall i$ to $Q_{tot}$ should be monotonic and be dependent on the current state. If the current state is inputed directly into the net then it is monotonic, too. And this is not what we want."
  },
  
  {
    "title": "Decision Transformers",
    "url": "/posts/Decision-Transformers/",
    "categories": "Artificial Intelligence, Machine Learning Basics",
    "tags": "tech, transformer, generalist",
    "date": "2023-11-11 18:40:00 +0000",
    





    
    "snippet": "Decision Transformer  Paper: Decision Transformer: Reinforcement Learning via Sequence Modeling - NeurIPS 2021  [Website][Code]Illustration from the corresponding paper.Illustration from the corres...",
    "content": "Decision Transformer  Paper: Decision Transformer: Reinforcement Learning via Sequence Modeling - NeurIPS 2021  [Website][Code]Illustration from the corresponding paper.Illustration from the corresponding paper. Decision Transformer Pseudocode for continuous actions.The following statements from the paper are key to understand this model:  “Unlike prior approaches to RL that fit value functions or compute policy gradients, Decision Transformer simply outputs the optimal actions by leveraging a causally masked Transformer. By conditioning an autoregressive model on the desired return (reward), past states, and actions, our Decision Transformer model can generate future actions that achieve the desired return. Despite its simplicity, Decision Transformer matches or exceeds the performance of state-of-the-art model-free offline RL baselines on Atari, OpenAI Gym, and Key-to-Door tasks.”  “States, actions, and returns are fed into modality- specific linear embeddings and a positional episodic timestep encoding is added. Tokens are fed into a GPT architecture which predicts actions autoregressively using a causal self-attention mask.”  “We will train transformer models on collected experience using a sequence modeling objective.”In my understanding, training a decision transformer is a supervised learning. By using offline data, the model is trained to anticipate an action that shares the same pattern in the offline data, given the current history.Trajectory Transformer  Paper: Offline Reinforcement Learning as One Big Sequence Modeling Problem - NeurIPS 2021  [Website][Code][Blog]A Generalist Agent: Gato  Paper: A Generalist Agent - Transactions on Machine Learning Research 2022Illustration from the corresponding paperIllustration from the corresponding paper. Training phase of Gato.Illustration from the corresponding paper. Running Gato as a control policy.The following statements from the paper are key to understand this work:  “The same network with the same weights can play Atari, caption images, chat, stack blocks with a real robot arm and much more, deciding based on its context whether to output text, joint torques, button presses, or other tokens.”  “Gato was trained on 604 distinct tasks with varying modalities, observations and action specifications.”  “Gato consumes a sequence of interleaved tokenized observations, separator tokens, and previously sampled actions to produce the next action in standard autoregressive manner. The new action is applied to the environment a game console in this illustration, a new set of observations is obtained, and the process repeats.”In my understanding, Gato is a decision transformer trained on various tasks using offline data. In different tasks, the tokenization is different but the core of the network shares the same one.Multi-Game Decision Transformer  Paper: Multi-Game Decision Transformers - NeurIPS 2022  [Website][Code][Blog]Illustration from the corresponding paper. An overview of the training and evaluation setup.The following statements from the paper are key to understand this work:  “We observe expert-level game-play in the interactive setting after offline learning from trajectories ranging from beginner to expert.”"
  },
  
  {
    "title": "Music Theory",
    "url": "/posts/Music-Theory/",
    "categories": "Music",
    "tags": "life, music",
    "date": "2023-11-04 18:40:00 +0000",
    





    
    "snippet": "  The following part has not been finished yet.",
    "content": "  The following part has not been finished yet."
  },
  
  {
    "title": "TED | Efficiency Tips",
    "url": "/posts/TED-Efficiency-Tips/",
    "categories": "Efficiency",
    "tags": "life, efficiency, TED",
    "date": "2023-11-04 18:30:00 +0000",
    





    
    "snippet": "  This note will be consistently updated.Communication SkillsHow to speak so that people want to listen | Julian TreasureAvoid these habits  Gossip  Judging  Negativity  Complaining  Excuses  Exagg...",
    "content": "  This note will be consistently updated.Communication SkillsHow to speak so that people want to listen | Julian TreasureAvoid these habits  Gossip  Judging  Negativity  Complaining  Excuses  Exaggeration (Lying)  DogmatismPrinciples  Honesty: Be clear and straight.  Authenticity: Be yourself.  Integrity: Be your word.  Love: Wish them well.The way you say it  Humans prefer voices that are low (coming from chest, rather than from the nose or the throat). Because we associate depth with power and with authority.  We prefer voices rich, warm, smooth.  You can train to get there (coach, posture, breathing, exercises).  Change tones, to communicate meanings. Dont use wrong tonality and make it reflect what you are trying to communicate.  Pace          Silence. There’s nothing wrong with a bit of silence. We don’t have to fill it with “um”s and “ah”s.        Pitch: How high.  Volume: Loud or quiet.Vocal warm-up exercises  Arms up, deep breath in while saying”Ahhhh”: Breath out as arms go down  Warm up lips “bah, bah, bah…”  Lips coming alive “brrrr” like kids  Tongue exaggerated “la, la, la, la”  “Rrrrrrrrr” like champagne for the tongue  Siren high to low “oooweeeeaaaw”7 Ways to Make a Conversation With Anyone | Malavika Varadan  The first word flood gates. Just say it. “Hi” with a big smile.  Skip the small talk. Ask really personal questions. Just ask something unique and personal.          “An interesting name. How did you parents think of it? Is there a story behind it?”      “How long have you lived in this city? Do you remember the first day you landed here?”      “Where do you come from? Where do you family live?”        Find the “Mee-Too”s.  Pay a unique compliment. The people will forget what you do, and they forget what you say, but they never forget how you make them feel."
  },
  
  {
    "title": "Set",
    "url": "/posts/Set/",
    "categories": "Mathematics",
    "tags": "tech, math",
    "date": "2023-11-02 18:40:00 +0000",
    





    
    "snippet": "  This note will be consistently updated. Related fields: Real Analysis, General Topology, Geometry.Supremum &amp; InfimumThe supremum of a nonempty set $X \\subset \\mathbb{R}$ is the smallest scala...",
    "content": "  This note will be consistently updated. Related fields: Real Analysis, General Topology, Geometry.Supremum &amp; InfimumThe supremum of a nonempty set $X \\subset \\mathbb{R}$ is the smallest scalar $y$ such that\\[y \\geq x \\text { for all } x \\in X.\\]The infimum of a set $X \\subset \\mathbb{R}$ is the largest scalar $y$ such that\\[y \\leq x \\text { for all } x \\in X.\\]If $\\sup X \\in X(\\inf X \\in X)$, then $\\sup X=\\max X(\\inf X=\\min X)$.Example:  $\\sup \\set{1 / n: n \\geq 1}=\\max \\set{1 / n: n \\geq 1}=1,$  $\\inf \\set{1 / n: n \\geq 1}=0 .$  Closed set  Bounded set  Compact set  CompleteFor $\\epsilon&gt;0$ and $x \\in \\mathbb{R}^n$ we define $B_\\epsilon(x)=\\set{y \\in \\mathbb{R}^n:|x-y|&lt;\\epsilon}$ to be open ball with radius $\\epsilon$ and center $x.$ Next, we collect further properties and terminologies for sets:  A set $X \\subset \\mathbb{R}^n$ is called open if for every $x \\in X$ there exists $\\epsilon&gt;0$ such that $B_\\epsilon(x) \\subset X.$  A set $X \\subset \\mathbb{R}^n$ is closed if $\\mathbb{R}^n \\backslash X$ is open. Alternatively, we can define closedness of set as follows: For every sequence $\\left(x^k\\right)$ with $x^k \\in X$ for all $k$ and $x^k \\rightarrow x$, we have $x \\in X.$  A set $X \\subset \\mathbb{R}^n$ is bounded if there exists $B \\in \\mathbb{R}$ with $|x| \\leq B$ for all $x \\in X$.  A bounded and closed set is called compact.  The set ${(𝑥,𝑦)\\mid 𝑥^2+𝑦^2&lt;1}$ is bounded but not closed.  The set ${(𝑥,𝑦)\\mid 𝑥\\ge0}$ is closed but not bounded.  The following part has not been finished yet."
  },
  
  {
    "title": "Convergence Analysis of Gradient Descent",
    "url": "/posts/Convergence-Gradient-Descent/",
    "categories": "Mathematics",
    "tags": "tech, math, gradient descent, convergence",
    "date": "2023-10-21 18:40:00 +0000",
    





    
    "snippet": "  The following part has not been finished yet.Gradient DescentThe goalWe want to solve this unconstrained minimization problem\\[\\min _x f(x) \\quad \\text { s.t. } \\quad x \\in \\mathbb{R}^n .\\]",
    "content": "  The following part has not been finished yet.Gradient DescentThe goalWe want to solve this unconstrained minimization problem\\[\\min _x f(x) \\quad \\text { s.t. } \\quad x \\in \\mathbb{R}^n .\\]"
  },
  
  {
    "title": "Contraction Mapping Theorem",
    "url": "/posts/Contraction/",
    "categories": "Mathematics",
    "tags": "tech, math, contraction, convergence",
    "date": "2023-10-19 18:40:00 +0000",
    





    
    "snippet": "Metric SpaceDefinition of metric space  Definition. A metric space is an ordered pair $(M, d)$ where $M$ is a set and $d$ is a metric on $M$, i.e., a function $d: M\\times M \\to \\mathbb{R}$ satisfyi...",
    "content": "Metric SpaceDefinition of metric space  Definition. A metric space is an ordered pair $(M, d)$ where $M$ is a set and $d$ is a metric on $M$, i.e., a function $d: M\\times M \\to \\mathbb{R}$ satisfying the following axioms for all points $x, y, z \\in M:$      The distance from a point to itself is zero: $d(x,x) = 0.$    (Positivity) The distance between two distinct points is always positive: If $x\\ne y,$ then $d(x,y)&gt;0.$    (Symmetry) The distance from x to y is always the same as the distance from y to x: $d(x,y) = d(y,x)$    The triangle inequality holds: $d(x,z)\\le d(x,y)+d(y,z).$  Note that $0 = d(x,x)\\le d(x,y)+d(y,x),$ so the second axiom can be weakened to “If $x\\ne y,$ then $d(x,y)\\ne 0$” and combined with the first axiom to “$d(x,y) = 0 \\Leftrightarrow x = y.$”Cauchy sequence  Definition.Let $\\set{x_t}_{t}^\\infty$ be a sequence in a metric space $(M,d),$ then it is a Cauchy sequence, if for every positive real number $\\epsilon&gt;0,$ there is a positive integer $N$ such that for all positive integers $m,n &gt; N,$ the distance $d(x_m, x_n) &lt; \\epsilon.$Symbolically, this is:\\[\\forall \\epsilon &gt; 0 (\\exists N\\in\\mathbb{N} (\\forall m,n\\in \\mathbb{N}(m,n\\ge N \\Rightarrow d(x_m,x_n)&lt; \\epsilon))).\\]Convergent sequence  Definition.Let $\\set{x_t}_{t}^\\infty$ be a sequence in a metric space $(M,d),$ then it is a convergent sequence, if there is a $x\\in M$ such that for every positive real number $\\epsilon&gt;0,$ there is a positive integer $N$ such that for all positive integer $n\\ge N,$ the distance $d(x_n, x) &lt; \\epsilon.$  Definition.A point $x$ of the metric space $(M, d)$ is the limit of the sequence $(x_n)$ if: For each $0&lt;\\epsilon\\in\\mathbb{R},$ there is $N\\in\\mathbb{N}$ such that, for every $N\\le n\\in \\mathbb{N},$ we have $d(x_n, x)&lt;\\epsilon.$Symbolically, this is:\\[\\forall \\epsilon &gt; 0 (\\exists N\\in\\mathbb{N} (\\forall n\\in \\mathbb{N}(n\\ge N \\Rightarrow d(x_n,x)&lt; \\epsilon))).\\]Completeness  Definition.A metric space $(M,d)$ is called complete, if every Cauchy sequence in it converges to an element of $M.$Examples:  $\\mathbb{R}$ with usual distance is complete;  $\\mathbb{Q}$ (the set of rational numbers) with usual distance is not complete. Because a sequence of rational numbers can converge to an irrational number, e.g. $\\pi.$Contraction Mapping  Definition.A contraction mapping on a metric space $(M,d)$ is a function $f:M\\to M$ with the property that there is some real number $0\\le k&lt; 1$ such that for all $x$ and $y$ in $M,$\\[d(f(x),f(y))\\le k\\cdot d(x,y).\\]Fixed Point  Definition.Let $(M,d)$ be a metric space and $f:M\\to M$ be a function. If $x\\in M$ and $x=f(x),$ then $x$ is called a fixed point of $f.$Contraction Mapping → Cauchy SequenceLemma  Lemma.Let $(M,d)$ be a metric space, $f:M\\to M$ be a function, $x_0\\in M$, and $x_{t+1} = f(x_t), t=0,1,\\ldots,$ then $\\set{x_t}_{t}^\\infty$ is a Cauchy sequence.ProofSince $f$ is a contraction mapping, there must exist a $\\beta \\in (0,1)$ such that\\[d(x_2, x_1) = d(f(x_1), f(x_0)) \\leq \\beta d(x_1, x_0)\\]and\\[d(x_3, x_2) \\leq \\beta d(x_2, x_1) \\leq \\beta^2 d(x_1, x_0), \\cdots.\\]Generally, we have\\[d(x_{t+1}, x_t) \\leq \\beta d(x_t, x_{t-1}) \\leq \\cdots \\leq \\beta^t d(x_1, x_0).\\]Let $N$ be a positive integer. Let positive integers $m, n$ satisfy $m &gt; n \\geq N$. Then by the triangle inequality, we have\\[\\begin{aligned}d(x_m, x_n) &amp; \\leq d(x_m, x_{m-1}) + d(x_{m-1}, x_{m-2}) + \\cdots + d(x_{n+2}, x_{n+1}) + d(x_{n+1}, x_n) \\\\&amp; \\leq \\left[\\beta^{m-1} + \\beta^{m-2} + \\cdots + \\beta^{n+1} + \\beta^n\\right] d(x_1, x_0) \\\\&amp; = \\beta^n \\left[\\beta^{m-1-n} + \\beta^{m-2-n} + \\cdots + \\beta^1 + 1\\right] d(x_1, x_0) \\\\&amp; = \\beta^n \\frac{1-\\beta^{m-n}}{1-\\beta} d(x_1, x_0) \\\\&amp; = \\frac{\\beta^n - \\beta^m}{1-\\beta} d(x_1, x_0) \\\\&amp; \\leq \\frac{\\beta^N}{1-\\beta} d(x_1, x_0) .\\end{aligned}\\]Let $\\epsilon &gt; 0$. If $d(x_1, x_0) &gt; 0$ (the case of $d(x_1, x_0) = 0$ will be discussed later), then for a positive integer $N$ satisfying\\[N &gt; \\frac{\\ln \\frac{(1-\\beta) \\epsilon}{d(x_1, x_0)}}{\\ln \\beta},\\]we have for any $m, n \\geq N$, that $d(x_m, x_n) \\leq \\frac{\\beta^N}{1-\\beta} d(x_1, x_0) &lt; \\epsilon$, hence $\\set{x_t}_{t=0}^{\\infty}$ is a Cauchy sequence.If $d(x_1, x_0) = 0$, then $\\set{x_t}_{t=0}^{\\infty}$ is a constant sequence, and therefore also a Cauchy sequence. $\\blacksquare$Contraction Mapping TheoremTheorem  Theorem. Let $(M, d)$ be a complete metric space, and $f: M \\to M$ be a contraction mapping. Then $f$ has a unique fixed point $x^* \\in M$. Furthermore, starting from any point $x_0 \\in M$, the sequence $\\left(x_0, f(x_0), f(f(x_0)), \\cdots\\right)$ converges to $x^*$.ProofUniqueness: First, let’s prove that if a point is a fixed point of $f$, then it is the only fixed point of $f$. Suppose $x_1, x_2 \\in M$ and $x_1=f(x_1)$, $x_2=f(x_2)$. Since $f$ is a contraction mapping, there exists a $\\beta \\in (0, 1)$ such that\\[d(x_1, x_2) = d(f(x_1), f(x_2)) \\leq \\beta d(x_1, x_2).\\]The above inequality holds only when $d(x_1, x_2) = 0$, i.e., $x_1 = x_2$.Convergence: Since $M$ is complete, there exists a point $x^* \\in M$ such that the sequence $\\set{x_t}$ converges to $x^*$.Fixed Point: Let $\\epsilon &gt; 0$. Since $\\set{x_t}$ is a Cauchy sequence and converges to $x^*$, there exists a positive integer $T$ such that for any $t \\geq T$, we have $d(x^*, x_t) &lt; \\frac{\\epsilon}{3}$ and $d(x_{t+1}, x_t) &lt; \\frac{\\epsilon}{3}$. For $t \\geq T$, by the triangle inequality, we have\\[d(x^*, f(x^*)) \\leq d(x^*, x_t) + d(x_t, f(x_t)) + d(f(x_t), f(x^*)).\\]Note that the middle term on the right-hand side is equal to $d(x_t, x_{t+1})$. Hence, the first two terms on the right-hand side are both less than $\\frac{\\epsilon}{3}$ by our earlier discussion. Now, consider the third term. We have\\[d(f(x_t), f(x^*)) \\leq \\beta d(x_t, x^*) &lt; \\frac{\\epsilon}{3}.\\]Thus, $d(x^*, f(x^*)) &lt; \\epsilon$. Since $\\epsilon$ was arbitrary, we have $d(x^*, f(x^*)) = 0$, i.e., $x^* = f(x^*)$. $\\blacksquare$"
  },
  
  {
    "title": "My Understanding and Appreciation of Batman",
    "url": "/posts/Batman/",
    "categories": "Literature",
    "tags": "life, literature, commics, Batman",
    "date": "2023-09-27 18:40:00 +0000",
    





    
    "snippet": "蝙蝠侠的悲剧内核在于，他必须接受自己无法改变的命运，接受自己当时的无能。 父母在他面前被枪杀，自己却什么也做不了。无论他以后变得多强，不管他帮助了多少人避免遇到相同的事情，已发生的事再也无法挽回了。一个比较激进的解读方式是，蝙蝠侠是布鲁斯在父母遇害之后想象出来的一个更强大的自己，哥谭里的所有反派都是自己的心理障碍，布鲁斯的余生都在与自己的心理障碍作斗争。在说蝙蝠侠之前，先说另一个到处泄愤的疯...",
    "content": "蝙蝠侠的悲剧内核在于，他必须接受自己无法改变的命运，接受自己当时的无能。 父母在他面前被枪杀，自己却什么也做不了。无论他以后变得多强，不管他帮助了多少人避免遇到相同的事情，已发生的事再也无法挽回了。一个比较激进的解读方式是，蝙蝠侠是布鲁斯在父母遇害之后想象出来的一个更强大的自己，哥谭里的所有反派都是自己的心理障碍，布鲁斯的余生都在与自己的心理障碍作斗争。在说蝙蝠侠之前，先说另一个到处泄愤的疯子：小丑。 这个角色的内核是：与其内耗折磨自己，不如发疯折磨他人。很多人应该都有过这样的体验，遇到事情会反思是不是自己不对，但是怎么想都没办法想清楚，怎么都觉得不对劲。因为很多时候这就不是你的错。自古以来的各种社会事件也表明，在被周围人pua、在被周围人不要脸地得寸进尺地剥削的时候，似乎只有发疯才能得到尊重。匹夫一怒，血溅五步，我连命都不要了，你怕还是不怕？现在你愿意听了吗，把我当回事了吗？小丑的意志是强大的，他的行为是一种寻求解放的反抗，从社会的角度来说，他希望得到被尊重机会。但他的行为是有害的，心态是懦弱的。他不像蝙蝠侠，他从来不会去承认自己的悲剧的起源，比如《黑暗骑士》中，他自己就口述过非常多的起源，他的精神是抽象的，他不希望别人觉得他是因为某个具体的原因才堕落，而是希望让人意识到就算不遇到这件事，也会遇到另一件事，因为这个世界就是荒诞的。他在寻求经历上的超越性，或者是时间上的超越性，这无可厚非。而他的懦弱却在于，他会去向周围人去证明这个世界是疯狂的，试图让别人认识到世界的荒诞，从而让自己好受些：你看吧，这个世界就是这样烂，和我一起发疯吧。围绕小丑这个角色的解说有很多，特别是《致命玩笑》中小丑说自己和蝙蝠侠是硬币的两面，可以共享同一个精神病病房。但是蝙蝠侠作为疯子，却做了正确的事，他正确引导了自己的精力（也许方法有待改进）。小丑当然觉得蝙蝠侠有意思，这个人和他一样是认识到世界的，但是他为什么要去做好事？他也要向蝙蝠侠证明，或者是想让自己相信，蝙蝠侠和自己一样烂。比如《黑暗骑士》中的炸船表演，他就是想证明有很多悲惨的命运是客观规律，以及逼疯哈维丹特：混乱是有很大的引力的，让一个人发疯你只需要轻轻一推就行了。另外一个例子是《致命玩笑》中的折磨咯噔局长，但是局长很坚强。还是炸船的例子更好一点，折磨咯噔局长是自己上场给别人加麻烦，档次太低了。。不过电影为了艺术性直接让两方都表现出了善良的一面，这个发展也太苍白了点。其实从我的专业的角度来说，小丑在一定程度上是对的，确实有很多这样无法反抗的荒诞的社会现象，这被称为是社会困境（Social Dilemma），意思是每个人都基于自己的最优决策，但是却导致了群体的社会效益很低。这是非常有研究价值的经济学领域。然而，这些现象虽然存在，但也是有解的，就是机制设计（Mechanism Design），你可以改变机制来引导每个人做自己最优的决策同时导向好的社会效益。如果小丑多学一些数学模型，也许可以不打打杀杀的，给蝙蝠侠邮箱里发几本书和论文，他们就可以用论文来battle了:)在2022年左右盛行的摆烂文化和发疯文学，就是越发丰富的物质条件和越发暗流涌动的社会问题越来越激起人们对于生命和权威的反思的网络现象。《小丑2019》这个电影想表达的核心就是这个意思，可惜电影里的小丑生活条件还是不够惨，也还是太矫情，激不起太大的共情。现实中的问题早已更加荒诞了。最让我惊艳的关于小丑的描写是Batman V3中的笑语谜语之战，小丑已经很久笑不出来了。杀了一个人：一个子弹撞上了另一个人的脑袋？不够好笑。Tom King本人表示过，他写蝙蝠侠故事就像是在写自己的经历，读到这个片段我才是真的信服了，这段描写已经是荒诞的巅峰。小丑解构太久，已经觉得无聊了。如果你抑郁过，而且真的到了想死的那种地步，你应该经历过小丑的这种阶段：我连命都不要了，我什么都不在乎，任何的权威和规则对我来说都是狗屁不通，我也厌倦了向别人表达我的想法，厌倦了到处去游说，我都要死了，你们怎么想关我什么事？（更何况我最在乎的蝙蝠侠都不理我，他还在做好事，生气）当然小丑并不是抑郁，是狂得不接受任何法则，我举这个例子只是为了方便共情这种心态。这时候自作聪明的谜语人出来了：哈，我看出来了，小丑你觉得无聊了，那么让聪明的我来帮你解决吧！但是他并没有意识到小丑真正面临的问题。谜语人的做法是：编织了一个借口，要打仗要杀人，要把蝙蝠侠牵扯进来以此让小丑牵扯进来，然后把城市搅得天翻地覆，然后故意设局杀了风筝人全家，制造出风筝人这么一个被命运打败的堕落人，然后又故意算漏，故意让这么个不起眼的loser风筝人称为自己被小丑打败的关键因素。好！又是一个蝙蝠侠拯救世界的故事！我设计了全局，所有的所有，只为博你小丑一笑。让我看到你在笑，我解决了这个问题！（不过谜语人确实挺牛的，连风筝人会委屈当卧底都算到了）风筝人的儿子被谜语人毒杀一个完全被命运支配的普通人，除了苦笑作乐还能做什么，惨风筝人的眼神充满仇恨，他来投靠谜语人是来当卧底的；谜语人的谜语其实也暗示他知道风筝人是小丑的卧底你的忍辱负重也是我计划的一环，正是我要讲的笑话的笑点所在谜语人暗恋小丑，小丑暗恋蝙蝠侠但是谜语人明显是低估了小丑的问题，小丑已经是解构得无聊了，而且也根本说服不了蝙蝠侠，你这再填一个trivial的例子又有什么用？但是没想到的是，反转来了，最后蝙蝠侠实在忍不住了：谜语人做了这一切，杀了那么多人，只是为了讲一个笑话？于是他举刀杀向了谜语人。却被小丑挡下来了。这下小丑笑了，因为他知道蝙蝠侠的后半辈子都无法自恰了，他打破了自己的准则balabala，可爱的蝙蝠侠嘿嘿嘿（小丑痴汉脸）。事实也还真是如此，因为整个笑话谜语之战其实都是蝙蝠侠的口述，是对猫女的自白。他一直放不下心来向猫女求婚，就是因为他认为自己并不像人们想象的那么好，他的一个心结就是自己失控想杀掉谜语人，因此他认为自己彻底堕落了，变得和小丑一样了。看看猫女是怎么回应的：这一幕是笑话谜语之战的开篇这一幕是笑话谜语之战的结尾停止想象就好:)说回蝙蝠侠，这个人确实是疯子。Batman V3中就有非常多的片段是自嘲，一个成年人穿着话剧服装在晚上打人，仿佛揍得越狠越有用。他确实在泄愤，但他也确实是在不要命地做好事。这个疯子是有非常严重的自我救赎和自我摧毁的倾向的。他想战斗到死，把自己的最后一丝精力全部贡献出来，这样才能让自己安心。这就是蝙蝠侠的自杀方式。漫画中有非常多的画面是讨论good death，在脑子里走马灯似地和自己的父母对话，比如在和贝恩打架的那个片段（第20话）。说实话，我就是从这个方面入坑的蝙蝠侠。对比85话最后的关于这一点的新的升华。。这里多的不想再说了，直接上图。20话克莱儿是个超人类女孩，本来蝙蝠侠觉得找到了这个人，自己就可以退休了，哥谭就有人照顾了。但是克莱儿被反派设局逼疯了。他需要找心灵海盗来医治克莱儿。但是心灵海盗在贝恩那，贝恩需要心灵海盗来让自己获得心灵的平静和救赎。所以蝙蝠侠就从贝恩那里把心灵海盗抢过来了，现在贝恩来到哥谭，找蝙蝠侠报仇。“那个女孩需要帮助，因此我帮助了她，仅此而已。”方框是蝙蝠侠脑中浮现的母亲的声音此时在和贝恩打架，打不过最右边是闪回，小时候父亲被枪杀最右边是闪回，小时候母亲被枪杀安详铁头功秒杀。。莫名其妙。。但是这不是重点85话会飞的这个就是上文提到的克莱儿；提到的父亲是指的平行宇宙的狗血剧情，不想提了说话的是蝙蝠侠，在和倒霉蛋风筝人喝酒；蝙蝠侠知道风筝人，风筝人不知道是蝙蝠侠这是风筝人猫女入场完结撒花85话的收场很草率，内容也很直白。因为销量不好Tom King被赶走了，这是他的最后一话。。或许很多人不吃这一套吧。另外Batman V3中很有意思的一话是53话，蝙蝠侠因为猫女逃婚，情绪失控揍了人，把急冻人给打懵了，暴力逼供，把急冻人送上了法庭。然后为了弥补，自己又以布鲁斯的身份参加陪审团，就是多个人决定急冻人有没有罪的那个会，然后以一己之力说服了其他所有人，最终判急冻人无罪，上演了一场《十二怒汉》的经典剧情。其中布鲁斯为蝙蝠侠定罪很有意思，讨论了“为什么蝙蝠侠不是神”，也是直接上图。53话精彩之处在于他的每一句话都是实话。布鲁斯自己也举手了，说明如果不走蝙蝠侠这条路，他会自杀无法反抗的命运他的每一句话都是实话“我”是有极限的这里的意思是要知足和顺从命运，V3中很多处都透露出斯多葛主义。这些地方是我不赞同的，有时候还是稍微小丑一点好。不过这里放引用应该是反讽再读的时候意识到，这是一个很不错的隐喻，上帝=无法反抗的命运，命运中有好有坏，你只能全盘接受。而，蝙蝠侠=人类社会，社会上有好有坏，但他终究不是命运，人是一定程度上可以改变的，你不能认为人类社会就是无法反抗的命运，他说什么就是什么，他打你你就只能接受宣判。人类社会也是会犯错的，而作为我们每个人，需要敢于面对真相，敢于给人类社会（蝙蝠侠）定罪。我们不是神也不是超人，我们无法创造、更改、删除法则，但是我们可以认识、包装法则，设计新的机制为我们所用，这就是人类的极限，也是蝙蝠侠的象征。我们要区分出什么是我们可以改变的，不要太过于悲观了。"
  },
  
  {
    "title": "A Note on Stochastic Processes",
    "url": "/posts/Stochastic-Processes/",
    "categories": "Mathematics",
    "tags": "tech, math, stochastic processes",
    "date": "2023-09-03 18:40:00 +0000",
    





    
    "snippet": "  This note partially uses the materials from the notes of MATH2750.Transition Matrix  The transition kernel $\\mathbf{M}$ is a square matrix of size $\\vert S\\vert \\times \\vert S\\vert$.  $\\mathbf{M}...",
    "content": "  This note partially uses the materials from the notes of MATH2750.Transition Matrix  The transition kernel $\\mathbf{M}$ is a square matrix of size $\\vert S\\vert \\times \\vert S\\vert$.  $\\mathbf{M}_{ij}$ means the probability of the current state $i$ transitioning to the next state $j$.  The exponent $k$ of the matrix power $\\mathrm{M}^k$ represents the state distribution after $k$ consecutive state transitions from the current state distribution.  $\\mathbf{M}_{ij}^k$ means the probability of the current state $i$ transitioning to the state $j$ after $k$ timesteps.Class StructureThere may be some “irrelevant” states in a Markov chain. We can eliminate them to simplify the model.AccessibleThese statements are equivalent:  State $j$ is accessible from state $i;$  $i$ can transition to $j$ in $n$ steps;  Starting from $i$, there’s a positive chance that we’ll get to $j$ at some point in the future;  $\\exists n, \\mathrm{s.t. }\\mathbf{M}_{ij}^n &gt; 0;$  $i \\to j.$Communicates withDefinitionThese statements are equivalent:  $i$ communicates with $j$;  $i\\to j$ and $j\\to i$;  $i \\leftrightarrow j$.Properties  Reflexive: $i\\leftrightarrow i, \\forall i.$  Symmetric: if $i \\leftrightarrow j$ then $j \\leftrightarrow i.$  Transitive: if $i \\leftrightarrow j$ and $j \\leftrightarrow k$ then $i \\leftrightarrow k.$Proof of propertiesReflexive$\\mathrm{M}_{ii}^{n=0}=1&gt;0$. In 0 step we stay where we are.SymmetricThe definition of $i\\leftrightarrow j$ is symmetric under swapping $i$ and $j$.Transitive  $i \\leftrightarrow j$ means $\\exists n_1\\ge 1$ s.t. $\\mathrm{M}_{ij}^{n_1}&gt;0.$  $j \\leftrightarrow k$ means $\\exists n_2\\ge 1$ s.t. $\\mathrm{M}_{jk}^{n_2}&gt;0.$\\[\\begin{aligned}&amp; P(X_n = k\\mid X_0 = i) \\\\=&amp; \\sum\\limits_{l,m\\in S} P(X_n = k, X_{n-n_2} = l, X_{n_1} = m\\mid X_0 = i) \\\\ =&amp; \\sum\\limits_{l,m\\in S} P(X_n = k \\mid X_{n-n_2} = l) \\cdot P(X_{n-n_2} = l \\mid X_{n_1} = m) \\cdot P(X_{n_1} = m\\mid X_0 = i) \\\\ \\ge&amp; P(X_n = k \\mid X_{n-n_2} = j) \\cdot P(X_{n-n_2} = j \\mid X_{n_1} = j) \\cdot P(X_{n_1} = j\\mid X_0 = i) \\\\=&amp; \\mathrm{M}_{ij}^{n_1} \\cdot \\mathrm{M}_{jj}^{n-n_1-n_2} \\cdot \\mathrm{M}_{jk}^{n_2} \\\\\\ge&amp; 0\\end{aligned}\\]Communicating classAll states $j$ that communicate with state $i$ are in the same equivalence class (or communicating class) as state $i.$Closed communicating classThese statements are equivalent:  Class $C\\subset S$ is closed if whenever there exist $i\\in C$ and $j\\in S$ with $i\\to j$, then $j\\in C$ also.  No state outside the class is accessible from any state within the class.  Once you enter a closed class, you can’t leave that class.  If a state $j$ is out of a communicating class where state $i$ in (i.e. $i \\nleftrightarrow j$), it still may enter the communicating class (i.e. $j\\to i$ but $i\\nrightarrow j$).Open communicating classIf a communicating class is not closed, then it is open.Irreducible Markov chainDefinitionA Markov chain is irreducible if its entire state space $S$ is on communicating class.How can I tell if a Markov chain is irreducible?The mere presence of zeros in the transition matrix does not mean that the Markov chain is reducible. E.g.:\\[\\begin{bmatrix}0.5 &amp; 0.5 &amp; 0.0 \\\\0.0 &amp; 0.5 &amp; 0.5 \\\\0.5 &amp; 0.0 &amp; 0.5 \\\\\\end{bmatrix}\\]In my understanding, if a whole row (or a whole column) of a transition matrix is all zeros except for the pivot, then the Markov chain with this transition matrix is reducible, and the corresponding row (or column) state can be eliminated.E.g., this is such a transition matrix with which the Markov chain is reducible:\\[\\begin{bmatrix}0.5 &amp; 0.5 &amp; 0.0 \\\\0.4 &amp; 0.5 &amp; 0.1 \\\\0.0 &amp; 0.0 &amp; 1.0 \\\\\\end{bmatrix}\\]Absorbing stateThese statements are equivalent:  State $i$ is an absorbing state.  State $i$ is in a communicating class ${i}$ by itself and that class is closed.  Once you reach state $i$, you can’t leave it.PeriodicityDefinitionPeriodicity describes a pattern of recurrence for a state. For a state $i$ in a Markov chain, its period $d(i)$ is defined as the greatest common divisor of the number of steps it can take to return to state $i$, excluding the possibility of 1-step self-transitions.More formally, consider all $n$ such that $\\mathbf{M}^n_{ii} &gt; 0$ (i.e., the probability of transitioning from state $i$ to state $i$ in $n$ steps is positive). The period $d(i)$ is the greatest common divisor of all such $n$.  If $d(i) = 1$, then state $i$ is said to be aperiodic.  If $d(i) &gt; 1$, then state $i$ is said to be periodic, with a period of $d(i)$.The transition kernel is a square matrix. The exponent $k$ of the matrix power $\\mathrm{M}^k$ represents the state distribution after $k$ consecutive state transitions from the current state distribution.ExamplesAll states in the Markov chain with the following transition matrix are periodic with a period of 3.\\[\\mathbf{M} = \\begin{bmatrix}0 &amp; 1 &amp; 0 \\\\0 &amp; 0 &amp; 1 \\\\1 &amp; 0 &amp; 0 \\\\\\end{bmatrix}\\]All states in the Markov chain with the following transition matrix are aperiodic.\\[\\mathbf{M} = \\begin{bmatrix}0.2 &amp; 0.4 &amp; 0.4 \\\\0.6 &amp; 0.2 &amp; 0.2 \\\\0.1 &amp; 0.8 &amp; 0.1 \\\\\\end{bmatrix}\\]  In my understanding, this constraint is very lenient; as long as the probability of transitioning to oneself is greater than 0, it counts.Hitting TimesHitting timeLet $(X_n)$ be a Markov chain on state space $S$. Let $H_{S’}$ be a random variable representing the hitting time to hit the set $S’\\subset S$, given by\\[H_{S'} = \\min \\{ n\\in \\{0, 1,2,\\ldots\\} : X_n \\in S' \\}\\]$H_{S’}$ means the first timestep that any state in $S’$ appears.If $S’$ contains only one state, i.e. $S’ = {i}$, then the $H_{i}$ means the first timesstep that state $i$ appears.$H_{S’} = \\infty$ if $X_n\\in S’$ for all $n,$ meaning that all the states in $S’$ will never appear.Hitting probabilityThe hitting probability $h_{iS’}$ of the set $S’$ from the state $i$ is\\[\\begin{aligned}    h_{iS'}     =&amp; P(X_n\\in S'\\text{ for some }n\\ge n\\mid X_0 = i) \\\\    =&amp; P(H_{S'} &lt; \\infty \\mid X_0 = i)\\end{aligned}\\]It means that the probability of $S’$ will appear in finite timesteps, given the starting state $i.$If $S’$ contains only one state, i.e. $S’ = {i}$, then the $h_{ij}$ means the probability of $i$ transitioning to $j$ in finite timesteps.\\[h_{ij}     = P(H_j &lt; \\infty \\mid X_0 = i)\\]Expected hitting timeThe expected hitting time $\\eta_{iS’}$ of the set $S’$ starting from state $i$ is\\[\\eta_{iS'} = \\mathbb{E}(H_{S'}\\mid X_0 = i)\\]$\\eta_{iS’}$ can only be finite if $h_{iS’} = 1.$Return timeFirst return timeGiven a state $i \\in S$ of a Markov chain, the first return time $T_i$ to state $i$ is defined as:\\[T_i = \\min \\{ n &gt; 0 : X_n = i | X_0 = i \\}\\]This is the number of time steps required to return to state $i$ for the first time, given that we start at $i$. Essentially, $T_i$ represents the first occurrence of the state $i$ after the initial time, assuming the chain started at state $i$.Expected return timeThe expected return time, $m_i$, to state $i$ is the average number of time steps it takes to return to state $i$ after initially starting at $i$. Mathematically, it’s given by the expected value of the first return time $T_i$:\\[\\begin{aligned}m_i =&amp; \\mathbb{E}[T_i | X_0 = i] \\\\=&amp; \\sum\\limits_{n=1}^\\infty n\\cdot \\mathbf{M}^n_{ii}\\end{aligned}\\]If the Markov chain is guaranteed to return to state $i$ and does so in an average of a finite number of steps, then $m_s$ is finite. If the Markov chain can return to state $i$ but takes, on average, an infinite number of steps, then $m_i$ is infinite. If the chain is not guaranteed to return to state $i$, then the expected return time is undefined (though it is often treated as infinite for certain analyses).Recurrence and TransienceRecurrent and transient statesRecurrent stateA state $i \\in S$ is recurrent if and only if the probability of eventually returning to $i$ starting from $i$ is 1, that is:\\[P(T_i &lt; \\infty) = 1\\]The following statements are equivalent:  State $i$ is a recurrent state;  $P(T_i &lt; \\infty) = 1;$  $P(R_i = \\infty \\mid X_0 = i) = 1$          or $P(R_i&lt;\\infty \\mid X_0 = i) = 0$      The number of return of $i$ is infinite.      $R_i$ is the number of return of $i.$        $\\mathbb{E}(R_i \\mid X_0 = i) = \\infty$State $i$ is a recurrent state if and only if $\\sum\\limits_{n\\ge 1}\\mathbf{M}_{ii}^n = \\infty$Positive ecurrent state            If there exists a constant $m_i$ (expected return time) such that $\\mathbb{E}[T_i      X_0 = i] = m_i &lt; \\infty$. This means that the expected time to return to state $i$ is finite.        If the state space of a Markov chain is finite (has a finite number of states), then all its recurrent states are positive recurrent.  If the state space of an irreducible Markov chain is finite (has a finite number of states), then all its states are positive recurrent.Null recurrent stateIf $\\mathbb{E}[T_i | X_0 = i] = \\infty$. This means that the expected time to return to state $i$ is infinite.Transient StateA state $i \\in S$ is transient if and only if the probability of eventually returning to $i$ starting from $i$ is less than 1, that is:\\(P(T_i &lt; \\infty) &lt; 1\\)  Comparison of the two kinds of state:                   Recurrent states      Transient states                  Visiting behavior      If we ever visit $s$, then we keep returning to $s$ again and again      We might visit $s$ a few times, but eventually we leave $s$ and never come back              Expected number of visits      Starting from $s$, the expected number of visits to $s$ is infinite      Starting from $s$, the expected number of visits to $s$ is finite              Certainty of number of visits      Starting from $s$, the number of visits to $s$ is certain to be infinite      Starting from $s$, the number of visits to $s$ is certain to be finite              Return probability      The return probability $m_s$ equals $1$      The return probability $m_s$ is strictly less than $1$        Consider a Markov chain with transition matrix $\\mathbf{M}$.      If the state $s$  is recurrent, then $\\sum_{n=1}^{\\infty} \\mathbf{M}_{ss}(n) = \\infty,$ and we return to state $s$ infinitely many times with probability 1.    If the state $s$ is transient, then $\\sum_{n=1}^{\\infty} \\mathbf{M}_{ss}(n) &lt; \\infty,$ and we return to state $s$ infinitely many times with probability 0.  Recurrent and transient classesThese statements are equivalent:  Within a communicating class, either every state is transient or every state is recurrent.  Let $i,j\\in S$ be such that $i \\leftrightarrow j$. If $i$ is recurrent, then $j$ is recurrent also; while if $i$ is transient, then $j$ is transient also.If a Markov chain is irreducible (the entire state space is a single communicating class), we can refer to it as a “recurrent Markov chain” or a “transient Markov chain”.  Every non-closed communicating class is transient.          Non-closed communicating class $S’$ means that there exists $i\\in S’$ such that $i\\to j$ and $j\\notin S’$, meaning that you can escape from $S’.$      Once you are escaped from $S’$, you will never come back. Beacause $i\\to j$ and $j\\notin S’ \\Rightarrow j\\nrightarrow i.$        Every finite closed communicating class is positive recurrent.  Infinite closed classes can be positive recurrent, null recurrent, or transient.Stationary DistributionsDefinition  The distribution is defined on the state set, representing the probability of each state occurring at a timestep.  E.g., a state set is ${s_1, s_2}$, then a distribution can be $(0.7, 0.3)$, meaning that $s_1, s_2$ will appear with the probability of $0.7, 0.3$ respectively.  If the current distribution is $\\mathbf{v}$, then the distribution at the next timestep is $\\mathbf{v} \\mathbf{M}$.If a distribution $\\mathbf{v} = \\mathbf{v} \\mathbf{M}$, then it is a stationary distribution.If a distribution $\\mathbf{v}_i = \\sum_j \\mathbf{v}_i \\cdot \\mathbf{M}_{ij}$, then it is a stationary distribution.A stationary distribution is a fixed point.Existence &amp; UniquenessConsider an irreducible Markov chain.  If the irreducible Markov chain is positive recurrent (i.e. the irreducible Markov chain is finite), then a stationary distribution $\\mathrm{v}$ exists, is unique, and is given by $\\mathrm{v}_i = 1/m_i$, where $m_i$ is the expected return time to state $i$.  If the Markov chain is null recurrent or transient, then no stationary distribution exists.Consider a Markov chain with multiple communicating classes.  If none of the classes are positive recurrent, then no stationary distribution exists.  If exactly one of the classes is positive recurrent (and therefore closed), then there exists a uniquestationary distribution, supported only on that closed class.  If more the one of the classes are positive recurrent, then many stationary distributions will exist.          It can be divided; calculate their respective stationary distributions, and then combine them together.      Proof of Existence            Let $X_n$ be the Markov chain, and let $\\pi_j$ be the probability that the chain is in state $j$ at time $n$ given it starts in state $i$ at time 0, i.e., $\\pi_j = P(X_n = j      X_0 = i)$.      Recall the definitions and basic concepts associated with positive recurrence:      For state $i$, the first return time $T_i$ is defined as $T_i = \\min{ n &gt; 0 : X_n = i }$. In other words, it’s the time required for the chain to return to state $i$ for the first time after starting from $i$.                                State $i$ is said to be positively recurrent if $E(T_i          X_0 = i) = m_i &lt; \\infty$. That is, the expected time to return to $i$ starting from $i$ is finite.                    Now, think about the long-time trajectory starting in state $i$. Given the positive recurrence, you’d expect the chain to return to state $i$ multiple times over a long period. Specifically, if you think of $m_i$ steps, you’d expect, on average, that one of these $m_i$ steps is in state $i$.So, when you think over even longer durations, say $n$ steps, you’d expect the chain to be in state $i$ about $n/m_i$ times. In other words, the “proportion” of time the chain spends in state $i$ should converge to $\\frac{1}{m_i}$.From the discussion above, we have:\\[\\lim_{n \\to \\infty} \\frac{1}{n} \\sum_{k=1}^{n} P(X_k = i | X_0 = i)\\]This represents the average proportion of the first $n$ steps where the chain is in state $i$ given it started in state $i$. And indeed, this should equate to $\\frac{1}{m_i}$.Now, let’s consider the average probability of being in state $j$ at time $n$ given we start in state $i$ at time 0:\\[\\lim_{n \\to \\infty} \\frac{1}{n} \\sum_{k=1}^{n} P(X_k = j | X_0 = i) = \\pi_j\\]This $\\pi_j$ is our desired stationary distribution for state $j$.Proof of UniquenessSuppose there are two stationary distributions $\\pi$ and $\\pi’$ for an irreducible Markov chain. Then, for each state $j$ and transition probability matrix $P$:\\[\\pi_j = \\sum_i \\pi_i P_{ij}\\]\\[\\pi'_j = \\sum_i \\pi'_i P_{ij}\\]Consider a convex combination of $\\pi$ and $\\pi’$:\\[\\rho = \\alpha \\pi + (1 - \\alpha) \\pi'\\]For some $0 &lt; \\alpha &lt; 1$.We can then show:\\(\\rho_j = \\sum_i \\rho_i P_{ij}\\)This means $\\rho$ is also a stationary distribution, but this contradicts the fact that the proportion of time the Markov chain spends in state $j$ (due to the ergodic theorem) should converge to the stationary probability of state $j$. Hence, $\\pi$ and $\\pi’$ cannot be distinct, so the stationary distribution is unique.  TODO: what is the ergodic theorem?"
  },
  
  {
    "title": "Sequential Social Dilemma",
    "url": "/posts/SSD/",
    "categories": "Artificial Intelligence, Multi-Agent Reinforcement Learning",
    "tags": "tech, game theory, sequential social dilemma, multi agents, reinforcement learning",
    "date": "2023-09-01 18:40:00 +0000",
    





    
    "snippet": "What is Social Dilemma?DefinitionA social dilemma refers to a situation in which individual actions that seem to be rational and in self-interest can lead to collective outcomes that are undesirabl...",
    "content": "What is Social Dilemma?DefinitionA social dilemma refers to a situation in which individual actions that seem to be rational and in self-interest can lead to collective outcomes that are undesirable for everyone.Examples  Public goods games (Prisoner’s Dilemma, Snowdrift)  Common recourses games.  Braess’s paradox.The free-rider problemThe free-rider problem arises when individuals can benefit from a resource or service without paying for it or contributing to its provision. This can lead to under-provision of that good or service, or overuse and depletion if it’s a shared resource.The problem is particularly pertinent in the case of public goods. Public goods are characterized by two properties:      Non-excludability: Once the good is produced, no one can be excluded from using or benefiting from it. This means producers can’t prevent non-payers from accessing the good.        Non-rivalry: One person’s use of the good doesn’t reduce its availability for others.  Given these properties, individuals often lack the incentive to pay for or contribute to the production of the good, since they can benefit from it without doing so. They instead “free-ride” on the contributions of others. If everyone adopts this mindset, the good may not be produced at all, or it may be produced in insufficient quantities.Examples of the Free-rider Problem:      National Defense: Once a country establishes a defense system, all residents are protected, regardless of how much they personally contributed to it. Some might benefit from the defense without paying taxes or contributing to its provision, effectively free-riding on the contributions of others.        Public Broadcast: A public radio station provides content freely to all listeners. Those who listen without donating or supporting the station are free riders.        Vaccination: When a significant portion of a population gets vaccinated against a contagious disease, herd immunity can protect even those who haven’t been vaccinated. Those who choose not to get vaccinated but still benefit from the reduced risk are free riders.        Public Parks and Clean Air: If a community raises funds to maintain a public park or invests in reducing air pollution, those who enjoy the park or the clean air without contributing to the funding or efforts are free riders.  The free-rider problem can lead to inefficiencies and under-provision of public goods. It’s a central challenge in economics and public policy and often necessitates government intervention or alternative funding mechanisms to ensure adequate provision of public goods or protection of shared resources.Strategic Intentions  Leibo, Joel Z., et al. “Multi-agent reinforcement learning in sequential social dilemmas.” arXiv preprint arXiv:1702.03037 (2017).  The following part has not been finished yet. One may check my writing schedule."
  },
  
  {
    "title": "Zero-Determinant Strategy",
    "url": "/posts/Zero-Determinant/",
    "categories": "Economics & Game Theory",
    "tags": "tech, game theory, sequential social dilemma, multi agents",
    "date": "2023-08-29 18:40:00 +0000",
    





    
    "snippet": "  This note aims to summarize the essence of this paper:Press, William H., and Freeman J. Dyson. “Iterated Prisoner’s Dilemma contains strategies that dominate any evolutionary opponent.” Proceedin...",
    "content": "  This note aims to summarize the essence of this paper:Press, William H., and Freeman J. Dyson. “Iterated Prisoner’s Dilemma contains strategies that dominate any evolutionary opponent.” Proceedings of the National Academy of Sciences 109.26 (2012): 10409-10413.Interesting Facts  As stated in the title: Iterated Prisoner’s Dilemma contains strategies that dominate any evolutionary opponent. And this kind of strategy is the Zero-Determinant strategy.  The “Iterated Prisoner’s Dilemma is an Ultimatum game”.  “One player can enforce a unilateral claim to an unfair share of rewards”, by the ZD strategy.  Any evolutionary agent will be exploited by the agent with the ZD strategy. “An evolutionary player’s best response is to accede to the extortion. Only a player with a theory of mind about his opponent can do better.”  “For any strategy of the longer-memory player Y, shorter-memory X’s score is exactly the same as if Y had played a certain shorter-memory strategy.”Iterated Prisoner’s DilemmaAt each timestep, two agents are playing Prisoner’s Dilemma:            Player1\\Player2      Cooperate (Deny)      Defect (Confess)                  Cooperate (Deny)      $R,R$      $S,T$              Defect (Confess)      $T,S$      $P,P$      where $T &gt; R &gt; P &gt; S$, and the meanings are as follows.  $T$: Temptation  $R$: Reward  $P$: Punishment  $S$: Sucker’s payoffThe two agents repeatedly play this game $T$ times. It might be finite or infinite.Longer-Memory Strategies Offer No Advantage  Player $i$:          Making decisions on short memories $\\tau^i$.      $\\pi^i(a^i\\mid \\tau^i)$        Player $j$          Making decisions on long memories $(\\tau^i,\\Delta\\tau)$.      $\\pi^j(a^j\\mid \\tau^i,\\Delta\\tau)$      Derivation:\\[\\begin{aligned}    &amp;\\sum\\limits_{\\tau^i,\\Delta\\tau}     \\pi^i(a^i\\mid \\tau^i) \\cdot    \\pi^j(a^j\\mid \\tau^i,\\Delta\\tau) \\cdot    \\mathrm{Pr}(\\tau^i,\\Delta\\tau) \\\\    =&amp; \\sum\\limits_{\\tau^i} \\pi^i(a^i\\mid \\tau^i) \\cdot    \\left[        \\sum\\limits_{\\Delta\\tau} \\pi^j(a^j\\mid \\tau^i,\\Delta\\tau)        \\cdot \\mathrm{Pr}(\\Delta\\tau\\mid \\tau^i) \\cdot \\mathrm{Pr}(\\tau^i)    \\right] \\\\    =&amp; \\sum\\limits_{\\tau^i} \\pi^i(a^i\\mid \\tau^i)  \\cdot    \\left[ \\sum\\limits_{\\Delta\\tau} \\mathrm{Pr}(a^j, \\Delta\\tau\\mid \\tau^i) \\right]    \\cdot \\mathrm{Pr}(\\tau^i)\\\\    =&amp; \\sum\\limits_{\\tau^i} \\pi^i(a^i\\mid \\tau^i) \\cdot \\mathrm{Pr}(a^j \\mid \\tau^i) \\cdot \\mathrm{Pr}(\\tau^i)\\end{aligned}\\]$\\mathrm{Pr}(a^j \\mid \\tau^i)$ is the player $j$’s marginalized strategy. And\\[\\mathrm{Pr}(a^j \\mid \\tau^i) = \\sum\\limits_{\\Delta\\tau} \\mathrm{Pr}(a^j, \\Delta\\tau\\mid \\tau^i).\\]  After some plays, $j$ can estimate the expectations, and it can switch to an equivalent short-memory strategy.  “$j$’s switching between a long- and short-memory strategy is completely undetectable (and irrelevant) to $i$.”  Is this conclusion only suitable for repeated games?Zero-Determinant StrategySome tedious parts were automatically filled in with the help of ChatGPT 4.Notation of 4 outcomes            Player1\\Player2      Cooperate (c)      Defect (d)                  Cooperate (c)      $\\mathrm{cc}$ (1)      $\\mathrm{cc}$ (2)              Defect (d)      $\\mathrm{dc}$ (3)      $\\mathrm{cc}$ (4)      Notation of strategies  There are two players, $i$ and $j$, with memory-one strategies.  Strategies are based on the outcome of last play.\\[\\begin{aligned}\\mathbf{p} =&amp; \\pi^i(a_t^i=\\mathrm{Cooperate}\\mid a_{t-1}^i,a_{t-1}^j) \\\\=&amp;(p_{\\mathrm{cc}}, p_{\\mathrm{cd}}, p_{\\mathrm{dc}}, p_{\\mathrm{dd}}) \\\\=&amp;(p_1, p_2, p_3, p_4)\\end{aligned}\\]$p_1, p_2, p_3, p_4$ are independent and range from $[0,1].$\\[\\begin{cases}    p_{\\mathrm{cc}} = \\pi^i(a_t^i=\\mathrm{Cooperate}\\mid a_{t-1}^i =\\mathrm{Cooperate},a_{t-1}^j=\\mathrm{Cooperate}) \\\\    p_{\\mathrm{cd}} = \\pi^i(a_t^i=\\mathrm{Cooperate}\\mid a_{t-1}^i=\\mathrm{Cooperate},a_{t-1}^j=\\mathrm{Defect}) \\\\    p_{\\mathrm{dc}} = \\pi^i(a_t^i=\\mathrm{Cooperate}\\mid a_{t-1}^i=\\mathrm{Defect},a_{t-1}^j=\\mathrm{Cooperate}) \\\\    p_{\\mathrm{dd}} = \\pi^i(a_t^i=\\mathrm{Cooperate}\\mid a_{t-1}^i=\\mathrm{Defect},a_{t-1}^j=\\mathrm{Defect})\\end{cases}\\]\\[\\mathbf{q} = \\pi^j(a_t^j=\\mathrm{Cooperate}\\mid a_{t-1}^i,a_{t-1}^j)\\]Markov transition matrix: $\\mathbf{M}(\\mathbf{p}, \\mathbf{q})$  It is a transition kernel of an MDP.  The rows indicates the current states.  The columns indicates the next states.  Each entry indicates the probability of the current row state transitioning to the next column state.                   $\\mathrm{cc}$      $\\mathrm{cd}$      $\\mathrm{dc}$      $\\mathrm{dd}$                  $\\mathrm{cc}$      $\\mathrm{Pr}(\\mathrm{cc}\\mid \\mathrm{cc})$      $\\mathrm{Pr}(\\mathrm{cd}\\mid \\mathrm{cc})$      $\\mathrm{Pr}(\\mathrm{dc}\\mid \\mathrm{cc})$      $\\mathrm{Pr}(\\mathrm{dd}\\mid \\mathrm{cc})$              $\\mathrm{cd}$      $\\mathrm{Pr}(\\mathrm{cc}\\mid \\mathrm{cd})$      $\\mathrm{Pr}(\\mathrm{cd}\\mid \\mathrm{cd})$      $\\mathrm{Pr}(\\mathrm{dc}\\mid \\mathrm{cd})$      $\\mathrm{Pr}(\\mathrm{dd}\\mid \\mathrm{cd})$              $\\mathrm{dc}$      $\\mathrm{Pr}(\\mathrm{cc}\\mid \\mathrm{dc})$      $\\mathrm{Pr}(\\mathrm{cd}\\mid \\mathrm{dc})$      $\\mathrm{Pr}(\\mathrm{dc}\\mid \\mathrm{dc})$      $\\mathrm{Pr}(\\mathrm{dd}\\mid \\mathrm{dc})$              $\\mathrm{dd}$      $\\mathrm{Pr}(\\mathrm{cc}\\mid \\mathrm{dd})$      $\\mathrm{Pr}(\\mathrm{cd}\\mid \\mathrm{dd})$      $\\mathrm{Pr}(\\mathrm{dc}\\mid \\mathrm{dd})$      $\\mathrm{Pr}(\\mathrm{dd}\\mid \\mathrm{dd})$                         $\\mathrm{cc}$      $\\mathrm{cd}$      $\\mathrm{dc}$      $\\mathrm{dd}$                  $\\mathrm{cc}$      $p_1\\cdot q_1$      $p_1\\cdot (1-q_1)$      $(1-p_1)\\cdot q_1$      $(1-p_1)\\cdot(1-q_1)$              $\\mathrm{cd}$      $p_2\\cdot q_3$      $p_2\\cdot (1-q_3)$      $(1-p_2)\\cdot q_3$      $(1-p_2)\\cdot(1-q_3)$              $\\mathrm{dc}$      $p_3\\cdot q_2$      $p_3\\cdot (1-q_2)$      $(1-p_3)\\cdot q_2$      $(1-p_3)\\cdot(1-q_2)$              $\\mathrm{dd}$      $p_4\\cdot q_4$      $p_4\\cdot (1-q_4)$      $(1-p_4)\\cdot q_4$      $(1-p_4)\\cdot(1-q_4)$      $\\mathbf{M}$ has a unit eigenvalue  This part requires some knowledge of stochastic processes. Check my other note.  The following part has not been finished yet. One may check my writing schedule.  $\\mathbf{M}$ is irreducible (?)  $\\mathbf{M}$ is finiteThen $\\mathbf{M}$ is positive recurrent.  $\\mathbf{M}$ is a positive recurrent Markov chain.  $\\Rightarrow$ $\\mathbf{M}$ has a stationary distribution $\\mathbf{v} = \\mathbf{v} \\mathbf{M}.$  $\\Rightarrow$ $\\mathbf{M}$ has a unit eigenvalue: $1\\cdot \\mathbf{v} = \\mathbf{v} \\mathbf{M}.$y distribution exists."
  },
  
  {
    "title": "Classic Games",
    "url": "/posts/Classic-Games/",
    "categories": "Economics & Game Theory",
    "tags": "tech, game theory, matrix game",
    "date": "2023-08-13 08:30:00 +0000",
    





    
    "snippet": "  This note will be consistently updated.Prisoner’s Dilemma  Two members of a criminal organization are arrested and imprisoned. Each prisoner is in solitary confinement with no means of communicat...",
    "content": "  This note will be consistently updated.Prisoner’s Dilemma  Two members of a criminal organization are arrested and imprisoned. Each prisoner is in solitary confinement with no means of communicating with the other. The prosecutors lack sufficient evidence to convict the pair on the principal charge, but they have enough to convict both on a lesser charge. Simultaneously, the prosecutors offer each prisoner a bargain. Each prisoner is given the opportunity either to betray the other by testifying that the other committed the crime, or to cooperate with the ot\ther by remaining silent.  Poundstone, William. Prisoner’s dilemma: John von Neumann, game theory, and the puzzle of the bomb. Anchor, 1993.Normal form            Player1\\Player2      Cooperate (Deny)      Defect (Confess)                  Cooperate (Deny)      $b,b$      $d,a$              Defect (Confess)      $a,d$      $c,c$      $a \\gt b \\gt c \\gt d$.If both cooperate, each gets R (for Reward).If both defect, each gets P (for Punishment).If one defects and the other cooperates, the defector gets T (for Temptation) and the cooperator gets S (for Sucker’s payoff).In some studies, $a,b,c$ and $d$ are represented by $T,R,P$ and $S$ respectively.  $T$: Temptation  $R$: Reward  $P$: Punishment  $S$: Sucker’s payoffAttractor of player1: Reversed N-like (or N-like)            $b\\downarrow$      $d\\downarrow$                  $a$      $c(\\nwarrow)$      Attractor of player2:            $b\\rightarrow$      $a$                  $d\\rightarrow$      $c(\\nwarrow)$      The “confess” action is the dominant one.Nash equilibriumCan be found by iterated elimination of strictly dominated strategies (IESDS). For player1: check the 1st num up-and-down. For player2: check 2nd num left-and-right.            Player1\\Player2      Cooperate (Deny)      Defect (Confess)                  Cooperate (Deny)      $b(\\downarrow),b(\\rightarrow)$      $d(\\downarrow),a(\\checkmark)$              Defect (Confess)      $a(\\checkmark),d(\\rightarrow)$      $c(\\checkmark),c(\\checkmark)$      Pareto optimal(check 3 times for each state)            Player1\\Player2      Cooperate (Deny)      Defect (Confess)                  Cooperate (Deny)      $b,b\\ldots (\\checkmark)$      $d,a\\ldots (\\checkmark)$              Defect (Confess)      $a,d\\ldots (\\checkmark)$      $c,c \\ldots(\\nwarrow)$      Stag Hunt  In the simple, matrix-form, two-player Stag Hunt each player makes a choice between a risky action (hunt the stag) and a safe action (forage for mushrooms). Foraging for mushrooms always yields a safe payoff while hunting yields a high payoff if the other player also hunts but a very low payoff if one shows up to hunt alone.      Peysakhovich, Alexander, and Adam Lerer. “Prosocial Learning Agents Solve Generalized Stag Hunts Better than Selfish Ones.” Proceedings of the 17th International Conference on Autonomous Agents and MultiAgent Systems. 2018.    Harsanyi, John C., and Reinhard Selten. “A general theory of equilibrium selection in games.” MIT Press Books 1 (1988).  Stag Hunt is an Assurance Game.Normal form            Player1\\Player2      Cooperate (Hunt)      Defect (Forage)                  Cooperate (Hunt)      $a,a$      $d,b$              Defect (Forage)      $b,d$      $c,c$      $a \\gt b \\ge c \\gt d$.Attractor of player1: Reversed u-like (or n-like)            $a$      $d\\downarrow$                  $b\\uparrow$      $c(\\leftarrow)$      Attractor of player2:            $a$      $b\\leftarrow$                  $d\\rightarrow$      $c(\\uparrow)$      Nash equilibrium            Player1\\Player2      Cooperate (Hunt)      Defect (Forage)                  Cooperate (Hunt)      $a(\\checkmark),a(\\checkmark)$      $d(\\downarrow),b(\\leftarrow)$              Defect (Forage)      $b(\\uparrow),d(\\rightarrow)$      $c(\\checkmark),c(\\checkmark)$      Pareto optimal            Player1\\Player2      Cooperate (Hunt)      Defect (Forage)                  Cooperate (Hunt)      $a,a\\ldots(\\checkmark)$      $d,b\\ldots(\\leftarrow)$              Defect (Forage)      $b,d\\ldots(\\uparrow)$      $c,c\\ldots(\\nwarrow)$      Snowdrift  The name Snowdrift game refers to the situation of two drivers caught with their cars in a snow drift. If they want to get home, they have to clear a path. The fairest solution would be for both of them to start shoveling (we assume that both have a shovel in their trunk). But suppose that one of them stubbornly refuses to dig. The other driver could do the same, but this would mean sitting through a cold night. It is better to shovel a path clear, even if the shirker can profit from it without lifting a finger.  Sigmund, Karl. The calculus of selfishness. Princeton University Press, 2010.This game has the same payoff pattern as the games Chicken and Hawk–Dove. Alternatively, I could say that these two games share the same potential game?Normal form            Player1\\Player2      Cooperate      Defect                  Cooperate      $b,b$      $c,a$              Defect      $a,c$      $d,d$      $a \\gt b \\gt c \\gt d$.Attractor of player1: Reversed n-like (or u-like)            $b\\downarrow$      $c(\\leftarrow)$                  $a$      $d\\uparrow$      Attractor of player2:            $b\\rightarrow$      $a$                  $c(\\uparrow)$      $d\\leftarrow$      Nash equilibrium            Player1\\Player2      Cooperate      Defect                  Cooperate      $b(\\downarrow),b(\\rightarrow)$      $c(\\checkmark),a(\\checkmark)$              Defect      $a(\\checkmark),c(\\checkmark)$      $d(\\uparrow),d(\\leftarrow)$      Pareto optimal            Player1\\Player2      Cooperate      Defect                  Cooperate      $b,b\\ldots(\\checkmark)$      $c,a\\ldots(\\checkmark)$              Defect      $a,c\\ldots(\\checkmark)$      $d,d\\ldots(\\leftarrow\\nwarrow\\uparrow)$      Battle of the SexesAlso known as Bach or Stravinsky.Normal form            Player1\\Player2      Bach      Stravinsky                  Bach      $2,1$      $0,0$              Stravinsky      $0,0$      $1,2$      Nash equilibrium            Player1\\Player2      Bach      Stravinsky                  Bach      $2(\\checkmark),1(\\checkmark)$      $0(\\downarrow),0(\\leftarrow)$              Stravinsky      $0(\\uparrow),(\\rightarrow)0$      $1(\\checkmark),2(\\checkmark)$      Mixed strategy Nash equilibrium            policy             q      1-q                         Player1\\Player2      Bach      Stravinsky              p      Bach      $2,1$      $0,0$              1-p      Stravinsky      $0,0$      $1,2$      If player1 chooses “Bach”, its expected payoff is\\[\\mathbb{E}\\left(r^1\\mid a^1 = \\text{Bach}\\right) = 2q + 0\\cdot (1-q) = 2q.\\]And if it chooses “Stravinsky”, its expected payoff is $(1-q)$. It should be indifferent about playing heads or tails, otherwise it can improve its expected payoff by increasing the probability of the action that causes higher expected payoff. In this way,\\[\\begin{aligned}    \\quad &amp; \\mathbb{E}\\left(r^1\\mid a^1 = \\text{Bach}\\right) =         \\mathbb{E}\\left(r^1\\mid a^1 = \\text{Stravinsky}\\right) \\\\    \\Rightarrow \\quad &amp; 2q = 1-q \\\\    \\Rightarrow \\quad &amp; q = 1/3\\end{aligned}\\]If player2 chooses “Bach”, its expected payoff is\\[\\mathbb{E}\\left(r^2\\mid a^2 = \\text{Bach}\\right) = p.\\]And if it chooses “Stravinsky”, its expected payoff is $(2-2p)$. It should be indifferent about playing heads or tails, otherwise it can improve its expected payoff by increasing the probability of the action that causes higher expected payoff. In this way,\\[\\begin{aligned}    \\quad &amp; \\mathbb{E}\\left(r^2\\mid a^2 = \\text{Bach}\\right) =         \\mathbb{E}\\left(r^2\\mid a^2 = \\text{Stravinsky}\\right) \\\\    \\Rightarrow \\quad &amp; p = 2-2p \\\\    \\Rightarrow \\quad &amp; p = 2/3\\end{aligned}\\]The outcome is            policy             1/3      2/3                         Player1\\Player2      Bach      Stravinsky              2/3      Bach      $2,1$      $0,0$              1/3      Stravinsky      $0,0$      $1,2$      \\[\\mathbb{E}\\left( r^1 \\right) = \\frac{2}{3}\\cdot \\frac{1}{3} \\cdot 2 + \\frac{1}{3}\\cdot \\frac{2}{3} \\cdot 1 = \\frac{2}{3}\\]\\[\\mathbb{E}\\left( r^2 \\right) = \\frac{2}{3}\\cdot \\frac{1}{3} \\cdot 1 + \\frac{1}{3}\\cdot \\frac{2}{3} \\cdot 2 = \\frac{2}{3}\\]And this outcome is worse than any of the pure strategy equilibria.Introducing a Trusted AuthorityMatching PenniesIt is zero-sum (at each entry). Players are fully competitive.Normal form            Player1\\Player2      Heads      Tails                  Heads      $1,-1$      $-1,1$              Tails      $-1,1$      $1,-1$      Mixed strategy Nash equilibriumNo pure strategy works.            policy             q      1-q                         Player1\\Player2      Heads      Tails              p      Heads      $1,-1$      $-1,1$              1-p      Tails      $-1,1$      $1,-1$      If player1 chooses “Heads”, its expected payoff is\\[\\mathbb{E}\\left(r^1\\mid a^1 = \\text{Heads}\\right) = q + (-1)\\cdot (1-q) = 2q-1.\\]And if it chooses “Tails”, its expected payoff is $(-2q+1)$. It should be indifferent about playing heads or tails, otherwise it can improve its expected payoff by increasing the probability of the action that causes higher expected payoff. In this way,\\[\\begin{aligned}    \\quad &amp; \\mathbb{E}\\left(r^1\\mid a^1 = \\text{Heads}\\right) =         \\mathbb{E}\\left(r^1\\mid a^1 = \\text{Tails}\\right) \\\\    \\Rightarrow \\quad &amp; 2q-1 = -2q+1 \\\\    \\Rightarrow \\quad &amp; q = 0.5\\end{aligned}\\]Rock paper scissorsIt is zero-sum (at each entry). Players are fully competitive.Normal form            Player1\\Player2      Rock      Paper      Scissors                  Rock      $0,0$      $-1,1$      $1,-1$              Paper      $1,-1$      $0,0$      $-1,1$              Scissors      $-1,1$      $1,-1$      $0,0$      Nash equilibriumNo pure strategy works.\\[\\begin{aligned}    \\quad &amp; \\mathbb{E}\\left(r^1\\mid a^1 = \\text{Rock}\\right) =         \\mathbb{E}\\left(r^1\\mid a^1 = \\text{Paper}\\right) =         \\mathbb{E}\\left(r^1\\mid a^1 = \\text{Scissors}\\right) \\\\    \\Rightarrow \\quad &amp; q = \\frac{1}{3}.\\end{aligned}\\]Muddy Children PuzzleCheck my other note: Dynamic Epistemic Logic.Trust      In the first stage, the Donor (or Investor) receives a certain endowment by the experimenter, and can decide whether or not to send a part of that sum to the Recipient (or Trustee), knowing that the amount will be tripled upon arrival: each euro spent by the Investor yields three euros on the Trustee’s account.    In the second stage, the Trustee can return some of it to the Investor’s account, on a one-to-one basis: it costs one euro to the Trustee to increase the Investor’s account by one euro.    This ends the game. Players know that they will not meet again.  Ultimatum  The experimenter assigns a certain sum, and the Proposer can offer a share of it to the Responder. If the Responder (who knows the sum) accepts, the sum is split accordingly between the two players, and the game is over. If the Responder declines, the experimenter withdraws the money. Again, the game is over: but this time, neither of the two players gets anything.Braess’s ParadoxA computational example about Braess’s Paradox is in Figure 18.2 of this chapter:  Roughgarden, Tim. “Routing games.” Algorithmic game theory 18 (2007): 459-484.Public Goods v.s. Common RecoursesThese two kinds of games are social dilemmas, i.e., the situations in which individual actions that seem to be rational and in self-interest can lead to collective outcomes that are undesirable for everyone.Public Goods GameSetupIn this game, a group of players is each given a sum of money (or any resource). They are offered the choice to invest any portion of this sum into a common pool. This pool is then multiplied by a factor greater than one (but less than the number of players) and distributed evenly among all players, regardless of their individual contributions.DilemmaThe group’s best outcome is if everyone contributes the maximum amount because this would result in the highest multiplication and distribution to everyone. However, from an individual’s perspective, the best strategy is to contribute nothing and free-ride on the contributions of others. If everyone thinks this way, no one contributes, and the group ends up worse off.Examples  The Snowdrift Game (or Hawk-Dove Game)  The Prisoner’s Dilemma.Common Resources Game (or the Tragedy of the Commons)SetupPlayers have access to a shared resource (like a common grazing field for sheep). Each player decides how many units of the resource (e.g., how many sheep) to use. The resource can regenerate over time, but if overused, it can get depleted.DilemmaIf all players use the resource sustainably, the resource persists, benefiting everyone continuously. However, each player has an incentive to use as much of the resource as quickly as possible to gain immediate benefits, especially before others use it up. If all players act on this individual incentive, the shared resource gets depleted, and everyone ends up worse off in the long run.Differences  Nature of the Good:          Public Goods: These are non-excludable (one person’s use doesn’t exclude another’s use) and non-rivalrous (use by one person doesn’t reduce its availability to others). Examples include clean air, national defense, and public parks.      Common Resources: These are non-excludable but rivalrous. One person’s use directly impacts another person’s ability to use it. Examples include fisheries, forests, and shared pastures.        Primary Challenge:          Public Goods: The challenge is about contributing to the provision of the good. The temptation is to free-ride on others’ contributions.      Common Resources: The challenge is about overuse and depletion of the resource. The temptation is to over-exploit before others do.        Outcome of Selfish Behavior:          Public Goods: If everyone acts selfishly, the public good is under-provided or not provided at all.      Common Resources: If everyone acts selfishly, the common resource is quickly depleted, rendering it unavailable even for future use.      The Cooperative GameDisambiguationThe canonical  definition of the cooperative game in game theory is different from the current common sense of MARL.Cooperative game theory deals with situations where players can benefit by cooperating, and binding agreements are possible. In these games, players form coalitions, and the outcomes depend on the behavior of these coalitions. The primary goal in cooperative games is often to understand how the gains from cooperation should be fairly distributed among the players.Notably, a key difference between cooperative and non-cooperative game theory is the idea of commitment. In cooperative games, it’s assumed that players can make binding commitments to each other, whereas in non-cooperative games, players choose strategies without the possibility of making binding agreements.Formal definitionKey elements of a cooperative game:  Players: A finite set of players $N$.  Value function: Given any subset $S$ of $N$ (a coalition), the value function $v: 2^N \\to \\mathbb{R}$ assigns a real number $v(S)$ representing the total value or utility that the coalition $S$ can achieve by cooperating. Note that $v(\\emptyset) = 0$, meaning the value of an empty coalition is zero.  Characteristic function form: Cooperative games often take this form, where for every subset $S$ of $N$, a value $v(S)$ is specified. The number $v(S)$ represents the payoff that the members of $S$ can guarantee by forming a coalition and excluding all other players.An example: The “Airport Game”Certainly! Let’s delve into a classic example of a cooperative game: The “Airport Game”.Scenario:Imagine there are three airlines: A, B, and C. They are considering building a runway at a shared airport. Each airline can benefit from the runway, but they benefit differently based on the size of their planes and the number of flights they operate. They want to decide how much each should contribute to the construction costs.  Airline A’s planes are large, and it would be willing to pay up to $600,000 for the runway if it had to bear all the costs itself.  Airline B operates smaller planes and would only be willing to pay $300,000.  Airline C operates the smallest planes and would pay just $100,000.The total cost of the runway is $800,000.Coalitional Values:If they cooperate, the total cost can be divided among them. The value each coalition (subset of airlines) can generate (or save) is as follows:  $v(A) = 600,000$  $v(B) = 300,000$  $v(C) = 100,000$  $v(A,B) = 800,000$ (since together they can cover the total cost)  $v(A,C) = 700,000$  $v(B,C) = 400,000$  $v(A,B,C) = 800,000$ (since all three together can cover the total cost)The question now becomes: How should the $800,000 cost be distributed among A, B, and C in a way that reflects their individual benefits and the benefit of their cooperation?Solution Concepts:There are various ways to allocate the costs based on cooperative game solution concepts. One of the most famous methods is the Shapley value, which provides a unique way to fairly allocate the costs based on individual and collective benefits.By computing the Shapley value for this game, one would find a fair division of the costs among A, B, and C.Iterated Prisoner’s Dilemma  Axelrod, Robert, and William D. Hamilton. “The evolution of cooperation.” science 211.4489 (1981): 1390-1396.Successful strategy conditions  Stated by Axelrod. The following conditions are adapted from Wikipedia.  Nice/optimistic: The strategy will not defect before its opponent does.  Retaliating: The strategy must sometimes retaliate. Otherwise it will be exploited by the opponent.  Forgiving: Though players will retaliate, they will cooperate again if the opponent does not continue to defect.  Non-envious: The strategy must not strive to score more than the opponent.Strategies  Tit-for-tat.  Win-stay, lose-switch.  Zero-determinant strategy. (Check my other note.)  Disclaimer: The description of games is from Wikipedia and other sources (books or papers). Corresponding links or references have been provided. The content regarding attractors is my personal understanding."
  },
  
  {
    "title": "Information Design in 10 Minutes",
    "url": "/posts/Information-Design-10min/",
    "categories": "Economics & Game Theory",
    "tags": "tech, game theory, information design, Bayesian persuasion, obedience, multi agents, incentive compatibility",
    "date": "2023-08-10 21:30:00 +0000",
    





    
    "snippet": "  This note provides a brief introduction to the basic concepts of information design. More details can be found in my other note on this topic.  “Sometimes, the truth is not good enough.” — Batman...",
    "content": "  This note provides a brief introduction to the basic concepts of information design. More details can be found in my other note on this topic.  “Sometimes, the truth is not good enough.” — Batman, The Dark Knight (2008).Information design focuses on scenarios of mixed-motive unidirectional communication, where one self-interested sender with informational advantage attempts to persuade a self-interested rational receiver to take actions that the sender prefers.  The “informational advatage” means that the sender has something that the receiver wants to know (i.e. which affects the receiver’s payoff) but cannot know,  “self-interested” refers to the agent being concerned only about its own expected payoff, and  “rational” means that when it believes one action’s payoff is greater than another’s, the agent will choose the action with higher expected payoff.Additionally, information design with a sender and a receiver is known as Bayesian persuasion. And the flow of an one-step Bayesian persuasion process is as follows:  The sender commits a signaling scheme to the receiver. The receiver will use this to calculate its posterior expected payoff. (This is referred to as the commitment assumption.);  The nature generates a state $s$. The sender observes the state $s$ and then samples a message according to the distribution of the committed signaling scheme; and  Receiving the message, the receiver calculates a posterior and chooses an optimal action for itself. Given the current state and the receiver’s chosen action, the sender and the receiver get rewards from the nature.The key to the sender successfully persuading a receiver with whom it has an interest conflict lies in obedience constraints. To introduce it, let’s simplify the problems first.Assuming that the sender’s signal set is equal to the receiver’s action set, the sender’s signals can be interpreted as recommending the receiver to take a specific action. This common assumption is without loss of generality according to the revelation principle, i.e., there is an optimal signaling scheme that does not require more signals than the number of actions available to the receiver.Under this premise, obedience constraints can be formalized as:\\[\\sum\\limits_{s} \\mu_0(s)   \\cdot \\varphi( a\\mid s )  \\cdot \\Big( r^j(s, a) - r^j(s, a') \\Big) \\ge 0,\\]where $s\\in S$ is the state which is only observable by the sender, $\\mu_0$ is a prior distribution which is a common knowledge (both know, both know both know, etc.), $a\\in A$ is the receiver’s action space, $\\varphi$ is the sender’s signaling scheme, and $r^j$ is the receiver’s reward function that depends on the state and the receiver’s chosen action.The obedience constraints ensure that the receiver will definitely follow the sender’s recommendations. A simple derivation is as follows:\\[\\begin{aligned}  &amp; \\sum\\limits_{s} \\mu_0(s)   \\cdot \\varphi( a\\mid s )  \\cdot \\Big( r^j(s, a) - r^j(s, a') \\Big) \\ge 0 \\\\  \\Leftrightarrow &amp;  \\sum\\limits_{s} \\frac{\\mu_0(s) \\cdot \\varphi( a\\mid s )}  { \\sum\\limits_{s'}\\mu_0(s') \\cdot \\varphi( a\\mid s')}  \\cdot \\Big( r^j(s, a) - r^j(s, a') \\Big) \\ge 0 , \\forall a'\\in A.\\\\  \\Leftrightarrow &amp;  \\sum\\limits_{s} \\mu(s\\mid a)  \\cdot \\Big( r^j(s, a) - r^j(s, a') \\Big) \\ge 0 , \\forall a'\\in A.\\\\  \\Leftrightarrow &amp;  \\sum\\limits_{s} \\mu(s\\mid a)  \\cdot r^j(s, a)  \\ge   \\sum\\limits_{s} \\mu(s\\mid a)  \\cdot r^j(s, a'), \\forall a'\\in A.\\end{aligned}\\]where $\\mu$ represents the posterior probability. Therefore, a self-interested and rational receiver will definitely follow the sender’s recommendations, because the posterior expected payoff of the action recommended by the sender is greater than or equal to the posterior expected payoffs of all other actions.This greatly simplifies the problem, allowing the sender to choose the receiver’s action that maximizes its expected payoff, while ensuring that the receiver obeys, and then recommend the receiver to take that action. Thus, the specific representation of the sender’s optimization goal is:\\[\\begin{aligned}\\max\\limits_{\\varphi} \\mathbb{E}_{\\varphi}[\\ r^i(s, a) \\ ],\\;\\;\\textrm{s.t. Obedience Constraints.}\\end{aligned}\\]"
  },
  
  {
    "title": "A Memo on Game Theory",
    "url": "/posts/Game-Theory-Memo/",
    "categories": "Economics & Game Theory",
    "tags": "tech, game theory",
    "date": "2023-08-10 06:30:00 +0000",
    





    
    "snippet": "  This note will be consistently updated.RationalityA rational player is one who chooses his action, to maximize his payoff consistent with his beliefs about what is going on in the game.1      “se...",
    "content": "  This note will be consistently updated.RationalityA rational player is one who chooses his action, to maximize his payoff consistent with his beliefs about what is going on in the game.1      “self-interested” refers to the agent being concerned only about its own expected payoff, and    “rational” means that when it believes one action’s payoff is greater than another’s, the agent will choose the action with higher expected payoff.  Bounded rationality is the idea that rationality is limited when individuals make decisions, and under these limitations, rational individuals will select a decision that is satisfactory rather than optimal.2Normal-form gameDefinition 3.31 A normal-form game includes three components as follows:  A finite set of players, $N = \\set{1,2,\\ldots,n}$.  A collection of sets of pure strategies, $\\set{S_1, S_2, \\ldots, s_n}$.  A set of payoff functions, $\\set{v_1,v_2,\\ldots, v_n}$, each assigning a payoff value to each combination of chosen strategies, that is, a set of functions $v_i: S_1\\times S_2, \\times \\dots \\times S_n \\to \\mathbb{R}$ for each $i\\in N$.Each of the players $i\\in N$ must simultaneously choose a possible strategy $s_i\\in S_i$.1  In my understanding, it’s like $(I,\\set{A^i}_{i\\in I}, \\set{R^i}_{i\\in I})$ in the notation of MARL.Game TreeDefinition 3.13 A tree is a set of nodes and directed edges connecting these nodes suchthat  there is an initial node, for which there is no incoming edge;  for every other node, there is exactly one incoming edge;  for any two nodes, there is a unique path that connect these two nodes.And:  The edges are actions.  Each non-terminal node has been defined with who is to perform the action at this moment.  Payoffs for each player are defined at each terminal node.Information SetDefinition 3.43 An information set is a collection of nodes such that  the same player $i$ is to move at each of these nodes;  the same moves are available at each of these nodes.  In my understanding, the nodes in an information set share the same parent node, and they are at the same depth of the tree. The player taking actions at the current depth cannot figure out which node it is at. It cannot see the opponent’s previous move.Definition 3.53 An information partition is an allocation of each non-terminal node of the tree to an information set; the starting node must be “alone”.  In game theory, an information set represents the information available to a player at a particular point in the game. Specifically, it denotes every decision point the player could be at, given what they know so far. An information set is used in games of imperfect information, where players do not have complete information about the actions previously taken (for example, in card games where some cards are hidden).  Here’s a more formal definition:  An information set for a player in a game is a set of decision points such that:      The player must make a decision at each point in the set.    When the play of the game reaches an information set for a player, that player must choose an action without knowing which particular decision point within the set they are at.    This means that if two decision points are in the same information set, the player must have the same set of actions available at both, and cannot distinguish between these decision points based on past play.  In simpler terms, an information set groups together all the situations a player might be in which are indistinguishable from each other from that player’s perspective. This concept is crucial in designing strategies for games of imperfect information.— Generated by ChatGPT 4Extensive-Form Games  Often credit this paper:H. W. Kuhn. Extensive games and the problem of information. Contributions to the Theory of Games, 2:193–216, 1953.The extensive-form representation of a game contains all the information about the game explicitly, by defining who moves when, what each player knows when he moves, what moves are available to him, and where each move leads to, etc. This is done by use of a game tree and information sets–as well as more basic information such as players and the payoffs.3Definition 3.33 (Extensive form) A Game consists of  a set of players,  a tree,  an allocation of non-terminal nodes of the tree to the players,  an informational partition of the non-terminal nodes, and  payoffs for each player at each terminal node.In my understanding, the nodes in extensive-form games are not equivalent to the states in MDP. Because the nodes are not Markovian.  I’m so confused. What should the corresponding quantity of “state” be? This could be a gap between the MARL and Game Theory frameworks.Information: Complete v.s. Perfect  I’ve looked up these two concepts so many times already, but every time I read about them, I forget shortly afterward. 😅Complete InformationA game of complete information requires that the following four components be common knowledge among all the players of the game:1  all the possible actions of all the players,  all the possible outcomes,  how each combination of actions of all players affects which outcome will materialize, and  the preferences of each and every player over outcomes.Preferences describe how the player ranks the set of possible outcomes, from most desired to least desired.1  In my understanding, a game is with complete information if the utility functions (with their domain) are common knowledge. Generally, rationality is a fundamental assumption, so the fourth point in the definition is usually satisfied.Each player has full information about others. Common knowledge includes4  utility functions (including risk aversion),  payoffs,  strategies, and  “types” of players (“private” information).  I’m not sure what strategies being common knowledge looks like. And it reads that Chess is with incomplete information because of this. I think there is a conflict with the definition in the book.But4  players may not know the others’ actions (e.g. the initial placement of ships in  Battleship), and  the game may has chance element (card games).Perfect InformationDefinition 7.31 A game of complete information in which every information set is a singleton and there are no moves of Nature is called a game of perfect information.Each player is perfectly informed of all the events that have previously occurred.5 There is no hidden information.Examples4 5  Perfect and complete: Chess, Tic-Tac-Toe, Go.  Perfect but incomplete: Bayesian game.  Complte but imperfect: Card games, where each player’s cards are hidden from other players but objectives are known. The dealing event is not public (imperfect).Social Choice FunctionIt aims to align the interests of all agents.Price of Anarchy v.s. Price of Stability  PoA = Price of Anarchy          Anarchy = 无政府主义、混乱      如果不要中心化的authority来组织，那要付出多大的代价（相比性能下降多少）（我的理解）        notation          game $G = (N,S,u)$      玩家有$N$个，单个某个玩家记为$i$      单个某个玩家的策略集合为$S_i$（这里只假定做的策略是确定性的，只选择某一个动作）      单个某个玩家的收益为$u_i:S\\to \\mathbb{R}$；其中$S=S_1\\times \\ldots\\times S_N$        PoA衡量“玩家的自私行为”恶化“系统表现”的“效率”          “系统”指的是所有玩家构成的那个抽象的整体（我的理解）      “系统表现”根据我们的目的来定义，记为$\\text{Welf}:S\\to\\mathbb{R}$，比如                  social welfare（utilitarian objective）：$\\text{Welf}=\\sum\\limits_{i\\in N}u_i(s)$；所有玩家收益的和          fairness（egalitarian objective）：$\\text{Welf}=\\min\\limits_{i\\in N}u_i(s)$          系统表现是我们想控制得到的最大化的目标          如果是想最小化某个目标，那就应该是$\\text{Cost}$                      社会困境：个体只注重自己收益期望的优化不一定能让系统表现优化          如果有中心化的authority：把所有人作为一个抽象的整体来分析，可以让某些人牺牲之类的，这样比较好分析得到一个$\\text{Welf}$很高的$s_{\\text{centralized}}$（或多个）      如果没有中心化的authority，每个人只优化自己，最终的结果会到达equilibrium，也就是某个$s_{\\text{equilibrium}}$（比如常见的Nash equilibria）。equilibria集合记为$\\text{Equil}$。      $\\text{PoA} = \\frac{\\max\\limits_{s\\in S}\\text{Welf}(s)} {\\min\\limits_{s\\in \\text{Equil}}\\text{Welf}(s)}$，分母是fully decentralized设置下的最坏情况的收益。（也有分子分母反过来定义的说法）      $\\text{PoA} = \\frac{\\max\\limits_{s\\in \\text{Equil}}\\text{Cost}(s)} {\\min\\limits_{s\\in S}\\text{Cost}(s)}$      另一个概念是PoS，the Price of Stability，      $\\text{PoS} = \\frac{\\max\\limits_{s\\in S}\\text{Welf}(s)} {\\max\\limits_{s\\in \\text{Equil}}\\text{Welf}(s)}$，分子和PoA一样，分母是fully decentralized设置下的最好情况的收益。      $\\text{PoS} = \\frac{\\min\\limits_{s\\in \\text{Equil}}\\text{Cost}(s)} {\\min\\limits_{s\\in S}\\text{Cost}(s)}$      根据定义，$\\text{PoA}\\ge\\text{PoS}\\ge1$，wiki里是这么说的，是负数我不知道咋办，比如：分母&lt;0&lt;分子&lt;-分母，这样加绝对值也不对。        例子：Prisoner’s Dilemma          希望最大化的东西是social walfare，即，我们想优化$\\text{Welf}=u_1(s_1,s_2)+u_2(s_1,s_2)$      如果有中心化的authority，则最优的情况肯定是$(s_1=\\text{Cooperate},s_2=\\text{Cooperate})$，此时$\\text{Welf}=2b$。      如果没有中心化的authority，每个人只优化自己，那么此时对于两个玩家来说，$\\text{Defect}$都是占优策略（别人选择合作，那我背叛收益高；别人选择背叛，那我背叛收益高；不论别人怎么选择，我选背叛收益高）。会收敛到$(s_1=\\text{Defect},s_2=\\text{Defect})$的Nash equilibrium。$\\text{Welf}$相比有中心化的情况，降低了。      $\\text{PoA} = \\frac{2b}{2c}=\\frac{b}{c}$，其跟表中的数值有关。      Shapley ValueIn my understanding, the Shapley value reflects how significant an individual’s effective contribution is to the total value of a certain coupling in a group. A real-world application is to determine how to distribute money: those who work more and contribute more effectively to the group should receive more money. It is a fair distribution method.Given a cooperative game with a set $N$ of players and a value function $v: 2^N \\to \\mathbb{R}$, which assigns a value to each coalition of players, the Shapley value of player $i$, denoted as $\\phi_i(v)$, is defined as:\\[\\begin{aligned}    \\phi_i(v)     &amp;= \\mathbb{E}_{s\\sim \\mathcal{P}(N\\setminus \\{i\\})}\\left[v(S \\cup \\{i\\}) - v(S) \\right] \\\\    &amp;= \\sum_{S \\subseteq N \\setminus \\{i\\}} \\frac{\\vert S\\vert! (\\vert N\\vert - \\vert S\\vert - 1)!}{\\vert N\\vert!} \\left[v(S \\cup \\{i\\}) - v(S)\\right]\\end{aligned}\\]Where:  $N$ is the set of all players.  $S$ is a subset of players not including player $i$.  $\\vert S\\vert$ is the number of players in subset $S$.  $\\vert N\\vert$ is the total number of players.  $v(S)$ is the value function, i.e., the value of coalition $S$.  The term $\\left[v(S \\cup {i}) - v(S)\\right]$ represents the marginal contribution of player $i$ to coalition $S$.  $\\mathcal{P}(N \\setminus {i})$ is the power set of $N \\setminus {i}$, which includes all possible subsets of it, ranging from the empty set to $N \\setminus {i}$ itself. And $S \\sim \\mathcal{P}(N \\setminus {i})$ means that the subset $S$ is uniformly randomly chosen from $\\mathcal{P}(N \\setminus {i})$.In words, the Shapley value of player $i$ is the average of its marginal contributions over all possible coalitions.RegretPotential GameBackward InductionForward InductionSolution ConceptNash equilibriumCorrelated equilibriumCoarse correlated equilibriumSubgame perfect equilibriumPerfect Bayesian equilibriumEfficiency  Perhaps the central theme of economic theory, efficiency is concerned with the optimal allocation of scarce resources. Efficiency is achieved when some specific criterion is maximized and no allocation of resources could yield a higher value according to that criterion. Usually, efficiency measures, such as Pareto optimality and Hicks optimality, attempt to maximize either individual or communal payoffs.— From this webpage.Hicks Optimal  Named after John Hicks, Hicks optimality is a measure of efficiency. An outcome of a game is Hicks optimal if there is no other outcome that results in greater total payoffs for the players. Thus, a Hicks optimal outcome is always the point at which total payoffs across all players is maximized. A Hicks optimal outcome is always Pareto optimal.— From this webpage.Pareto Optimal  Named after Vilfredo Pareto, Pareto optimality is a measure of efficiency. An outcome of a game is Pareto optimal if there is no other outcome that makes every player at least as well off and at least one player strictly better off. That is, a Pareto Optimal outcome cannot be improved upon without hurting at least one player. Often, a Nash Equilibrium is not Pareto Optimal implying that the players’ payoffs can all be increased.— From this webpage.Risk  In game theory, risk refers to the uncertainty about the outcomes of a decision. It’s the possibility that the actual outcome of a decision or an action might differ from the expected outcome. This uncertainty can arise from a variety of sources, such as incomplete information, unpredictable behavior of other players, or random events.— ChatGPT 4.Risk-Averse PlayersThese players prefer a certain outcome over a gamble with a higher expected value. They tend to avoid risks and might settle for a lower but guaranteed benefit.Risk-Seeking PlayersIn contrast, risk-seeking players prefer taking risks even when the expected value of the gamble is less than the certain outcome. They are attracted to the potential of higher gains.Risk-Neutral PlayersThese players are indifferent to risk. They make decisions based solely on expected values, without regard to the variability of outcomes.The canonical RL agents are risk-neutral.Von Neumann–Morgenstern Utility TheoremReferences  Disclaimer: The above content is summarized from Wikipedia and other sources. Corresponding links or references have been provided.            Tadelis, Steven. Game theory: an introduction. Princeton university press, 2013. &#8617; &#8617;2 &#8617;3 &#8617;4 &#8617;5 &#8617;6              Wikipedia: Bounded Rationality. &#8617;              MIT OpenCourseWare: 14.12 Economic Applications of Game Theory. &#8617; &#8617;2 &#8617;3 &#8617;4 &#8617;5              Wikipedia: Complete Information. &#8617; &#8617;2 &#8617;3              Wikipedia: Perfect Information. &#8617; &#8617;2      "
  },
  
  {
    "title": "Fictitious Self-Play and Zero-Shot Coordination",
    "url": "/posts/Fictitious-Self-Play-Zero-Shot-Coordination/",
    "categories": "Artificial Intelligence, Multi-Agent Reinforcement Learning",
    "tags": "tech, game theory, fictitious play, self-play, multi agents, reinforcement learning",
    "date": "2023-07-31 18:40:00 +0000",
    





    
    "snippet": "Fictitious Play  Fictitious play is a learning rule.  In it, each player presumes that the opponents are playing stationary (possibly mixed) strategies.  At each round, each player thus best respon...",
    "content": "Fictitious Play  Fictitious play is a learning rule.  In it, each player presumes that the opponents are playing stationary (possibly mixed) strategies.  At each round, each player thus best responds to the empirical frequency of strategy of their opponent.          Belief of the opponent’s strategy. Average of the history actions. Monte-Carlo style.      Best response.      (Wikipedia)ProcessGiven a game with $n$ players, where each player $i$ has a strategy $\\pi^i$.  Players initialize their beliefs about the strategies of the other players. And $\\hat{\\pi}^{i,j}$ means the belief of $i$ regarding the $j$’s strategy.  At each round $t$:          Player $i$ observes the action $a_{t-1}^j$ (or pure strategy) of every other player $j \\neq i$.      Player $i$ updates their belief of player $j$’s strategy based on the empirical frequency: $\\hat{\\pi}_t^{i,j} = \\frac{1}{t-1} \\sum_{k=1}^{t-1} a_k^j$. The actions are one-hot encoded.      Player $i$ then plays a best response to $\\hat{\\pi}_t^{i,j}$.      Convergence?Fictitious play doesn’t always converge to a Nash equilibrium in all games. It’s been proven that fictitious play converges to the set of Nash equilibria in certain classes of games, like zero-sum games and potential games. However, there are games where fictitious play does not necessarily converge to a Nash equilibrium.E.g., in the Matching Pennies game, the trajectory of Fictitious Play tends to circle around the mixed-strategy Nash equilibrium rather than directly converging to it.Self-PlaySelf-play involves an agent (or a model) playing against itself or versions of itself. This can be thought of as a kind of bootstrapping method where an agent learns and refines its strategies through continuous iterations of gameplay against its own evolving strategies.Process  Initialization: Start with a randomly initialized agent or a naive version.  Play: Let the agent play games against itself or against past versions of itself.  Update: After each game, or a batch of games, update the agent’s model based on the results, rewards, and feedback from the games.  Iterate: Repeat the Play and Update steps for a desired number of iterations or until the agent’s strategy converges to an optimal or satisfactory level.Significance  Unlimited Opponents: Self-play provides an infinite supply of opponents since the agent is essentially playing against itself. This eliminates the need for external opponent data or human opponents, which can be limited or introduce variability.  Evolving Difficulty: As the agent improves, its past versions also present incrementally challenging opponents, ensuring that the agent is always pushed to improve and adapt.  Consistency: By playing against itself, the agent is exposed to a consistent level of gameplay, which can lead to more stable learning.ApplicationsThe most famous application of self-play is perhaps in the training of AlphaGo and its successors by DeepMind. AlphaGo utilized self-play to achieve superhuman performance in the game of Go, a feat that was previously thought to be decades away. Following this, AlphaZero utilized a generalized version of this self-play approach to achieve superhuman performance not only in Go but also in Chess and Shogi.Zero-Shot CoordinationDefinitionZero-shot coordination is about developing agents capable of coordinating in the testing phase with some other agents (even humans) that they have never seen in the training phase.  No Prior Coordination: Agents do not have the opportunity to coordinate or train to learn how to work together before starting the task. Essentially, agents start “from scratch” without any prior coordination strategies.  No Communication: Agents cannot communicate with each other during task execution.The self-play is not good enoughSelf-play is a bootstrapping method. And  It assumes sample is representative: Bootstrapping works under an assumption based on the original sample data, i.e., the sample data is randomly drawn and is representative of the population. If the original sample is not a good representative of the population, bootstrapping can yield misleading results.  It may cuases potential overfitting: Given that bootstrapping is based on multiple resamples of the sample data, it may overemphasize certain features or outliers in the sample, leading to overfitting.In the current setting, the self-play agent’s policy may converge to overfit its own policy, thus forming a specialized convention.After training with self-play, agents might develop different conventions if there are multiple maxima in the sense of the joint policy, which can potentially be symmetric to each other. Thus, without a good way to break the symmetries, agents may fail to coordinate.PerspectivesIn my understanding, the zero-shot coordination problem exists beacuse:  There are symmetric descriptions in the tasks. Some quantities are not (and should not be) labeled.  There are multiple maxima in the optimization problem of the task. Different pairs of agent cannot know which maxima their partner is at.  In different runs of the same experimental code, the agents may have different random seeds.  Self-play is a biased learning rule.Example: LeverIt is a matrix game. There are two agents with the same action space, each of size $m$. If the two agents choose the same actions, then they each receive a reward of 1; otherwise, they get 0.                   $a_1$      $a_2$      $a_3$                  $a_1$      (1,1)      (0,0)      (0,0)              $a_2$      (0,0)      (1,1)      (0,0)              $a_3$      (0,0)      (0,0)      (1,1)      If the two agents are trained through self-play successfully, then they will both choose the same action, a convention influenced by the initial random seeds used in the training process. Different pairs may converge to different outcomes, thus agents from different pairs may fail to coordinate.By the way, the mixed strategy Nash equilibrium is $\\left(\\frac{1}{3}, \\frac{1}{3}, \\frac{1}{3}\\right).$ The calculation is like the Matching Pennies case.In the paper “Other-Play,” which proposed this task, there is another version of it:                   $a_1$      $a_2$      $a_3$                  $a_1$      (1,1)      (0,0)      (0,0)              $a_2$      (0,0)      (1,1)      (0,0)              $a_3$      (0,0)      (0,0)      (0.9,0.9)      And the authors claimed that the most robust strategy is for everyone to choose the action $a_3$, which which would result in a payoff expectation $0.9.$ Otherwise, some pairs may choose $a_1$ and the others may choose $a_2$, it would lead to a payoff expectation $0.5.$  @fortytwo6364: But 0.9 is actually not the only unique answer to the lever problem, there is a unique 1.0 lever directly opposite to the .9 lever. This is stable to the symmetries implied by the problem statement where both players are shown the same set of levers, as well as being robust to different reflections and rotations being presented to different players (though not arbitrary permutations). So assuming whoever I am paired with is also optimizing we would both earn 1. This strategy doesn’t work if there are an odd number of buttons to begin with.The task introduced in the paper includes an illustration where the actions are circled up. However, the authors claim that these actions are not labeled. A more accurate description would be that the actions are uniformly sampled within a closed space, meaning that they cannot be identified by their positions.  In my understanding, despite agents in zero-shot coordination never having interacted with others before, the designer assumes they are aware of the presence of others participating in the same task and are rational (and with some level of theory of mind). There is no free lunch here.Equivalence mappingThis definition used to describe the symmetry is from the paper “other-play” and it is based on the Dec-POMDPs. It resembles the one used for finding Nash equilibria in large games like poker.An equivalence mapping of can find the states and actions that share the same reward, transition probability, and observation. Formally,\\[\\begin{aligned}  \\phi \\in \\Phi \\iff &amp; P(\\phi(s')\\mid\\phi(s),\\phi(a))   = P(s'\\mid s,a) \\\\   &amp;\\land R(\\phi(s'), \\phi(a), \\phi(s)) = R(s', a, s) \\\\  &amp;\\land O(\\phi(o)\\mid\\phi(s), \\phi(a), i) = O(o\\mid s, a, i) \\\\  &amp;\\forall s', s, a, o, i.\\end{aligned}\\]And it’s a shorthand for $\\phi = {\\phi_S, \\phi_A, \\phi_O}.$ It thus can be extended to trajectories and the policies.  \\[\\phi(\\tau_{t}^{i}) = \\{\\phi(o_{0}^{i}), \\phi(a_{0}^{i}), \\phi(r_{0}), \\ldots , \\phi(o_{t}^{i})\\},\\]    \\[\\pi' = \\phi(\\pi) \\iff \\pi'(\\phi(a)\\mid\\phi(\\tau)) = \\pi(a\\mid\\tau), \\forall \\tau, a.\\]  In this way, a Dec-POMDP with two players has the following properties.  \\[J(\\pi_A, \\pi_B) = J(\\phi(\\pi_{A}^1), \\phi(\\pi_{B}^2)), \\forall \\phi \\in \\Phi, \\pi_A, \\pi_B.\\]    \\[\\{ \\phi \\cdot \\phi' : \\phi' \\in \\Phi \\} = \\Phi, \\quad \\forall \\phi \\in \\Phi.\\]  In my understanding, the \\cdot here means function composition. That is, $(\\phi \\cdot \\phi’)(x)$ means $\\phi(\\phi’(x))$.Evaluation: Cross-Play  The algorithm aimed to be tested is written in code, used to train the agents.  The code runs the traning experiment multiple times, each time with a different random seed.  The agents in different runs (with different random seeds) are required to play the coordination task in the test time.Success in cross-play is a necessary condition for the algorithm to achieve a level of zero-shot coordination.Illustration of Cross-Play from the paper “A New Formalism, Method and Open Issues for Zero-Shot Coordination.“Other-PlayHu, Hengyuan, et al. ““other-play” for zero-shot coordination.” International Conference on Machine Learning. PMLR, 2020.  Other-play is a learning rule.  The optimization problem explicitly involves considerations related to policy symmetry.Consider a Dec-POMDP with two players, the optimization problem in the sense of the other-play is\\[\\arg\\max\\limits_{\\boldsymbol\\pi} \\mathbb{E}_{\\phi \\sim \\Phi}\\left[J(\\pi^1, \\phi(\\pi^2)) \\right],\\]where the distribution is unifrom, and $\\mathbb{E}_{\\phi \\sim \\Phi}\\left[J(\\pi^1, \\phi(\\pi^2)) \\right]$ is denoted as $J_{OP}(\\boldsymbol\\pi).$  The expected OP return of $\\boldsymbol{\\pi}$ is equal to the expected return of each player independently playing a policy $\\pi_\\Phi^i$ which is the uniform mixture of $\\phi(\\pi^i)$ for all $\\phi\\in\\Phi.$\\[\\begin{aligned}  \\mathbb{E}_{\\phi \\sim \\Phi}\\left[J(\\pi^1, \\phi(\\pi^2)) \\right]  =&amp; \\mathbb{E}_{\\phi_1 \\sim \\Phi, \\phi_2 \\sim \\Phi}\\left[J(\\phi_1(\\pi^1), \\phi_1(\\phi_2(\\pi^2))) \\right] \\\\  =&amp; \\mathbb{E}_{\\phi_1 \\sim \\Phi, \\phi_2 \\sim \\Phi}\\left[J(\\phi_1(\\pi^1), (\\phi_2(\\pi^2))) \\right] \\\\\\end{aligned}\\]  The distribution $\\boldsymbol{\\pi}^*_{OP}$ produced by OP will be the uniform mixture $\\boldsymbol{\\pi}_{\\Phi}$ with the highest return $J(\\boldsymbol{\\pi}_{\\Phi}).$This means the optimal solution $\\boldsymbol{\\pi}^*_{OP} = \\arg\\max\\limits_{\\boldsymbol\\pi} \\mathbb{E}_{\\phi \\sim \\Phi}\\left[J(\\pi^1, \\phi(\\pi^2)) \\right]$ is the uniform mixture of the maxima of $\\boldsymbol{\\pi}^* = \\arg\\max\\limits_{\\boldsymbol\\pi} J(\\pi^1, \\pi^2).$OP’s best response is also OP. Since OP is a learning rule, the equilibrium reached can be seen as a kind of meta-equilibrium.Simplified Action DecoderHu, Hengyuan, and Jakob N. Foerster. “Simplified Action Decoder for Deep Multi-Agent Reinforcement Learning.” International Conference on Learning Representations. 2019.Motivation  Fundamentally, RL requires agents to explore in order to discover good policies. However, when done naively, this randomness will inherently make their actions less informative to others during training.The more random the signal is, the less informative it will be.In my experience, this can be easily seen by the Recommendation Letter example from information design, or by the differential privacy.A communication protocol is a mapping $f:X\\to Y,$ where $X$ is the observation set, $Y$ is the signal set, and they are two random variables. “Informative” here refers to the degree to which $Y$ is related to $X.$  If they are the same then the agent reveals all of the information.  If $Y$ is irrelevant to $X$ then the agent reveals no information.Thus the necessary randomness for exploration (of the sender) is harmful to the communication, as it might cause the receiver to lose faith in the sender and eventually learn to ignore the signals.In the paper, equations $(3)$ to $(7)$ demonstrate that $\\epsilon$-greedy exploration is harmful.Technique list  CTDE (TODO)  Joint Q-functions (VDN, QMIX) (TODO)  Recurrent DQN  Auxiliary tasks  Theory of mind and Bayesian reasoningBayesian reasoningThe basic setting is the Dec-POMDPs.  The complete trajectory $\\tau = (s_0, \\boldsymbol{a}_0, r_1, \\ldots, r_T, S_T).$  The partially observable trajectory of agent $i$ is $\\tau^i = (o_0^i, \\boldsymbol{a}_0, r_1, \\ldots, r_T, S_T).$          The observation $o_t^i = O(s_t,i)$ is deterministic.      The agents are fully coorperative and they share the same reward at each timestep.      Each agent will calculate its prior guess about the complete trajectory, based on its own trajectory. That is $P(\\tau_t\\mid \\tau_t^i),$ and it is denoted as $B(\\tau_t).$If the agent can observe another agent’s action $a_t^j$, then it can update its belief.\\[\\begin{aligned}  P(\\tau_t \\mid \\tau_t^i, a_t^j)   =&amp; \\frac{P(a_t^j\\mid \\tau_t)\\cdot P(\\tau_t\\mid \\tau_t^i)}  {P(a_t^j \\mid \\tau_t^i)}  = \\frac{P(a_t^j\\mid \\tau_t)\\cdot P(\\tau_t\\mid \\tau_t^i)}  {\\sum\\limits_{\\tau_t'} P(a_t^j \\mid \\tau_t')\\cdot P(\\tau_t'\\mid \\tau_t^i)} \\\\  =&amp; \\frac{\\pi^j\\left(a_t^j\\mid O(\\tau_t,j) \\right)\\cdot B(\\tau_t)}{\\sum\\limits_{\\tau_t'} \\pi^j\\left(a_t^j\\mid O(\\tau_t',j) \\right)\\cdot B(\\tau_t')}\\end{aligned}\\]$\\pi^j\\left(a_t^j\\mid O(\\tau_t,j) \\right)$ represents how agent $i$ perceives the strategy of agent $j$. And this is the theory of mind.  Agent $i$ needs to have access to the policy of agent $j$ during training. Can be justified by CTDE.  If this explicit belief is inputed into the network then it will cause higher order beliefs.  The authors use RNN to learn it implicitly. (How? TODO.)Simplified beliefIf agent $j$’s policy is with the $\\epsilon$-greedy exploration, then agent $i$’s belief of it is like\\[\\pi^j\\left(a_t^i\\mid O(\\tau_t, j)\\right) = (1-\\epsilon)\\cdot \\mathbf{I}\\left(a^*(\\tau_t) = a_t^j\\right) + \\epsilon / \\vert A \\vert,\\]where $\\mathbf{I}\\left(a^*(\\tau_t) = a_t^j\\right)$ is the indicator function (one-hot encoded), and $a^*(\\tau_t) = \\arg\\max\\limits_{a} Q^j\\left(O(\\tau_t, a), a\\right)$.Then, equations $(3)$ to $(7)$ demonstrate that $\\epsilon$-greedy exploration is harmful, by substitute it into the equation\\[\\begin{aligned}  P(\\tau_t \\mid \\tau_t^i, a_t^j)   =&amp; \\frac{\\pi^j\\left(a_t^j\\mid O(\\tau_t,j) \\right)\\cdot B(\\tau_t)}{\\sum\\limits_{\\tau_t'} \\pi^j\\left(a_t^j\\mid O(\\tau_t',j) \\right)\\cdot B(\\tau_t')}.\\end{aligned}\\]Now, if agent $i$ can get agent $j$’s greedy action (in the centralized training phase), then $i$’s posterior can be\\[\\begin{aligned}  P(\\tau_t \\mid \\tau_t^i, a^{j*})   =&amp; \\frac{\\mathbf{I}\\left(a^{j*}(\\tau_t) = a^{j*}\\right)\\cdot B(\\tau_t)}  {\\sum\\limits_{\\tau'} \\mathbf{I}\\left(a^{j*}(\\tau') = a^{j*}\\right)\\cdot B(\\tau')}.\\end{aligned}\\]Note that the agent $i$ only takes $j$’s greedy action into account. And this stabilizes the training process.Trajectory DiversityAndrei Lupu, Brandon Cui, Hengyuan Hu, Jakob Foerster. “Trajectory diversity for zero-shot coordination.” International conference on machine learning. PMLR, 2021.Off-Belief LearningHengyuan Hu, Adam Lerer, Brandon Cui, Luis Pineda, Noam Brown, Jakob Foerster. “Off-belief learning.” International Conference on Machine Learning. PMLR, 2021.Population-Based TrainingZhao, Rui, et al. “Maximum entropy population-based training for zero-shot human-ai coordination.” Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 37. No. 5. 2023.  The following part has not been finished yet. One may check my writing schedule.To-ReadJakob Foerster  SAD [code]Hu, Hengyuan, and Jakob N. Foerster. “Simplified Action Decoder for Deep Multi-Agent Reinforcement Learning.” International Conference on Learning Representations. 2019.  Other-Play [code]Hu, Hengyuan, et al. ““other-play” for zero-shot coordination.” International Conference on Machine Learning. PMLR, 2020.  Trajectory DiversityAndrei Lupu, Brandon Cui, Hengyuan Hu, Jakob Foerster. “Trajectory diversity for zero-shot coordination.” International conference on machine learning. PMLR, 2021.  Off-Belief LearningHengyuan Hu, Adam Lerer, Brandon Cui, Luis Pineda, Noam Brown, Jakob Foerster. “Off-belief learning.” International Conference on Machine Learning. PMLR, 2021.Tencent  Maximum Entropy Population-Based TrainingZhao, Rui, et al. “Maximum entropy population-based training for zero-shot human-ai coordination.” Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 37. No. 5. 2023.Env  A matrix-game [code][paper]          communication through action challenge      BAD        Lever [code][paper]          Other-Play        Corridor [paper]          Trajectory Diversity        Overcooked [code]  Hanabi [code] [paper]"
  },
  
  {
    "title": "Details on the Analysis of Policy Gradient Methods",
    "url": "/posts/Policy-Gradient-Details/",
    "categories": "Artificial Intelligence, Reinforcement Learning",
    "tags": "tech, convergence, policy gradient, reinforcement learning, tech",
    "date": "2023-07-24 18:40:00 +0000",
    





    
    "snippet": "  The only way to make sense out of change is to plunge into it, move with it, and join the dance. — Alan Watts.Policy Gradient Theorem  The proofs of the stochastic and deterministic policy gradie...",
    "content": "  The only way to make sense out of change is to plunge into it, move with it, and join the dance. — Alan Watts.Policy Gradient Theorem  The proofs of the stochastic and deterministic policy gradient theorem are mainly summarized from this blog and the supplementary of the paper “Deterministic Policy Gradient Algorithms,” respectively.Stochastic Policy GradientObjectitve\\[\\max\\limits_{\\theta} J(\\theta) = \\mathbb{E}_{s_0\\sim d_0}\\left[ \\textcolor{red}{V^{\\pi_\\theta} (s_0)} \\right].\\]Gradient\\[\\begin{aligned}\\nabla_\\theta V^{\\pi_\\theta}(s_0) =&amp; \\frac{1}{(1-\\gamma)} \\sum\\limits_{s} d^{\\pi_\\theta}(s\\mid s_0) \\sum\\limits_{a} Q^{\\pi_\\theta}(s,a) \\cdot \\nabla_\\theta \\pi_\\theta(a\\mid s) \\\\=&amp; \\frac{1}{(1-\\gamma)} \\sum\\limits_{s} d^{\\pi_\\theta}(s\\mid s_0) \\sum\\limits_{a} \\pi(a\\mid s) \\cdot Q^{\\pi_\\theta}(s,a) \\cdot \\nabla_\\theta \\ln \\pi_\\theta(a\\mid s) \\\\=&amp; \\frac{1}{(1-\\gamma)} \\mathbb{E}_{s \\sim d^{\\pi_\\theta}(\\cdot \\mid s_0)} \\mathbb{E}_{a\\sim \\pi(\\cdot \\mid s)} \\left[Q^{\\pi_\\theta}(s,a) \\cdot \\nabla_\\theta \\ln \\pi_\\theta(a\\mid s)\\right] \\end{aligned}\\]Proof\\[\\begin{aligned}\\nabla_\\theta V^\\pi(s) =&amp; \\nabla_\\theta \\sum\\limits_{a} \\pi(a\\mid s) \\cdot Q^\\pi(s,a) \\\\=&amp; \\sum\\limits_{a} \\left[Q^\\pi(s,a) \\cdot \\nabla_\\theta\\pi(a\\mid s) + \\pi(a\\mid s) \\cdot \\nabla_\\theta Q^\\pi(s,a)\\right] \\\\=&amp; \\sum\\limits_{a} \\left[Q^\\pi(s,a) \\cdot \\nabla_\\theta\\pi(a\\mid s) + \\pi(a\\mid s) \\cdot \\nabla_\\theta \\sum\\limits_{s',r} P(s',r\\mid s,a)\\cdot \\left(r+ \\gamma \\cdot V^\\pi(s')\\right)\\right] \\\\=&amp; \\sum\\limits_{a} \\left[Q^\\pi(s,a) \\cdot \\nabla_\\theta\\pi(a\\mid s) + \\gamma \\cdot  \\pi(a\\mid s) \\cdot \\sum\\limits_{s'} P(s'\\mid s,a)\\cdot \\nabla_\\theta V^\\pi(s')\\right] \\\\=&amp; \\sum\\limits_{a} Q^\\pi(s,a) \\cdot \\nabla_\\theta\\pi(a\\mid s) + \\gamma \\sum\\limits_{s',a} \\pi(a\\mid s) \\cdot  P(s'\\mid s,a)\\cdot \\nabla_\\theta V^\\pi(s') \\\\\\end{aligned}\\]Note that the action $a$ is sampled from the parameterized policy $\\pi_\\theta.$ Thus $\\nabla_\\theta a$ is $0,$ without using the gumbel-softmax technique. In the deterministic policy, $\\nabla_\\theta a$ is not $0,$ and thus the derivation is different.\\[\\begin{aligned}\\nabla_\\theta V^\\pi(s) =&amp; \\sum\\limits_{a} Q^\\pi(s,a) \\cdot \\nabla_\\theta\\pi(a\\mid s) + \\sum\\limits_{s',\\textcolor{red}{a}} \\textcolor{red}{\\gamma \\cdot \\pi(a\\mid s) \\cdot  P(s'\\mid s,a)} \\cdot \\nabla_\\theta V^\\pi(s') \\\\=&amp; \\sum\\limits_{a} Q^\\pi(s,a) \\cdot \\nabla_\\theta\\pi(a\\mid s) + \\sum\\limits_{s'} \\textcolor{red}{\\gamma \\cdot \\mathrm{Pr}(s\\to s', k=1, \\pi_\\theta)} \\cdot \\nabla_\\theta V^\\pi(s') \\\\=&amp; \\sum\\limits_{a} Q^\\pi(s,a) \\cdot \\nabla_\\theta\\pi(a\\mid s) + \\sum\\limits_{s'} \\gamma \\cdot \\mathrm{Pr}(s\\to s', k=1, \\pi_\\theta) \\left[\\sum\\limits_{a} Q^\\pi(s',a) \\cdot \\nabla_\\theta\\pi(a\\mid s') + \\sum\\limits_{s''} \\gamma \\cdot \\mathrm{Pr}(s'\\to s'', k=1, \\pi_\\theta)\\cdot \\nabla_\\theta V^\\pi(s'')\\right] \\\\=&amp; \\sum\\limits_{a} Q^\\pi(s,a) \\cdot \\nabla_\\theta\\pi(a\\mid s) + \\gamma \\sum\\limits_{s'} \\mathrm{Pr}(s\\to s', k=1, \\pi_\\theta) \\sum\\limits_{a} Q^\\pi(s',a) \\cdot \\nabla_\\theta\\pi(a\\mid s') \\\\\t&amp;+ \\textcolor{red}{\\gamma^2 \\sum\\limits_{s'} \\mathrm{Pr}(s\\to s', k=1, \\pi_\\theta) \\sum\\limits_{s''} \\mathrm{Pr}(s'\\to s'', k=1, \\pi_\\theta)}\\cdot \\nabla_\\theta V^\\pi(s'') \\\\=&amp; \\sum\\limits_{a} Q^\\pi(s,a) \\cdot \\nabla_\\theta\\pi(a\\mid s) + \\gamma \\sum\\limits_{s'} \\mathrm{Pr}(s\\to s', k=1) \\sum\\limits_{a} Q^\\pi(s',a) \\cdot \\nabla_\\theta\\pi(a\\mid s') \\\\\t&amp;+ \\textcolor{red}{\\gamma^2 \\sum\\limits_{s'} \\mathrm{Pr}(s\\to s'', k=2, \\pi_\\theta) }\\cdot \\nabla_\\theta V^\\pi(s'') \\\\=&amp;\\ldots \\\\=&amp; \\sum\\limits_{x\\in S} \\textcolor{blue}{\\sum\\limits_{k=0}^\\infty \\gamma^k \\cdot \\mathrm{Pr}(s\\to x, k, \\pi_\\theta)} \\cdot \\sum\\limits_{a} Q^\\pi(x,a) \\cdot \\nabla_\\theta\\pi(a\\mid x)\\end{aligned}\\]The blue part is defined as the discounted state visitation distribution \\(d^{\\pi_\\theta}(s\\mid s_0) = (1-\\gamma )\\cdot \\sum\\limits_{k=0}^\\infty \\gamma^k \\cdot \\mathrm{Pr}(s_0\\to s, k, \\pi_\\theta).\\)\\[\\begin{aligned}\\sum\\limits_{k=0}^\\infty \\gamma^k \\cdot \\mathrm{Pr}(s_0\\to s, k, \\pi_\\theta) \\le \\sum\\limits_{k=0}^\\infty \\gamma^k = \\frac{1}{1-\\gamma}\\end{aligned}\\]The distribution should beshould lie within the range of $[0,1]$ and thus the coefficient $(1-\\gamma)$ is is for normalization.\\[\\begin{aligned}\\nabla_\\theta V^{\\pi_\\theta}(s_0) =&amp; \\textcolor{blue}{\\frac{1}{(1-\\gamma)}} \\sum\\limits_{s} \\textcolor{blue}{d^{\\pi_\\theta}(s\\mid s_0)} \\sum\\limits_{a} Q^{\\pi_\\theta}(s,a) \\cdot \\nabla_\\theta \\pi_\\theta(a\\mid s) \\\\=&amp; \\frac{1}{(1-\\gamma)} \\sum\\limits_{s} d^{\\pi_\\theta}(s\\mid s_0) \\sum\\limits_{a} \\pi(a\\mid s) \\cdot Q^{\\pi_\\theta}(s,a) \\cdot \\nabla_\\theta \\ln \\pi_\\theta(a\\mid s) \\\\=&amp; \\frac{1}{(1-\\gamma)} \\mathbb{E}_{s \\sim d^{\\pi_\\theta}(\\cdot \\mid s_0)} \\mathbb{E}_{a\\sim \\pi(\\cdot \\mid s)} \\left[Q^{\\pi_\\theta}(s,a) \\cdot \\nabla_\\theta \\ln \\pi_\\theta(a\\mid s)\\right] &amp; \\blacksquare\\end{aligned}\\]Deterministic Policy GradientBasics  $a = \\mu_\\theta (s)$  $V^{\\mu_\\theta}(s) = Q^{\\mu_\\theta}(s,a) = Q^{\\mu_\\theta}(s,\\mu_\\theta(s))$  $\\nabla_\\theta a \\ne 0$          $\\nabla_\\theta r(s,a)\\ne 0$      $\\nabla_\\theta P(s’\\mid s,a)\\ne 0$        $P(s’\\mid s,\\mu_\\theta(s)) = \\mathrm{Pr}(s\\to s’, k=1, \\mu_\\theta)$Gradient\\[\\begin{aligned}\\nabla_\\theta V^{\\mu_\\theta}(s) =&amp; \\frac{1}{(1-\\gamma)}\\int_{x\\in S} d^{\\mu_\\theta}(s) \\cdot \\nabla_\\theta \\mu_\\theta(x)\\cdot \\nabla_a Q^{\\mu_\\theta} (x, \\mu_\\theta(x))\\Big\\vert_{a=\\mu_\\theta(x)} \\,\\mathrm{d} x \\\\=&amp;\\frac{1}{(1-\\gamma)} \\mathbb{E}_{s\\sim d^{\\mu_\\theta}} \\bigg[ \\nabla_\\theta \\mu_\\theta(x)\\cdot \\nabla_a Q^{\\mu_\\theta} (x, \\mu_\\theta(x))\\Big\\vert_{a=\\mu_\\theta(x)}\\bigg]\\end{aligned}\\]Proof\\[\\begin{aligned}\\nabla_\\theta V^{\\mu_\\theta}(s) =&amp; \\textcolor{blue}{\\nabla_\\theta Q^{\\mu_\\theta} (s, \\mu_\\theta(s))} \\\\=&amp; \\nabla_\\theta \\left( r(s,\\mu_\\theta(s)) + \\gamma \\int_S P\\left(s'\\mid s,\\mu_\\theta(s)\\right)\\cdot V^{\\mu_\\theta}(s') \\,\\mathrm{d} s' \\right) \\\\=&amp; \\textcolor{red}{\\nabla_\\theta r(s,\\mu_\\theta(s))} + \\gamma \\int_S V^{\\mu_\\theta}(s') \\cdot \\textcolor{red}{\\nabla_\\theta P\\left(s'\\mid s,\\mu_\\theta(s)\\right)} \\,\\mathrm{d} s' + \\gamma \\int_S P\\left(s'\\mid s,\\mu_\\theta(s)\\right)\\cdot \\nabla_\\theta V^{\\mu_\\theta}(s') \\,\\mathrm{d} s' \\\\=&amp; \\nabla_\\theta \\mu_\\theta(s)\\cdot \\textcolor{blue}{\\nabla_a} \\left( r(s,\\mu_\\theta(s)) + \\gamma \\int_S \\textcolor{blue}{V^{\\mu_\\theta}(s')} \\cdot P\\left(s'\\mid s,\\mu_\\theta(s)\\right) \\,\\mathrm{d} s' \\right)\\Bigg\\vert_{a=\\mu_\\theta(s)} + \\gamma \\int_S P\\left(s'\\mid s,\\mu_\\theta(s)\\right)\\cdot \\nabla_\\theta V^{\\mu_\\theta}(s') \\,\\mathrm{d} s' \\\\=&amp; \\textcolor{blue}{\\nabla_\\theta \\mu_\\theta(s)\\cdot \\nabla_a Q^{\\mu_\\theta} (s, \\mu_\\theta(s))\\Big\\vert_{a=\\mu_\\theta(s)}} + \\gamma \\int_S \\textcolor{green}{P\\left(s'\\mid s,\\mu_\\theta(s)\\right)}\\cdot \\nabla_\\theta V^{\\mu_\\theta}(s') \\,\\mathrm{d} s' \\\\=&amp; \\nabla_\\theta \\mu_\\theta(s)\\cdot \\nabla_a Q^{\\mu_\\theta} (s, \\mu_\\theta(s))\\Big\\vert_{a=\\mu_\\theta(s)} + \\gamma \\int_S \\textcolor{green}{\\mathrm{Pr}(s\\to s', k=1, \\mu_\\theta)} \\cdot \\nabla_\\theta V^{\\mu_\\theta}(s') \\,\\mathrm{d} s' \\\\=&amp; \\nabla_\\theta \\mu_\\theta(s)\\cdot \\nabla_a Q^{\\mu_\\theta} (s, \\mu_\\theta(s))\\Big\\vert_{a=\\mu_\\theta(s)} + \\gamma \\int_S \\mathrm{Pr}(s\\to s', k=1, \\mu_\\theta) \\cdot \\left( \\nabla_\\theta \\mu_\\theta(s')\\cdot \\nabla_a Q^{\\mu_\\theta} (s', \\mu_\\theta(s'))\\Big\\vert_{a=\\mu_\\theta(s')} + \\gamma \\int_S \\mathrm{Pr}(s'\\to s'', k=1, \\mu_\\theta) \\cdot \\nabla_\\theta V^{\\mu_\\theta}(s'') \\,\\mathrm{d} s'' \\right) \\,\\mathrm{d} s' \\\\=&amp; \\nabla_\\theta \\mu_\\theta(s)\\cdot \\nabla_a Q^{\\mu_\\theta} (s, \\mu_\\theta(s))\\Big\\vert_{a=\\mu_\\theta(s)} + \\gamma \\int_S \\mathrm{Pr}(s\\to s', k=1, \\mu_\\theta) \\cdot \\nabla_\\theta \\mu_\\theta(s')\\cdot \\nabla_a Q^{\\mu_\\theta} (s', \\mu_\\theta(s'))\\Big\\vert_{a=\\mu_\\theta(s')} \\,\\mathrm{d} s' \\\\\t&amp;+ \\gamma^2 \\int_S \\mathrm{Pr}(s\\to s', k=1, \\mu_\\theta) \\int_S \\mathrm{Pr}(s'\\to s'', k=1, \\mu_\\theta) \\cdot \\nabla_\\theta V^{\\mu_\\theta}(s'') \\,\\mathrm{d} s'' \\,\\mathrm{d} s' \\\\=&amp;\\ldots \\\\=&amp; \\int_{x\\in S} \\sum\\limits_{k=0}^\\infty \\gamma^k \\cdot \\mathrm{Pr}(s\\to x, k, \\mu_\\theta) \\cdot \\nabla_\\theta \\mu_\\theta(x)\\cdot \\nabla_a Q^{\\mu_\\theta} (x, \\mu_\\theta(x))\\Big\\vert_{a=\\mu_\\theta(x)} \\,\\mathrm{d} x \\\\=&amp; \\frac{1}{(1-\\gamma)}\\int_{x\\in S} d^{\\mu_\\theta}(s) \\cdot \\nabla_\\theta \\mu_\\theta(x)\\cdot \\nabla_a Q^{\\mu_\\theta} (x, \\mu_\\theta(x))\\Big\\vert_{a=\\mu_\\theta(x)} \\,\\mathrm{d} x \\\\=&amp;\\frac{1}{(1-\\gamma)} \\mathbb{E}_{s\\sim d^{\\mu_\\theta}} \\bigg[ \\nabla_\\theta \\mu_\\theta(x)\\cdot \\nabla_a Q^{\\mu_\\theta} (x, \\mu_\\theta(x))\\Big\\vert_{a=\\mu_\\theta(x)}\\bigg]\\end{aligned}\\]Note that the Bellman equation here is different from the one in the stochastic case: the reward is not dependent on the next state. $\\blacksquare$  Calculating $\\nabla_a Q(s,a)$ is the result of accounting for both $\\nabla_\\theta r(s,a)$ and $\\nabla_\\theta p(s’\\mid s,a)$.Performance Difference LemmaFor all policies $\\pi, \\pi^\\prime$ and states$s_0$,\\[\\begin{aligned} V^\\pi(s_0) - V^{\\pi^\\prime}(s_0) =&amp; \\mathbb{E}_{\\tau \\sim {\\Pr}^\\pi(\\tau|s_0=s) } \\left[\\sum_{t=0}^\\infty \\gamma^t A^{\\pi'}(s_t,a_t)\\right] \\\\ =&amp; \\frac{1}{1-\\gamma}\\mathbb{E}_{s\\sim d_{s_0}^\\pi }\\mathbb{E}_{a\\sim \\pi(\\cdot|s) } \\left[  A^{\\pi^\\prime}(s,a)\\right]. \\end{aligned}\\]  Kakade, Sham, and John Langford. “Approximately optimal approximate reinforcement learning.” Proceedings of the Nineteenth International Conference on Machine Learning. 2002.ProofThe proof is provided in the appendix of “On the theory of policy gradient methods: Optimality, approximation, and distribution shift” and I just transcribed it here with additional details.Let $\\Pr^\\pi(\\tau \\mid s_0 = s)$ denote the probability of observing a trajectory $\\tau$ when starting in state $s$ and following the policy $\\pi$. Using a telescoping argument, we have:\\[\\begin{aligned}&amp;V^\\pi(s) - V^{\\pi'}(s) \\\\=&amp;  \\mathbb{E}_{\\tau \\sim {\\Pr}^\\pi(\\tau|s_0=s) }\\left[\\sum_{t=0}^\\infty \\gamma^t r(s_t,a_t)\\right] - V^{\\pi'}(s) \\\\=&amp; \\mathbb{E}_{\\tau \\sim {\\Pr}^\\pi(\\tau|s_0=s) }\\left[\\sum_{t=0}^\\infty \\gamma^t \\left(r(s_t,a_t)+V^{\\pi'}(s_t)-V^{\\pi'}(s_t) \\right)\\right]-V^{\\pi'}(s)\\\\\\stackrel{(a)}{=}&amp; \\mathbb{E}_{\\tau \\sim {\\Pr}^\\pi(\\tau|s_0=s) }    \\left[\\sum_{t=0}^\\infty \\gamma^t \\left(r(s_t,a_t)+\\gamma V^{\\pi'}(s_{t+1})-V^{\\pi'}(s_t)\\right)\\right]\\\\\\stackrel{(b)}{=}&amp;\\mathbb{E}_{\\tau \\sim {\\Pr}^\\pi(\\tau|s_0=s) }    \\left[\\sum_{t=0}^\\infty \\gamma^t \\left(r(s_t,a_t)+\\gamma \\mathbb{E}[V^{\\pi'}(s_{t+1})|s_t,a_t]-V^{\\pi'}(s_t)\\right)\\right]\\\\\\stackrel{(c)}{=}&amp; \\mathbb{E}_{\\tau \\sim {\\Pr}^\\pi(\\tau|s_0=s) }    \\left[\\sum_{t=0}^\\infty \\gamma^t A^{\\pi'}(s_t,a_t)\\right] \\\\=&amp; \\frac{1}{1-\\gamma}\\mathbb{E}_{s'\\sim d^\\pi_s }\\,\\mathbb{E}_{a\\sim \\pi(\\cdot | s')}    \\left[ A^{\\pi'}(s',a) \\right],\\end{aligned}\\]where $(a)$ rearranges terms in the summation and cancels the $V^{\\pi’}(s_0)$ term with the $-V^{\\pi’}(s)$ outside the summation, and $(b)$ uses the tower property of conditional expectations and the final equality follows from the definition of $d^\\pi_s$. $\\blacksquare$Details$(a)$:\\[- a_0 +\\sum\\limits_{k=0}^{\\infty} \\left(a_k - b_k \\right) = \\sum\\limits_{k=0}^{\\infty} \\left(a_{k+1} - b_k\\right).\\]$(b)$: The tower property of conditional expectations (or law of total probability):If $\\mathcal{H} \\subseteq \\mathcal{G}$, then\\[\\mathbb{E}\\left[\\mathbb{E}\\left[X\\mid \\mathcal{G} \\right] \\mid \\mathcal{H} \\right] = \\mathbb{E}\\left[X\\mid \\mathcal{H} \\right].\\]Correspondingly,  $\\mathcal{G} = \\tau \\sim {\\Pr}^\\pi(\\tau \\mid s_0=s)$,  $\\mathcal{H} = (s_t,a_t)$.$(c)$: Step $(b)$ is necessary. Note that\\[Q^{\\pi}(s, a) \\ne r(s, a) + \\gamma \\cdot V^{\\pi}(s').\\]But\\[Q^{\\pi}(s, a) = r(s, a) + \\gamma \\cdot \\sum\\limits_{s'} P(s' \\mid s,a) \\cdot V^{\\pi}(s').\\]Other proofs  Check other proofs here and here.###ConvergenceAboutThis section is based on the awesome paper:  Agarwal, Alekh, et al. “On the theory of policy gradient methods: Optimality, approximation, and distribution shift.” The Journal of Machine Learning Research 22.1 (2021): 4431-4506.And I will provide some omitted details here. The writing of the entire note may be somewhat verbose, and this is to familiarize myself with the content.Details of Setting$V(s) \\le \\frac{1}{1-\\gamma}$$V(s)$ reaches its upper bound when $r(s,a)=1,\\forall s,a$, which equals $\\sum\\limits_{t=0}^\\infty \\gamma^t$.And it is a geometric progression:  $a_n = a_0 \\cdot \\gamma^{n-1}$,  $S_n = a_0 \\cdot \\frac{1-\\gamma^n}{1-\\gamma}$,          $S_n = a_0 + a_1 + \\ldots + a_n$,      $\\gamma\\cdot S_n = a_1 + \\ldots + a_n + a_{n+1}$,      $(1-\\gamma)\\cdot S_n = a_0\\cdot (1 - \\gamma^n)$.        $\\lim\\limits_{n\\to\\infty}S_n = \\frac{a_0}{1-\\gamma} = \\frac{1}{1-\\gamma}$.The famous theorem of Bellman and Dreyfus (1959)  The famous theorem of Bellman and Dreyfus (1959) shows there exists a policy $\\pi^\\star$ which simultaneously maximizes $V^\\pi(s_0)$, for all states $s_0\\in S$.I have read this referenced paper, and I do not find any theorem. This paper is mainly about trading additional computing time for additional memory capacity.However this statement is intuitive and is not hard to understand. Assume there is a fixed $s_{-1}$ and can be transited to $s_0$ according to $\\rho$, then this problem is equivalent to the one that has a fixed $s_0$.Direct parameterization$\\theta\\in\\Delta(A)^{\\vert S\\vert}$ means for every state $s$ the parameters are  a point in a simplex.For eample, for state $s_0$, there are actions $a_1, a_2$, the parameters of the current policy $\\pi_\\theta(\\cdot \\mid s_0)$ are  $\\theta_{s_0,a_1} = 0.2 = \\pi_\\theta(a_1 \\mid s_0)$, and  $\\theta_{s_0,a_2} = 0.8 = \\pi_\\theta(a_2 \\mid s_0)$.Softmax parameterizationSometimes it can be $\\pi_\\theta(a\\mid s) = \\frac{\\exp(\\tau\\cdot \\theta_{s,a})}{\\sum\\limits_{a’}\\exp(\\tau\\cdot \\theta_{s,a’})}$, which is called energy-based policy, where $\\tau$ is the temperature parameter (inverse temperature) and $\\theta_{s,a}$ is the energy function.  Haarnoja, Tuomas, et al. “Reinforcement learning with deep energy-based policies.” International conference on machine learning. PMLR, 2017.$V^{\\pi_\\theta}(s)$ is non-concave (Lemma 1)We want to maximize $V^{\\pi_\\theta}(s)$, so if $V^{\\pi_\\theta}(s)$ is concave then we can apply standard tools of convex optimization.  Unfortunately it is not.As shown in the appendix, there is a MDP where exists policy points $\\pi_1, \\pi_2$ that $V^{\\pi_1}(s)+V^{\\pi_2}(s)&gt; 2\\cdot V^{\\frac{1}{2}(\\pi_1+\\pi_2)}(s)$. This shows a property of convex, so $V^{\\pi_\\theta}(s)$ is non-concave.Why is there a coefficient $(1-\\gamma)$ in $(4)$?\\[d_{s_0}^\\pi(s) := (1-\\gamma) \\sum_{t=0}^\\infty \\gamma^t {\\Pr}^\\pi(s_t=s|s_0).\\]Recall that the derivation of the policy gradient theorem:\\[\\nabla_{\\theta} V^{\\pi_\\theta}(s_0) = \\sum\\limits_{s} \\sum\\limits_{k=0}^{\\infty} \\gamma^k \\cdot \\text{Pr}(s_0\\to s, k) \\sum\\limits_{a} \\pi(a\\mid s) \\cdot Q^\\pi(s,a)\\cdot \\nabla_{\\theta}\\ln \\pi(a\\mid s).\\]  Policy Gradient      Williams, Ronald J. “Simple statistical gradient-following algorithms for connectionist reinforcement learning.” Machine learning 8 (1992): 229-256.    Sutton, Richard S., et al. “Policy gradient methods for reinforcement learning with function approximation.” Advances in neural information processing systems 12 (1999).  Note that $\\lim\\limits_{k\\to\\infty} \\sum\\limits_{k=0}^{\\infty} \\gamma^k = \\frac{1}{1-\\gamma} &gt; 1$. The value of discounted state visitation distribution should not larger than $1$. So the coefficient $(1-\\gamma)$ is for normalization.Why is there a coefficient $\\frac{1}{1-\\gamma}$ in $(5)$?\\[\\begin{aligned}\\nabla_\\theta V^{\\pi_\\theta}(s_0) =&amp; \\frac{1}{1-\\gamma} \\, \\mathbb{E}_{s \\sim d_{s_0}^{\\pi_\\theta} }\\mathbb{E}_{a\\sim \\pi_\\theta(\\cdot | s) }\\big[\\nabla_\\theta \\log\\pi_{\\theta}(a| s) Q^{\\pi_\\theta}(s,a)\\big] \\\\=&amp; \\sum\\limits_{s} d^\\pi_{s_0}(s) \\sum\\limits_{a} \\pi(a\\mid s) \\cdot Q^\\pi(s,a)\\cdot \\nabla_{\\theta}\\log \\pi(a\\mid s).\\end{aligned}\\]It is used to cancel that normalization.Advantage\\[\\begin{aligned}A^{\\pi}(s,a):=&amp; Q^\\pi(s,a)-V^\\pi(s) \\\\=&amp; Q^\\pi(s,a) - \\sum\\limits_{a}\\pi(a\\mid s) \\cdot Q^\\pi(s,a).\\end{aligned}\\]Given $s$ and $\\pi$, $A^{\\pi}(s,a)$ measures how much better the expected future return after selecting action $a$ is compared to the expected future return of sampling action based on the current policy $\\pi$ in this state $s$.Baseline  This part partially use material from Prof. Wang’s Lecture note 18: Variance reduction and Reinforcement learning: An introduction.Policy gradient is unbiased but with high variance. Recall that the form is\\[\\nabla_{\\theta} V^{\\pi_\\theta}(s_0) =\\frac{1}{1-\\gamma} \\mathbb{E}_{s\\sim d_{s_0}^\\pi}\\mathbb{E}_{a\\sim\\pi(\\cdot\\mid s)}\\left[ Q^\\pi(s,a)\\cdot \\nabla_{\\theta}\\log \\pi(a\\mid s)\\right].\\]To reduce it, a natural solution is to subtract a baseline $b(s)$ from $Q^\\pi$ which can be any function, even a random variable, as long as it does not depend on the action $a$, i.e.,\\[\\nabla_{\\theta} V^{\\pi_\\theta}(s_0) =\\frac{1}{1-\\gamma} \\mathbb{E}_{s\\sim d_{s_0}^\\pi}\\mathbb{E}_{a\\sim\\pi(\\cdot\\mid s)}\\left[ \\left(Q^\\pi(s,a) - b(s)\\right)\\cdot \\nabla_{\\theta}\\log \\pi(a\\mid s)\\right],\\]\\[\\nabla_{\\theta} V^{\\pi_\\theta}(s_0) = \\sum\\limits_{s} d^\\pi_{s_0}(s) \\sum\\limits_{a} \\pi(a\\mid s) \\cdot \\left(Q^\\pi(s,a) - b(s)\\right)\\cdot \\nabla_{\\theta}\\log \\pi(a\\mid s),\\]or\\[\\nabla_{\\theta} V^{\\pi_\\theta}(s_0) = \\sum\\limits_{s} d^\\pi_{s_0}(s) \\sum\\limits_{a} \\left(Q^\\pi(s,a) - b(s)\\right)\\cdot \\nabla_{\\theta} \\pi(a\\mid s).\\]This is still unbiased:\\[\\begin{aligned}&amp;\\sum\\limits_a b(s)\\cdot \\nabla_{\\theta} \\pi(a\\mid s) \\\\=&amp; b(s) \\cdot\\nabla_\\theta\\sum\\limits_a \\pi(a\\mid s) \\\\=&amp; b(s) \\cdot\\nabla_\\theta 1 \\\\=&amp; 0. \\end{aligned}\\]But it has lower variance:  Assume that:      $X = Q^\\pi(s,a)\\cdot \\nabla_{\\theta} \\pi(a\\mid s)$,    $Y = \\nabla_{\\theta} \\pi(a\\mid s)$,    $\\mathbb{E} \\left[ X \\right] = \\mu$,    $\\mathbb{E} \\left[ Y \\right] = \\eta = 0$,    $X’ = X + c(Y-\\eta)$.    Then:      $\\mathbb{E} \\left[ X’ \\right] = \\mu$,    \\[\\begin{aligned} \\mathbb{V} \\left[ X' \\right] =&amp; \\mathbb{V} \\left[ X + c(Y-\\eta) \\right] \\\\ =&amp; \\mathbb{V} \\left[ X \\right] + c^2\\cdot \\mathbb{V} \\left[ Y-\\eta \\right] +2c\\cdot \\text{Cov}(X,Y-\\eta) \\\\ =&amp; \\mathbb{V} \\left[ Y-\\eta \\right] \\cdot c^2 + 2\\cdot \\text{Cov}(X,Y-\\eta)\\cdot c + \\mathbb{V} \\left[ X \\right],\\end{aligned}\\]        $\\min \\mathbb{V} \\left[ X’ \\right] = \\left(1 - \\text{Corr(X,Y)}\\right)\\cdot \\mathbb{V} \\left[ X \\right]$.    Common usage: GAE (Generalized Advantage Estimation).  Schulman, John, et al. “High-dimensional continuous control using generalized advantage estimation.” arXiv preprint arXiv:1506.02438 (2015).Equation (6) does not hold for the direct parameterization\\[\\begin{aligned}   \\sum\\limits_a \\nabla_\\theta \\pi(a) =&amp; \\left(\\sum\\limits_a\\frac{\\partial \\pi(a)}{\\partial \\theta_1},\\ldots, \\sum\\limits_a\\frac{\\partial \\pi(a)}{\\partial \\theta_m}\\right)\\\\ \\end{aligned}\\]If every $\\frac{\\partial \\pi(a)}{\\partial \\theta_1}$ has the same variables, then $\\sum\\limits_a\\frac{\\partial \\pi(a)}{\\partial \\theta_1} = \\frac{\\partial \\sum\\limits_a\\pi(a)}{\\partial \\theta_1} = 0$. But in the case of the direct parameterization, this assumption does not hold, i.e., $\\sum\\limits_a\\frac{\\partial \\pi(a)}{\\partial \\theta_1} = 1$.Distribution mismatch coefficient (pass)I think this concept is introduced too soon. Let’s discuss it later.Details on Constrained Tabular Parameterization  This algorithm is projected gradient ascent on the direct policy parametrization of the MDP.Equation $(7)$$\\mu$ is a distribution of $s_0$.\\[\\begin{aligned}&amp; \\nabla_\\theta V^{\\pi_\\theta}(\\mu) \\\\=&amp;  \\frac{\\partial V^{\\pi_\\theta}(\\mu)}{\\partial } \\\\=&amp; \\frac{1}{1-\\gamma} \\, \\mathbb{E}_{s \\sim d_{s_0}^{\\pi_\\theta} }\\mathbb{E}_{a\\sim \\pi_\\theta(\\cdot | s) }\\big[\\nabla_\\theta \\log\\pi_{\\theta}(a| s) Q^{\\pi_\\theta}(s,a)\\big] \\\\=&amp; \\sum\\limits_{s} d^\\pi_{s_0}(s) \\sum\\limits_{a} \\pi(a\\mid s) \\cdot Q^\\pi(s,a)\\cdot \\nabla_{\\theta}\\log \\pi(a\\mid s).\\end{aligned}\\]  The following part has not been finished yet. One may check my writing schedule."
  },
  
  {
    "title": "Sequence-to-Sequence Models",
    "url": "/posts/Seq2Seq/",
    "categories": "Artificial Intelligence, Machine Learning Basics",
    "tags": "tech, NLP, RNN, LSTM, GRU, Seq2Seq, transformer, BERT, GPT",
    "date": "2023-07-15 18:40:00 +0000",
    





    
    "snippet": "NLP TermsNLP = Natural Language ProcessingEmbedding  In a general sense, “embedding” refers to the process of representing one kind of object or data in another space or format. It involves mapping...",
    "content": "NLP TermsNLP = Natural Language ProcessingEmbedding  In a general sense, “embedding” refers to the process of representing one kind of object or data in another space or format. It involves mapping objects from a higher-dimensional space into a lower-dimensional space while preserving certain relationships or properties.  — GPT 3.5For example, the Principal Component Analysis (PCA) algorithm is an embedding technique. PCA is a widely used dimensionality reduction technique that projects high-dimensional data into a lower-dimensional space, while retaining as much of the data’s variance as possible. In my understanding, the constraint on variance in PCA is intended to allow us to distinguish each point as effectively as possible in the new lower-dimensional space. And that is the “preserved property” in this case.In NLP, the embedding layer is used to obtain the feature vector for each token in the input sequence. One-hot is a simple example of the embedding.Items $\\to$ one-hot variables:import torchimport torch.nn.functional as FA = torch.tensor([3, 0, 2])output1 = F.one_hot(A)'''tensor([[0, 0, 0, 1],        [1, 0, 0, 0],        [0, 0, 1, 0]])'''output2 = F.one_hot(A, num_classes=5)'''tensor([[0, 0, 0, 1, 0],        [1, 0, 0, 0, 0],        [0, 0, 1, 0, 0]])'''Items $\\to$ learnable embedded variables:import torchtorch.manual_seed(1)words = [\"hello\", \"world\", \"haha\"]vocab_size = len(words)idx = list(range(vocab_size))dictionary = dict(zip(words, idx))  # {'hello': 0, 'world': 1, 'haha': 2}embedding_dim = 2embedding_layer = torch.nn.Embedding(num_embeddings=vocab_size,     # how many values in a dim (input)                                     embedding_dim=embedding_dim)   # output_dimprint(list(embedding_layer.parameters()))'''[Parameter containing:tensor([[ 0.6614,  0.2669],        [ 0.0617,  0.6213],        [-0.4519, -0.1661]], requires_grad=True)]'''lookup_tensor = torch.tensor([dictionary[\"haha\"]], dtype=torch.long)haha_embed = embedding_layer(lookup_tensor)print(haha_embed)# tensor([[-0.4519, -0.1661]], grad_fn=&lt;EmbeddingBackward0&gt;)# The result is exactly the third row of the embedding layer parameter matrix.torch.nn.Embedding:  Each element in the weight matrix is sampled from $\\mathcal{N}(0,1)$.  “An Embedding layer is essentially just a Linear layer.” (From this website.)  The padding_idx parameter: (int, optional) - If specified, the entries at padding_idx do not contribute to the gradient.  PAD_TOKEN, &lt;BOS&gt; and others should be considered in the num_embeddings count.Map back:  Given an embedding result $y$, find the corresponding word $x$.  Calculate the embeddings of all words $(y_1, \\ldots, y_n)$,  $x = \\arg\\min_i \\Vert y - y_i \\Vert$all_embeddings = embedding_layer(torch.arange(vocab_size))distances = torch.norm(all_embeddings - hello_embed, dim=1)min_index = torch.argmin(distances).item()closest_word = [word for word, idx in dictionary.items() if idx == min_index][0]print(f\"Closest word for the given vector: {closest_word}\")# Closest word for the given vector: hahaInteger (index) tensors $\\to$ learnable embedded variables:import torchvocab_size = 10embedding_dim = 3embedding_layer = torch.nn.Embedding(vocab_size, embedding_dim)print(list(embedding_layer.parameters()))'''tensor([[-2.3097,  2.8327,  0.2768],        [-1.8660, -0.5876, -0.5116],        [-0.6474,  0.7756, -0.1920],        [-1.2533, -0.7186,  1.8712],        [-1.5365, -1.0957, -0.9209],        [-0.0757,  2.3399,  0.9409],        [-0.9143,  1.3293,  0.8625],        [ 1.3818, -0.1664, -0.5298],        [ 2.2011, -0.8805,  1.7162],        [-0.9934,  0.3914,  0.9149]], requires_grad=True)]'''# A batch of 2 samples of 4 indices each# Each index is in [0, vocab_size - 1]input = torch.LongTensor([[1, 2, 4, 5],     # Sentence A                          [4, 3, 2, 9]])    # Sentence Boutput = embedding_layer(input)print(output)'''tensor([[[-1.8660, -0.5876, -0.5116],         [-0.6474,  0.7756, -0.1920],         [-1.5365, -1.0957, -0.9209],         [-0.0757,  2.3399,  0.9409]],        [[-1.5365, -1.0957, -0.9209],         [-1.2533, -0.7186,  1.8712],         [-0.6474,  0.7756, -0.1920],         [-0.9934,  0.3914,  0.9149]]], grad_fn=&lt;EmbeddingBackward0&gt;)'''Differentiable embedding  “An Embedding layer is essentially just a Linear layer.” From this website.embedding_layer = torch.nn.Sequential(    torch.nn.Linear(input_dim, output_dim, bias=False),)Tokenization  Check this website.sentence = 'We do not see things as they are, we see them as we are.'character_tokenization = list(sentence)# ['W', 'e', ' ', 'd', 'o', ' ', 'n', 'o', 't', ' ', 's', 'e', 'e', ' ', 't', 'h', 'i', 'n', 'g', 's', ' ', 'a', 's', ' ', 't', 'h', 'e', 'y', ' ', 'a', 'r', 'e', ',', ' ', 'w', 'e', ' ', 's', 'e', 'e', ' ', 't', 'h', 'e', 'm', ' ', 'a', 's', ' ', 'w', 'e', ' ', 'a', 'r', 'e', '.']word_tokenization = sentence.split()# ['We', 'do', 'not', 'see', 'things', 'as', 'they', 'are,', 'we', 'see', 'them', 'as', 'we', 'are.']sentence_tokenization = sentence.split(', ')# ['We do not see things as they are', 'we see them as we are.']BoWBOW = Bag of WordsIn my understanding, BoW is a step  to make and clean a word-level tokenization, and  to count and store the occurrences of each word.  For instance, given the vocabulary {apple, banana, cherry}:      Text: \"apple banana apple\"    BoW representation: {2, 1, 0}    — ChatGPT 4Word2VecWord2Vec = Word to Vector  Word2Vec is a group of related models that are used to produce word embeddings. Word2Vec takes a large corpus of text as its input and produces a high-dimensional space (typically of several hundred dimensions), with each unique word in the corpus being assigned a corresponding vector in the space.  — ChatGPT 4In my understanding:  During training, the process involves presenting a set of words (the context) and predicting the surrounding words.  The model learns from reading the corpus to understand the context in which each word occurs in the language.  If a certain word at a particular position in a context appears frequently in the corpus, then when given that context, the model’s output probability for that word should also be high.CBOWCBOW = Continuous Bag of WordsThe CBOW model predicts the current word based on its context. The objective function to maximize can be expressed as:\\(J(\\theta) = \\frac{1}{T} \\sum_{t=1}^{T} \\log p(w_t | C_t)\\)Skip-GramThe Skip-Gram model predicts the context given a word. Its objective function can be expressed as:\\(J(\\theta) = \\frac{1}{T} \\sum_{t=1}^{T} \\sum_{-c \\leq j \\leq c, j \\neq 0} \\log p(w_{t+j} | w_t)\\)Here, $T$ is the total number of words in the training corpus, and $c$ is the size of the context.Illustration of CBOW and Skip-Gram from the paper “Exploiting Similarities among Languages for Machine Translation”.RNN ModelsRNNRNN = Recurrent Neural Network\\[\\begin{aligned}    \\begin{cases}      h_t = f_{w_h}(x_t, h_{t-1}) \\\\      o_t = g_{w_o}(h_t) \\\\    \\end{cases}\\end{aligned}\\]Illustration of RNN from the book “Dive into Deep Learning”. Boxes represent variables (not shaded) or parameters (shaded) and circles represent operators.If there are $L$ RNN layers, then\\[\\begin{aligned}    \\begin{cases}      h_t^1 = \\mathrm{Sigmoid}\\left(\\mathrm{Linear}(x_t) + \\mathrm{Linear}(h_{t-1}^1)\\right) \\\\      h_t^2 = \\mathrm{Sigmoid}\\left(\\mathrm{Linear}(h_t^1) + \\mathrm{Linear}(h_{t-1}^2)\\right) \\\\      \\ldots \\\\      o_t = \\mathrm{Linear}(h_t^L)    \\end{cases}\\end{aligned}\\]Illustration of Stacked RNN from the paper “RNNCon: Contribution Coverage Testing for Stacked Recurrent Neural Networks”.# Adapted from https://pytorch.org/docs/stable/generated/torch.nn.RNN.htmlimport torchrnn = torch.nn.RNN(10, 20, 2)   # input_size, hidden_size, num_layersinput = torch.randn(5, 3, 10)   # sequence_length, batch_size, input_sizeh0 = torch.randn(2, 3, 20)      # num_layers, batch_size, hidden_sizeoutput, hn = rnn(input, h0)     # h0: Defaults to zeros if not provided.print(output.size(), hn.size())# torch.Size([5, 3, 20]) torch.Size([2, 3, 20])# output size: sequence_length, batch_size, hidden_size# If parameter batch_first=True,# then the first parameter should be the batch_size.In NLP:  An input is a sentence. The sequence_length is the number of words in the sentence.  The input_size is the embedding dimension of each word.  The output_size equals the sequence_length.  All the hidden states have the same hidden_size.An example that makes it easy to remember the dimensions of various quantities:# input_size = embedding_dim = 3embedded_words = {\"I\": [0.3, 0.5, 0.1],                  \"am\": [0.4, 0.4, 0.3],                  \"so\": [0.1, 0.9, 0.2],                  \"happy\": [0.5, 0.3, 0.8],                  \"sad\": [0.2, 0.6, 0.1],                  \".\": [0.6, 0.2, 0.6],                  \"&lt;EOS&gt;\": [0.0, 0.0, 0.0],                  \"&lt;PAD&gt;\": [0.9, 0.9, 0.9]}sentences = [\"I am so happy.\",             \"I am sad.\"]# dataset_size = (batch_size, sequence_length, embedding_dim) = (2, 6, 3,)# sequence_length = max(sentence_length); Paddingembedded_sentences = [[[0.3, 0.5, 0.1],                       [0.4, 0.4, 0.3],                       [0.1, 0.9, 0.2],                       [0.5, 0.3, 0.8],                       [0.2, 0.6, 0.1],                       [0.0, 0.0, 0.0]],                      [[0.3, 0.5, 0.1],                       [0.4, 0.4, 0.3],                       [0.5, 0.3, 0.8],                       [0.6, 0.2, 0.6],                       [0.0, 0.0, 0.0],                       [0.9, 0.9, 0.9]]]# hidden_size, num_layers: determined by parameter tuning :)BPTTBPTT = Backpropagation Through Time\\(L = \\frac{1}{T}\\sum\\limits_{t} l(o_t,y_t)\\)\\(\\begin{aligned}    \\frac{\\partial L}{\\partial w_o} =&amp; \\frac{1}{T}\\sum\\limits_{t} \\frac{\\partial l(y_t,o_t)}{\\partial w_o} \\\\    =&amp; \\frac{1}{T}\\sum\\limits_{t}    \\frac{\\partial l(y_t,o_t)}{\\partial o_t}\\cdot     \\frac{\\partial o_t}{\\partial w_o}\\end{aligned}\\)\\(\\begin{aligned}    \\frac{\\partial L}{\\partial w_h} =&amp; \\frac{1}{T}\\sum\\limits_{t} \\frac{\\partial l(y_t,o_t)}{\\partial w_h} \\\\    =&amp; \\frac{1}{T}\\sum\\limits_{t} \\frac{\\partial l(y_t,o_t)}{\\partial o_t} \\cdot     \\frac{\\partial o_t}{\\partial h_t} \\cdot      \\textcolor{red}{\\frac{\\partial h_t}{\\partial w_h}}\\end{aligned}\\)\\[\\begin{aligned}    \\textcolor{red}{\\frac{\\partial h_t}{\\partial w_h}} =&amp;    \\frac{\\partial f(x_t, h_{t-1}, w_h)}{\\partial w_h} + \\frac{\\partial f(x_t, h_{t-1}, w_h)}{\\partial h_{t-1}} \\cdot     \\textcolor{red}{\\frac{\\partial h_{t-1}}{\\partial w_h}}\\end{aligned}\\]\\[\\begin{cases}      z_0 = a_0 = 0 \\\\      z_k = a_k + b_k \\cdot z_{k-1} \\\\\\end{cases}\\]\\[\\begin{aligned}    z_k = a_k + \\sum\\limits_{i=0}^{k-1}        \\left(\\prod\\limits_{j=i+1}^k b_j \\right) \\cdot a_i\\end{aligned}\\]LSTMLSTM = Long Short-Term Memory  To learn long-term dependencies (owing to vanishing and exploding gradients).  In my understanding, there are two kinds of hidden states, $\\mathrm{c}$ and $\\mathrm{h}$. And $\\mathrm{c}$ is renamed as the memory cell internal state.Each layer is like:\\[\\begin{aligned}  \\begin{cases}      i_t =&amp; \\mathrm{Sigmoid}\\left(\\mathrm{Linear}(x_t) + \\mathrm{Linear}(h_{t-1})\\right) \\\\      f_t =&amp; \\mathrm{Sigmoid}\\left(\\mathrm{Linear}(x_t) + \\mathrm{Linear}(h_{t-1})\\right) \\\\      g_t =&amp; \\mathrm{tanh}\\left(\\mathrm{Linear}(x_t) + \\mathrm{Linear}(h_{t-1})\\right) \\\\      o_t =&amp; \\mathrm{Sigmoid}\\left(\\mathrm{Linear}(x_t) + \\mathrm{Linear}(h_{t-1})\\right) \\\\      c_t =&amp; f_t \\odot c_{t-1} + i_t \\odot g_t \\\\      h_t =&amp; o_t \\odot \\mathrm{tanh}(c_t)  \\end{cases}\\end{aligned}\\]  Adapted from the PyTorch document.  $h_t$: the hidden state.  $c_t$: the cell state.  $i_t$: the input gate.  $f_t$: the forget gate.  $g_t$: the cell gate.  $o_t$: the output gate.  All values of the three gates are in the range of $(0, 1)$ because of the sigmoid function.  $g_t$ is the vanilla part of an RNN, and it indicates the information that we currently get.  $i_t$ controls how much we cares about the current information.  $c$ is an addtional hidden state channel, and it also indicates the memory.  $f_t$ controls how much we cares about the memory.  $c_t$ and $h_t$ do not impact the curren output $o_t$.Illustration of LSTM from the book “Dive into Deep Learning”.# Adapted from https://pytorch.org/docs/stable/generated/torch.nn.LSTM.htmlimport torchrnn = torch.nn.LSTM(10, 20, 2)    # input_size, hidden_size, num_layersinput = torch.randn(5, 3, 10)     # sequence_length, batch_size, input_sizeh0 = torch.randn(2, 3, 20)        # num_layers, batch_size, hidden_sizec0 = torch.randn(2, 3, 20)        # num_layers, batch_size, hidden_sizeoutput, (hn, cn) = rnn(input, (h0, c0)) # h0: Defaults to zeros if not provided.print(output.size(), hn.size(), cn.size())# torch.Size([5, 3, 20]) torch.Size([2, 3, 20]) torch.Size([2, 3, 20])# output size: sequence_length, batch_size, hidden_size# If parameter batch_first=True,# then the first parameter should be the batch_size.GRUGRU = Gated Recurrent UnitsEach layer is like:\\[\\begin{aligned}  \\begin{cases}      r_t =&amp; \\mathrm{Sigmoid}\\left(\\mathrm{Linear}(x_t) + \\mathrm{Linear}(h_{t-1})\\right) \\\\      z_t =&amp; \\mathrm{Sigmoid}\\left(\\mathrm{Linear}(x_t) + \\mathrm{Linear}(h_{t-1})\\right) \\\\      n_t =&amp; \\mathrm{tanh}\\left(\\mathrm{Linear}(x_t) + r_t \\odot \\mathrm{Linear}(h_{t-1})\\right) \\\\      h_t =&amp; (1-z_t)\\odot n_t + z \\odot h_{t-1}  \\end{cases}\\end{aligned}\\]  $h_t$: the hidden state. It can be used as the output.  $r_t$: the reset gate, controls how much we cares about the memory. It is a bit like the forget gate $f_t$ in LSTM  $z_t$: the update gate, controls how much we cares about the current information. It is a bit like the input gate $i_t$ in LSTM.  $n_t$: the candidate hidden state, or the new gate.          If the reset gate $r_t$ is close to $1$, then it is like the vanilla RNN.      If the reset gate $r_t$ is close to $0$, then the new gate $n_t$ is the result of an MLP of $x_t$.      Illustration of GRU from the book “Dive into Deep Learning”.# Adapted from https://pytorch.org/docs/stable/generated/torch.nn.GRU.htmlimport torchrnn = torch.nn.GRU(10, 20, 2)   # input_size, hidden_size, num_layersinput = torch.randn(5, 3, 10)   # sequence_length, batch_size, input_sizeh0 = torch.randn(2, 3, 20)      # num_layers, batch_size, hidden_sizeoutput, hn = rnn(input, h0)     # h0: Defaults to zeros if not provided.print(output.size(), hn.size())# torch.Size([5, 3, 20]) torch.Size([2, 3, 20])# output size: sequence_length, batch_size, hidden_size# If parameter batch_first=True,# then the first parameter should be the batch_size.Encoder-DecoderVariable-length inputs  Truncation and Padding          Dive Into Deep Learning 10.5.3        Relation Network          A blog      ICLR 2017        Embedding.  Encoder-decoder.  In general sequence-to-sequence problems like machine translation (Section 10.5), inputs and outputs are of varying lengths that are unaligned. The standard approach to handling this sort of data is to design an encoder–decoder architecture (Fig. 10.6.1) … — Dive into Deep Learning.The structureIllustration of the encoder-decoder architecture from the book “Dive into Deep Learning”.  Encoder: \"Hello, world.\" $\\to$ a hidden state (or context variable) of fixed-shape.  Decoder 1: the state $\\to$ \"你好，世界。\"  Decoder 2: the state $\\to$ \"Hola mundo\"Illustration of the encoder-decoder architecture (teacher forcing) from the book “Dive into Deep Learning”.Illustration of the encoder-decoder architecture (prediction) from the book “Dive into Deep Learning”.The encoder and the decoder are usually RNNs.  &lt;eos&gt; means the end of the sequence.          Inputting &lt;eos&gt; into the encoder indicates the end of this sentence.      In prediction: When the decoder outputs &lt;eos&gt;, it will automatically stop and no longer continue generating output.        &lt;bos&gt; means the beginning of the sequence, used to signal the decoder when to begin generating a new sequence.  The input of the encoder is a variable-length sequence, but its output is of fixed-length, named as the state or the context variable $c$.  $c = q(h_1, \\ldots, h_t)$, where $q$ is a customized function. In the figures, $c = h_t$.  The context variable will be fed into the decoder at evry time step or at the first time step.  Teacher Forcing: The input of the decoder is (&lt;bos&gt;, sequence), and the target is (sequence, &lt;eos&gt;).  Prediction: The input of the decoder at every time step is the output from the previous time step.  When calculating the loss, the padding tokens are masked.Illustration of the encoder-decoder architecture where the RNNs are stacked, from the book “Dive into Deep Learning”.Teacher forcingTeacher Forcing: The input of the decoder is (&lt;bos&gt;, sequence), and the target is (sequence, &lt;eos&gt;).  Without using teacher forcing, the model at each timestep would receive the output from the previous timestep and use this output to predict the next timestep. However, this approach has an inherent problem: early in training, the model is likely to produce incorrect predictions, leading the next timestep prediction to be based on this incorrect output. Such mistakes can accumulate in subsequent timesteps.  To combat this, the Teacher Forcing technique is introduced during training. Specifically, instead of the model receiving its prediction from the previous timestep, it directly receives the actual output from the previous timestep. In this way, even if the model makes an error at a particular timestep, it can continue making predictions based on the actual data, preventing error accumulation.  — ChatGPT 4Code# Generated by ChatGPT 4import torchimport torch.nn as nnimport torch.optim as optimclass Encoder(nn.Module):    def __init__(self, input_dim, emb_dim, hidden_dim, n_layers, dropout):        super().__init__()        self.embedding = nn.Embedding(input_dim, emb_dim)        self.rnn = nn.RNN(emb_dim, hidden_dim, n_layers, dropout=dropout)        self.dropout = nn.Dropout(dropout)    def forward(self, src):        embedded = self.dropout(self.embedding(src))        outputs, hidden = self.rnn(embedded)        return hiddenclass Decoder(nn.Module):    def __init__(self, output_dim, emb_dim, hidden_dim, n_layers, dropout):        super().__init__()        self.output_dim = output_dim        self.embedding = nn.Embedding(output_dim, emb_dim)        self.rnn = nn.RNN(emb_dim, hidden_dim, n_layers, dropout=dropout)        self.fc_out = nn.Linear(hidden_dim, output_dim)        self.dropout = nn.Dropout(dropout)    def forward(self, input, hidden):        input = input.unsqueeze(0)        embedded = self.dropout(self.embedding(input))        output, hidden = self.rnn(embedded, hidden)        prediction = self.fc_out(output.squeeze(0))        return prediction, hiddenclass Seq2Seq(nn.Module):    def __init__(self, encoder, decoder, device):        super().__init__()        self.encoder = encoder        self.decoder = decoder        self.device = device    def forward(self, src, trg, teacher_forcing_ratio=0.5):        trg_len = trg.shape[0]        batch_size = trg.shape[1]        trg_vocab_size = self.decoder.output_dim        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)        hidden = self.encoder(src)        input = trg[0, :]        for t in range(1, trg_len):            output, hidden = self.decoder(input, hidden)            outputs[t] = output            teacher_force = torch.rand(1).item() &lt; teacher_forcing_ratio            top1 = output.argmax(1)            input = trg[t] if teacher_force else top1        return outputsINPUT_DIM = 1000OUTPUT_DIM = 1000ENC_EMB_DIM = 256DEC_EMB_DIM = 256HID_DIM = 512N_LAYERS = 2ENC_DROPOUT = 0.5DEC_DROPOUT = 0.5device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT)model = Seq2Seq(enc, dec, device).to(device)optimizer = optim.Adam(model.parameters())criterion = nn.CrossEntropyLoss()def train(model, iterator, optimizer, criterion, clip):    model.train()    epoch_loss = 0    count = 0    for i, (src, trg) in enumerate(iterator):        optimizer.zero_grad()        output = model(src, trg)        output_dim = output.shape[-1]        output = output[1:].view(-1, output_dim)        trg = trg[1:].view(-1)        loss = criterion(output, trg)        loss.backward()        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)        optimizer.step()        epoch_loss += loss.item()        count = i    return epoch_loss / (count + 1)def main():    src = torch.randint(0, INPUT_DIM, (10, 32)).to(device)  # sequence length 10, batch szie 32    trg = torch.randint(0, OUTPUT_DIM, (10, 32)).to(device)    dataset = [(src, trg) for _ in range(100)]  # 100 batch    iterator = iter(dataset)    N_EPOCHS = 10    CLIP = 1    for epoch in range(N_EPOCHS):        train_loss = train(model, iterator, optimizer, criterion, CLIP)        print(f'Epoch: {epoch + 1:02} | Train Loss: {train_loss:.3f}')def evaluate(model, iterator, criterion):    model.eval()    epoch_loss = 0    count = 0    with torch.no_grad():        for i, (src, trg) in enumerate(iterator):            output = model(src, trg, 0)  # 0 means not using teacher forcing            output_dim = output.shape[-1]            output = output[1:].view(-1, output_dim)            trg = trg[1:].view(-1)            loss = criterion(output, trg)            epoch_loss += loss.item()            count = i    return epoch_loss / (count + 1)def main_test():    src_test = torch.randint(0, INPUT_DIM, (10, 32)).to(device)  # sequence_length 10,  batchsize 32    trg_test = torch.randint(0, OUTPUT_DIM, (10, 32)).to(device)    test_dataset = [(src_test, trg_test) for _ in range(50)]  # 50 batch    test_iterator = iter(test_dataset)    test_loss = evaluate(model, test_iterator, criterion)    print(f'Test Loss: {test_loss:.3f}')if __name__ == \"__main__\":    main()    main_test()Transformer  The Transformer, BERT, and GPT architectures do not use RNNs. Instead, they rely on the self-attention mechanism to process sequences.  — ChatGPT 4Queries, Keys, and Values  A data set $\\mathcal{D}:={ (k_i, v_i) \\mid i\\in {1, \\ldots, n} }$.          $k$ is the key, $v$ is the value.      It is a dictionary (in python).        We input the query $q$ to search the data set.  The program returns the value most relevant $v_{i^*}$, where $i^* = \\arg\\min_{i} \\Vert q - x_i \\Vert$.keys = range(1, 8, 3)  # [1, 4, 7]values = [\"xixi\", \"haha\", \"wuwu\"]data_set = dict(zip(keys, values))# {1: 'xixi', 4: 'haha', 7: 'wuwu'}def search(query: int):    distances = [abs(query - key_i) for key_i in keys]    idx_optimal = distances.index(min(distances))    key_optimal = keys[idx_optimal]    value_optimal = data_set[key_optimal]    return value_optimalprint(search(query=3))  # hahaAttention  Attention is all you need.\\[\\mathrm{Attention}(q, \\mathcal{D}) := \\sum\\limits_{i=1}^n v_i \\cdot \\alpha(q, k_i)\\]  $\\alpha(q, k_i)$ is usually a function of the distance between $q$ and $k_i$, reflecting their similarity.  $\\boldsymbol\\alpha = (\\alpha(q, k_1), \\ldots, \\alpha(q, k_n))$ should be a convex combination.          $\\alpha(q, k_i) \\ge 0, \\forall i$      $\\sum\\limits_{i=1}^n \\alpha(q, k_i) = 1$        If $\\boldsymbol\\alpha$ is one-hot, then the attention mechanism is just like the traditional database query.Illustration of the attention mechanism from the book “Dive into Deep Learning”.Common similarity kernels\\[\\boldsymbol\\alpha(q, \\boldsymbol{k}) = \\mathrm{softmax}(\\textcolor{blue}{(}f(\\Vert q - k_1 \\Vert), \\ldots, f(\\Vert q - k_n \\Vert)\\textcolor{blue}{)})\\]$f$ is the similarity kernel (or Parzen Windows).  $f(\\Vert q - k \\Vert) = \\exp\\left(-\\frac{1}{2}\\Vert q-k \\Vert^2\\right)$ (Gaussian)  $f(\\Vert q - k \\Vert) = 1 \\mathrm{if} \\Vert q-k \\Vert \\le 1$ (Boxcar)  $f(\\Vert q - k \\Vert) = \\max(0, 1- \\Vert q-k \\Vert )$ (Epanechikov)Attention Scoring FunctionsCalculating the distances between queries and keys costs a lot. So we should find another efficient way to quantify the similarity between them. The function that measures this similarity is named the scoring function.Illustration of the attention mechanism from the book “Dive into Deep Learning”.  The following part has not been finished yet. One may check my writing schedule.BERTBERT = Bidirectional Encoder Representations from TransformersGPTGPT = Generative Pre-trained Transformer"
  },
  
  {
    "title": "MARL Basics",
    "url": "/posts/MARL-Basics/",
    "categories": "Artificial Intelligence, Multi-Agent Reinforcement Learning",
    "tags": "tech, multi agents, reinforcement learning, Markov model",
    "date": "2023-06-29 09:38:00 +0000",
    





    
    "snippet": "  This note has not been finished yet. One may check my writing schedule.Markov Models  MDP          Markov decision process      $(S, A, \\mathcal{P}, R, \\gamma)$      Single-agent, fully observabl...",
    "content": "  This note has not been finished yet. One may check my writing schedule.Markov Models  MDP          Markov decision process      $(S, A, \\mathcal{P}, R, \\gamma)$      Single-agent, fully observable, and dynamic.        POMDP          Partially observable Markov decision process      $(S, A, \\mathcal{P}, R, O, \\mathcal{Q}, \\gamma)$      Single-agent, partially observable, and dynamic.        Dec-POMDP          Decentralized partially observable Markov decision process      $(I, S,\\set{A^i}_{i\\in I}, \\mathcal{P}, R, \\set{O^i}_{i\\in I}, \\mathcal{Q}, \\gamma)$      Multi-agent, fully observable, cooperative, and dynamic.        Stochastic games (or Markov games)          $(I, S,\\set{A^i}_{i\\in I}, \\mathcal{P}, \\set{R^i}_{i\\in I}, \\gamma)$      Multi-agent, fully observable, C/A/M (meaning that it can be cooperative, adversarial, or mixed-motive), and dynamic.      where  $I$ is a set of agents (and $\\vert I\\vert = n$, where $n\\ge 2$),  $S$ is the state space,  $A$ is the action space,  $P: S\\times A \\to \\Delta(S)$ is the state transition function,  $R: S\\times A$  (TODO)  Normal-form game (or repeated games)          $(I,\\set{A^i}_{i\\in I}, \\set{R^i}_{i\\in I})$      Multi-agent, C/A/M, and static.        Extensive-form game          $(I, X, Z, f,\\set{A^i}_{i\\in I}, \\set{R^i}_{i\\in I})$      Multi-agent, C/A/M, and dynamic.        Bayesian gameCheck this note, A Memo on Game Theory.(TODO)Some potentially confusing concepts:  Stochastic game  Markov game  Repeated game  Matrix game  Static game  Dynamic game  Normal-form game  Extensive-form game  Bayesian gameLearning GoalsThe learning goals of MARL are mainly stability and adaptation.StabilityStability is an inherent characteristic of the learning process within the algorithm we are examining. It signifies the achievement of convergence towards stationary policies or other measurable criteria.Here are some common criteria (in repeated games, where $t$ means the number of times the agents have played) listed, with their strength gradually increasing (Bowling 2004):  Average reward $\\sum\\limits_{t\\le T} (r_t\\cdot \\pi_t)/T$;  Empirical distribution of actions $\\sum\\limits_{t\\le T} (\\pi_t)/T$;  Expected reward $r_t\\cdot \\pi_t$;  Policies $\\pi_t$.The analysis of equilibrium should take dynamic environment into account.  I have a question: Do the resulting policies form an equilibrium when convergence occurs? What if the it converges to a suboptimal case or failed due to insufficient exploration?AdaptationThis concept includes several aspects:  The performance of agents should be maintained or improved.  Agents can be adaptive to the update of the others’ policies since the others are learning too.  Agents can avoid exploitation by the others.  When agents have several optimal actions in coordination cases (which are common), they can achieve coordination to some extent.To check whether the designed algorithm for agents are adaptive or not, there are two criteria:  Rationality: Whether each agent converges to a best response when the other agents remain stationary.          But even in this kind of equilibrium, agents can be exploited by the others. Rationality cannot prevent the learner from ‘being exploited’ by the other agents.        No-regret: An algorithm has no-regret $\\Leftrightarrow$ the average regret is less than or equal to $0$ against all others’ policies.          No-regret can prevent the learner from ‘being exploited’ by the other agents.      Regret $\\mathscr{R}$      Scenarios (or Tasks)There are three types of scenarios based on their properties: fully cooperative, fully competitive, and mixed scenarios. These scenarios are classified according to the reward functions assigned to the agents: a scenario is  fully cooperative if all the reward functions are the same function (i.e. $r^i=r^j\\ \\forall i, j\\in I$),          (Without a centralized controller, a coordination problem will arise. Discussed below.)        fully competitive if $r^i = -r^j$ (two-agent scenarios are mostly discussed in this case),          (Games in this case are called zero-sum.)        and mixed otherwise.Some additional points:  If there is no constraint on the reward functions, the game is called general-sum game, and pure cooperative games and zero-sum games are special cases.  The second and the third case are called noncooperative games.  In coordination games, rewards are always positively related.AlgorithmsMARL algorithms are designed for variant tasks. Some of them are built relying on the assumptions of the tasks, making them applicable only to those particular tasks.For fully cooperative tasksIn this case, if a centralized controller is available, the task reduces to a MDP. Otherwise the agents take actions independently, and a coordination problem arises. When there are multiple equilibria, the agents should coordinate to break ties in a same way.The algorithms designed for the fully cooperative scenarios without centralized controller (i.e. the coordination problems) can be classified into three categories based on the dimension of coordination:  Coordination-free methods: teammate-independent  Direct coordination methods:          teammate-independent if it rely on common-knowledge,      and teammate-aware if they use negotiation.        Indirect coordination methods: team-aware(TODO)  Team-Q: It is the Q-learning algorithm, assuming that the optimal joint action are unique (which will rarely be the case).  Distributed-Q: It is deterministic. Every one can see the  OAL  JAL  FMQFor fully cooperative tasks  minimax-QFor mixed-motive tasksStatic  Fictitious play  MetaStrategy  IGA  WoLF-IGA  GIGA  GIGA-WoLF  AWESOME  Hyper-QDynamic  Single-agent RL  Nash-Q  CE-Q  Asymmetric-Q  NSCP  EIRA  WoLF-PHC  PD-WoLF  EXORLReading List  Convergence and No-Regret in Multiagent Learning. Michael Bowling. Advances in Neural Information Processing Systems (NIPS) 2004.          GIGA-WoLF (Generalized Infinitesimal Gradient Ascent, Win or Learn Fast)        Multi-Agent Reinforcement Learning: A survey. L. Busoniu, R. Babuska, B. De Schutter. International Conference on Control, Automation, Robotics and Vision (ICARCV) 2006.          This note is basically built on the framework of this paper.        Playing is Believing: The Role of Beliefs in Multi-Agent Learning. Yu-Han Chang, Leslie Pack Kaelbling. Advances in Neural Information Processing Systems (NIPS) 2001.  Nash Q-Learning for General-Sum Stochastic Games. Junling Hu, Michael P. Wellman. Journal of machine learning research (JMLR) 2003.  Convergence of Q-learning: A Simple Proof. Francisco S. Melo.  Online Convex Programming and Generalized Infinitesimal Gradient Ascent. Martin Zinkevich. International Conference on Machine Learning (ICML) 2003.  Value-Function Reinforcement Learning in Markov Games. Michael L. Littman. Cognitive Systems Research 2001.  Multi-Agent Reinforcement Learning: A Critical Survey. Yoav Shoham, Rob Powers, Trond Grenager. Technical report, Stanford University 2003.  An Overview of Multi-Agent Reinforcement Learning from Game Theoretical Perspective. Yaodong Yang, Jun Wang. arXiv 2020.  A Survey and Critique of Multiagent Deep Reinforcement Learning. Pablo Hernandez-Leal, Bilal Kartal, Matthew E. Taylor. Autonomous Agents and Multi-Agent Systems (AAMAS) 2019.  Multi-Agent Reinforcement Learning: A Selective Overview of Theories and Algorithms. Kaiqing Zhang, Zhuoran Yang, Tamer Basar. Handbook of reinforcement learning and control 2021.  Disclaimer: The above content partially uses materials from the cited papers. Corresponding links or references have been provided."
  },
  
  {
    "title": "Computation Graph Visualization",
    "url": "/posts/Computation-Graph-Visualization/",
    "categories": "Code",
    "tags": "tech, computation graph, visualization, torchviz, torchsummary, graphviz, reparameterization",
    "date": "2023-06-24 18:00:01 +0000",
    





    
    "snippet": "PyTorchvizBasics  Install          brew install graphviz (or here)      pip install torchviz        Documentation: Github  Official examples: Colab  If a node represents a backward function, it is ...",
    "content": "PyTorchvizBasics  Install          brew install graphviz (or here)      pip install torchviz        Documentation: Github  Official examples: Colab  If a node represents a backward function, it is gray. Otherwise, the node represents a tensor and is either blue, orange, or green:      Blue: reachable leaf tensors that requires grad (tensors whose .grad fields will be populated during .backward())    Orange: saved tensors of custom autograd functions as well as those saved by built-in backward nodes    Green: tensor passed in as outputs    Dark green: if any output is a view, we represent its base tensor with a dark green node.  Example 1: Basicsimport torchfrom torchviz import make_dotif __name__ == '__main__':    batch_size, input_size, hidden_size, output_size = 100, 4, 2, 1    critic = torch.nn.Sequential(        torch.nn.Linear(in_features=input_size, out_features=hidden_size, bias=False, dtype=torch.float),        torch.nn.Tanh(),        torch.nn.Linear(in_features=hidden_size, out_features=output_size, bias=True, dtype=torch.float),    )    states = torch.rand(batch_size, input_size, dtype=torch.float)    values = critic(states).squeeze()    graph = make_dot(values, params=dict(critic.named_parameters()), show_attrs=True, show_saved=True)    graph.view()Example 2: Without Netimport torchfrom torchviz import make_dotif __name__ == '__main__':    x = torch.rand(100, 5, requires_grad=True)    f1 = 2 * x + 3    f2 = torch.softmax(f1, dim=-1)    g = 5 * f1    h = f2+g    graph = make_dot(h, show_attrs=True, show_saved=True)    graph.view()Example 3: Detachimport torchfrom torchviz import make_dotif __name__ == '__main__':    x = torch.rand(100, 5, requires_grad=True)    f1 = 2 * x + 3    f2 = torch.softmax(f1, dim=-1)    g = 5 * f1.detach()    h = f2+g    graph = make_dot(h, show_attrs=True, show_saved=True)    graph.view()Example 4: $\\nabla_\\theta a$ ❌import torchfrom torchviz import make_dotif __name__ == '__main__':    batch_size, input_size, output_size = 10, 4, 2    actor = torch.nn.Sequential(        torch.nn.Linear(in_features=input_size, out_features=output_size, bias=False, dtype=torch.float),        torch.nn.Softmax(dim=-1)    )    states = torch.rand(batch_size, input_size, dtype=torch.float)    pi_list = actor(states)    distributions = torch.distributions.Categorical(pi_list)    actions = distributions.sample().squeeze(dim=0)    graph = make_dot(actions, params=dict(actor.named_parameters()), show_attrs=True, show_saved=True)    graph.view()No gradient.Example 5: $\\nabla_\\theta a$ with Gumbel-Softmax (Reparameterization)import torchfrom torchviz import make_dotif __name__ == '__main__':    batch_size, input_size, output_size = 10, 4, 2    temperature = 1    actor = torch.nn.Sequential(        torch.nn.Linear(in_features=input_size, out_features=output_size, bias=False, dtype=torch.float),        torch.nn.Softmax(dim=-1)    )    states = torch.rand(batch_size, input_size, dtype=torch.float)    pi_list = actor(states)    logits = torch.log(pi_list)    actions = torch.nn.functional.gumbel_softmax(logits, tau=temperature, hard=True)    graph = make_dot(actions, params=dict(actor.named_parameters()), show_attrs=True, show_saved=True)    graph.view()  Applying Gumbel-Softmax may cause NaN during training. Changing the data type of the variable to float64 seems to have avoided this issue.  Jang, Eric, Shixiang Gu, and Ben Poole. “Categorical reparameterization with gumbel-softmax.” arXiv preprint arXiv:1611.01144 (2016).Model StructureIf the visualization object is an nn.Module, then its structure can be easily inspected using the approaches in this section.Printimport torchif __name__ == '__main__':    batch_size, input_size, hidden_size, output_size = 100, 4, 2, 1    critic = torch.nn.Sequential(        torch.nn.Linear(in_features=input_size, out_features=hidden_size, bias=False, dtype=torch.float),        torch.nn.Tanh(),        torch.nn.Linear(in_features=hidden_size, out_features=output_size, bias=True, dtype=torch.float),    )    print(critic)Sequential(  (0): Linear(in_features=4, out_features=2, bias=False)  (1): Tanh()  (2): Linear(in_features=2, out_features=1, bias=True))Process finished with exit code 0Torchsummarypip install torchsummaryimport torchfrom torchsummary import summaryif __name__ == '__main__':    batch_size, input_size, hidden_size, output_size = 100, 4, 2, 1    critic = torch.nn.Sequential(        torch.nn.Linear(in_features=input_size, out_features=hidden_size, bias=False, dtype=torch.float),        torch.nn.Tanh(),        torch.nn.Linear(in_features=hidden_size, out_features=output_size, bias=True, dtype=torch.float),    )        summary(critic, input_size=(batch_size, 4))----------------------------------------------------------------        Layer (type)               Output Shape         Param #================================================================            Linear-1               [-1, 100, 2]               8              Tanh-2               [-1, 100, 2]               0            Linear-3               [-1, 100, 1]               3================================================================Total params: 11Trainable params: 11Non-trainable params: 0----------------------------------------------------------------Input size (MB): 0.00Forward/backward pass size (MB): 0.00Params size (MB): 0.00Estimated Total Size (MB): 0.01----------------------------------------------------------------Process finished with exit code 0"
  },
  
  {
    "title": "Dynamic Epistemic Logic",
    "url": "/posts/Dynamic-Epistemic-Logic/",
    "categories": "Mathematics",
    "tags": "tech, math, epistemic logic, common knowledge, logic, induction puzzles, multi agents",
    "date": "2023-06-22 18:00:00 +0000",
    





    
    "snippet": "  Three logicians walk into a bar.The bartender asks: “Do you all want a drink?”The first logician says: “I don’t know.”The second logician says: “I don’t know.”The third logician says: “Yes.”What ...",
    "content": "  Three logicians walk into a bar.The bartender asks: “Do you all want a drink?”The first logician says: “I don’t know.”The second logician says: “I don’t know.”The third logician says: “Yes.”What is Dynamic Epistemic Logic?1  Dynamic epistemic logic (DEL) is a logical framework dealing with knowledge and information change.Typically, DEL focuses on situations involving multiple agents and studies how their knowledge changes when events occur.There are two kinds of events:  Ontic events: These events can change factual properties of the actual world, e.g., a red card is painted in blue.  Epistemic events: These events can change agent’s knowledge without changing factual properties of the world, e.g., a card is revealed publicly (or privately) to be red.  Originally, DEL focused on epistemic events. In computer science, DEL is for example very much related to multi-agent systems, which are systems where multiple intelligent agents interact and exchange information.I discovered that this topic shares some relevance with the theory of mind in certain aspects. Both of them concern the knowledge of agents and involve analyzing how the agents modeling others.However this is perhaps not what I am looking for, for it being a modal logic. So this blog simply aims to provide a brief overview of the topic, focusing on the game-theoretic part of it.  Epistemic logic is a modal logic dealing with the notions of knowledge and belief. As a logic, it is concerned with understanding the process of reasoning about knowledge and belief.Some relevant keywords:  Common knowledge (logic),  Induction puzzles,  Dynamic epistemic logic.Induction Puzzles2A good way to understand this topic is through examples of the problems that it aims to investigate, namely induction puzzles.  Induction puzzles are logic puzzles, which are examples of multi-agent reasoning, where the solution evolves along with the principle of induction.A puzzle’s scenario always involves multiple players with the same reasoning capability, who go through the same reasoning steps.A logical feature:  According to the principle of induction, a solution to the simplest case makes the solution of the next complicated case obvious. Once the simplest case of the induction puzzle is solved, the whole puzzle is solved subsequently.Basic settings:  Players know informaiton about others but does not know their owns.          Typical tell-tale features of these puzzles include any puzzle in which each participant has a given piece of information (usually as common knowledge) about all other participants but not themselves.        Players are smart and are capaple of theory of mind.          Also, usually, some kind of hint is given to suggest that the participants can trust each other’s intelligence — they are capable of theory of mind (that “every participant knows modus ponens” is common knowledge).        “Doing nothing” is also an action and indicates something.          Also, the inaction of a participant is a non-verbal communication of that participant’s lack of knowledge, which then becomes common knowledge to all participants who observed the inaction.        The introduction of “common knowledge” will be presented in the next section.The Muddy Children PuzzleThis is one of the most classic examples of inductin puzzles. This problem also has other descriptive forms and variations, such as the Blue-Eyed Islanders and cheating wives/husbands puzzles.Description  There is a set of attentive children. They think perfectly logically. The children consider it possible to have a muddy face. None of the children can determine the state of their own face themselves. But, every child knows the state of all other children’s faces. A custodian tells the children that at least one of them has a muddy face. The children are each told that they should step forward if they know that their own face is muddy. Hereafter, the custodian starts to count and after every stroke, every muddy child has an opportunity to step forward.Let us extract the core of it.What is the goal?Players (children) want to find out a piece of information (whether their face is clean or not) that they do not know but others do.What are the assumptions?  Before the game starts, there will be an outsider who provides some indirect information.  Players know informaiton about others but does not know their owns.  There are occasions when making decisions openly (the chances for children to step forward), and the actions taken become common knowledge.  Players are smart and are capaple of theory of mind.  If players know their own information, they will reveal it to their own best interests.  “Doing nothing” is also an action and indicates something. Specifically, it means that a player does not know its own information. And this is new information to others.Logical solutionAssume that there are $n$ children and $k$ of them are dirty where $k&gt;1$.  Let’s assume that there are just 2 children: Alice and Bob. If only Alice is dirty, she will step forward at the first stroke, because she does not see any other dirty faces. The same is true for Bob. If Alice sees Bob not stepping forward at the first stroke, she must conclude that he certainly sees another muddy child and they will step forward simultaneously at the second stroke.Let’s assume that there are just 3 children: Alice, Bob, and Charly. If there are less than 3 muddy children, the puzzle evolves like in the case with 2 children. If Charly sees that Alice and Bob are muddy and not stepping forward at the second stroke, they all together will step forward at the third stroke.It can be proven that $k$ muddy children will step forward after $k$ strokes.Denote Alice as $i$ and Bob as $j$. There are three cases:  $i$ is dirty and $j$ is not.          $i$ knows $j$ is not dirty. So $i$ can infer that it is dirty. Thus $i$ will step forward at the first stroke.      $j$ knows $i$ is dirty but $j$ cannot infer its situation. But after the first stroke $j$ will know that “$i$ can infer itself is dirty”, and thus $j$ can infer that $j$ is not dirty.        $j$ is dirty and $i$ is not. The analysis is symmetrical to that of the first case.  $i$ and $j$ are both dirty.          $i$ knows $j$ is dirty but $i$ cannot infer its situation. Thus $i$ will do nothing at the first stroke.      $j$ knows $i$ is dirty but $j$ cannot infer its situation. Thus $j$ will do nothing at the first stroke.      After the first stroke, $i$ will know that $j$ cannot infer anything, meaning that it is not the first case (not “You are dirty but I am not”). So $i$ can infer that it is dirty. Thus $i$ will step forward at the second stroke.      So do $j$. Thus $j$ will step forward at the second stroke.        There is a straightforward proof by induction that the first $k − 1$ times he asks the question, they will all say “No,” but then the $k$th time the children with muddy foreheads will all answer “Yes.”3So what information does “At least one of you has mud on your forehead” bring to players? The common knowledge.  Let us denote the fact “at least one child has a muddy forehead” by $p$. Notice that if $k &gt; 1$, i.e., more than one child has a muddy forehead, then every child can see at least one muddy forehead, and the children initially all know $p$. Thus, it would seem that the father does not provide the children with any new information, and so he should not need to tell them that $p$ holds when $k &gt; 1$.But this is false! What the father provides is common knowledge. If exactly $k$ children have muddy foreheads, then it is straightforward to see that $E^{k−1}p$ holds before the father speaks, but $E^kp$ does not (here $E^k\\varphi$ means $\\varphi$, if $k = 0$, and everyone knows $E^{k-1}\\varphi$, if $k \\ge 1$). The father’s statement actually converts the children’s state of knowledge from $E^{k−1}p$ to $Cp$ (here $Cp$ means that there is common knowledge of $p$). With this extra knowledge, they can deduce whether their foreheads are muddy.In the muddy children puzzle, the children do not actually need common knowledge; Ekp suffices for them to figure out whether they have mud on their foreheads.3I found an alternative and more general notation:  $p$: A piece of information.  $K_i p$: Agent $i$ knows $p$.  $B_i p$: Agent $i$ believes $p$.  $E p := \\land_{i} K_i p$: All agents know $p$.  $C p := Ep \\land EEp \\land EEEp \\land \\ldots$: Common knowledge.  Two interaction axioms: $K_i p \\to B_i p$ (i.e. if $i$ knows $p$ then it believes $p$) and $B_i p \\to KB_i p$ (i.e. if $i$ believes $p$ then it knows that it believes $p$).Game-theoretic solution  Muddy children puzzle can also be solved using backward induction from game theory.The reference is from this paper, written in German.  Muddy children puzzle can be represented as an extensive form game of imperfect information. Every player has two actions — stay back and step forwards. There is a move by nature at the start of the game, which determines the children with and without muddy faces. Children do not communicate as in non-cooperative games. Every stroke is a simultaneous move by children. It is a sequential game of unlimited length. The game-theoretic solution needs some additional assumptions:      All children are rational and all children’s rationality is common knowledge. This means that Alice is rational, Alice knows that Bob is rational and Alice knows that Bob knows that Charly is rational and so on and vice versa.    Stepping forward without having a muddy face results in a big penalty.    Stepping forward with a muddy face results in a reward.    Every stroke results in minor negative penalty aka discount factor for every child until any of them stepped forward. Any multiple of the minor penalty is always a lesser evil than the big penalty.    If only Alice is muddy, the last assumption makes it irrational for her to hesitate. If Alice and Bob are muddy, Alice knows that Bob’s only reason of staying back after the first stroke is the apprehension to receive the big penalty of stepping forward without a muddy face. In the case with $k$ muddy children, receiving $k$ times the minor penalty is still better than the big penalty.This is a non-cooperative extensive-form (sequential) game of imperfect information.  Agents: finite, rational (common knowledge), and not hesitant.  States: All agents’ face situations.  Actions: Do nothing, or step forward. Everyone moves  simultaneously at every timestep.  Observations: The others’ face situations.  Reward function:          $R^i(s^i=\\text{clean face}, a^i=\\text{step forward}) = -\\infty$,      $R^i(s^i=\\text{muddy face}, a^i=\\text{step forward}) = r$, where $r$ is a predefined constant reward.      $R^i(\\cdot, a^i=\\text{step forward}) = 0$, I guess?      So this is just formulation. What about the method “backward induction”?  I found a paper about this, cited 181 times. I will read it later:Rational Dynamics and Epistemic Logic in Games.Johan Van Benthem.International Game Theory Review 2007.Hat Puzzles  One type of induction puzzle concerns the wearing of colored hats, where each person in a group can only see the color of those worn by others, and must work out the color of their own.The King’s Wise Men Hat Puzzle  The King called the three wisest men in the country to his court to decide who would become his new advisor. He placed a hat on each of their heads, such that each wise man could see all of the other hats, but none of them could see their own. Each hat was either white or blue. The king gave his word to the wise men that at least one of them was wearing a blue hat; in other words, there could be one, two, or three blue hats, but not zero. The king also announced that the contest would be fair to all three men. The wise men were also forbidden to speak to each other. The king declared that whichever man stood up first and correctly announced the colour of his own hat would become his new advisor. The wise men sat for a very long time before one stood up and correctly announced the answer. What did he say, and how did he work it out?Common Knowledge  The notion of common knowledge, where everyone knows, everyone knows that everyone knows, etc., has proven to be fundamental in various disciplines, including Philosophy, Artificial Intelligence, Game Theory, Psychology, and Distributed Systems. This key notion was first studied by the philosopher David Lewis in the context of conventions. Lewis pointed out that in order for something to be a convention, it must in fact be common knowledge among the members of a group. (For example, the convention that green means “go” and red means “stop” is presumably common knowledge among the drivers in our society.)3  Common knowledge also arises in discourse understanding. Suppose Ann asks Bob “What did you think of the movie?” referring to a showing of Monkey Business they have just seen. Not only must Ann and Bob both know that “the movie” refers to Monkey Business, but Ann must know that Bob knows (so that she can be sure that Bob will give a reasonable answer to her question), Bob must know that Ann knows that Bob knows (so that Bob knows that Ann will respond appropriately to his answer), and so on. In fact, by a closer analysis of this situation, it can be shown that there must be common knowledge of what movie is meant in order for Bob to answer the question appropriately.3  This note is incomplete, and I have no intention of completing it because I realized it is not the topic I am looking for.References            Wikipedia: Dynamic Epistemic Logic. &#8617;              Wikipedia: Induction Puzzles &#8617;              Ronald Fagin, Joseph Y. Halpern, Yoram Moses, Moshe Y. Vardi. “Common knowledge revisited.” Annals of Pure and Applied Logic (1999). &#8617; &#8617;2 &#8617;3 &#8617;4      "
  },
  
  {
    "title": "Theory of Mind and Markov Models",
    "url": "/posts/ToM-MM/",
    "categories": "Artificial Intelligence, Multi-Agent Reinforcement Learning",
    "tags": "tech, theory of mind, Markov model, reinforcement learning, multi agents",
    "date": "2023-06-19 12:00:01 +0000",
    





    
    "snippet": "  We do not see things as they are, we see them as we are. — Anaïs Nin.What is Theory of Mind?  In psychology, theory of mind refers to the capacity to understand other people by ascribing mental s...",
    "content": "  We do not see things as they are, we see them as we are. — Anaïs Nin.What is Theory of Mind?  In psychology, theory of mind refers to the capacity to understand other people by ascribing mental states to them (that is, surmising what is happening in their mind). This includes the knowledge that others’ beliefs, desires, intentions, emotions, and thoughts may be different from one’s own.  Possessing a functional theory of mind is considered crucial for success in everyday human social interactions. People use such a theory when analyzing, judging, and inferring others’ behaviors. The discovery and development of theory of mind primarily came from studies done with animals and infants.  Empathy—the recognition and understanding of the states of mind of others, including their beliefs, desires, and particularly emotions—is a related concept. Empathy is often characterized as the ability to “put oneself into another’s shoes”. Recent neuro-ethological studies of animal behaviour suggest that even rodents may exhibit empathetic abilities. While empathy is known as emotional perspective-taking, theory of mind is defined as cognitive perspective-taking.1In my understanding, theory of mind refers to the ability of an individual modeling others’ decision making processes based on others’ partial observations.Basic Markov ModelsDec-POMDPA decentralized partially observable Markov decision process (Dec-POMDP) is a tuple $(I, S,\\set{A^i}_{i\\in I}, \\mathcal{P}, R, \\set{O^i}_{i\\in I}, \\mathcal{Q}, \\gamma)$, where  $I$ is a set of agents ($\\vert I\\vert=n$ and they are cooperative),  $S$ is a set of global states of the environment (and agents cannot see the sampled state at any time, but they know the state set),  $A^i$ is a set of actions for agent $i$, with $\\boldsymbol{A}=\\times_{i\\in I} A^i$ is the set of joint actions,  $\\mathcal{P}:S\\times \\boldsymbol{A}\\to\\Delta(S)$ is the state transition probability where $\\Delta(S)$ is a set of distributions over $S$,  $R:S\\times \\boldsymbol{A}\\to \\mathbb{R}$ is a reward function (not $\\mathbb{R}^n$ since the agents are cooperative),  $O^i$ is a set of observations for agent $i$, with $\\boldsymbol{O} = \\times_{i\\in I} O^i$ is the set of joint observations,  $\\mathcal{Q}:S\\times \\boldsymbol{A}\\to\\Delta(\\boldsymbol{O})$ is an observation emission function (sometimes $\\mathcal{Q}:S\\to\\Delta(\\boldsymbol{O})$), and  $\\gamma\\in[0,1]$ is the discount factor.One step of the process is: $(s_{t}, \\boldsymbol{o}_{t})\\to \\boldsymbol{a}_{t}\\to (s_{t+1} \\to (\\boldsymbol{o}_{t+1}, r_{t}))$. At each timestep $t$,  each agent takes an action $a_t^i\\in A^i$ based on its belief of the current state, given its observable $o_t^i$ and previous belief (the term “belief” will be introduced later),  $s_{t+1}\\sim \\mathcal{P}(\\cdot \\mid s_t, \\boldsymbol{a}_t)$,  $\\boldsymbol{o}_{t+1} \\sim \\mathcal{Q}(\\cdot \\mid s_{t+1}, \\boldsymbol{a}_{t})$ and a reward is generated for the whole team based on the reward function $R(s,\\boldsymbol{a})$.These timesteps repeat until some given horizon (called finite horizon) or forever (called infinite horizon). The discount factor $\\gamma$ maintains a finite sum in the infinite-horizon case ($\\gamma \\in [0,1)$). The goal is to maximize expected cumulative reward over a finite or infinite number of steps.1What is different from what I previously thought is that the observations are sampled after agents make decisions at each timestep.  This definition is adapted from that of Wikipedia2. When discussing Dec-POMDP, these papers34 are often referenced.MDP &amp; POMDPMarkov decision processes (MDPs) and partially observable Markov decision processes (POMDPs) are degenerated cases of Dec-POMDPs:  Dec-POMDP: $(I, S,\\set{A^i}_{i\\in I}, \\mathcal{P}, R, \\set{O^i}_{i\\in I}, \\mathcal{Q}, \\gamma)$.  POMDP: $(S, A, \\mathcal{P}, R, O, \\mathcal{Q}, \\gamma)$, a single-agent version of Dec-POMDP.  MDP: $(S, A, \\mathcal{P}, R, \\gamma)$, a fully observable version of POMDP.  One may check this slides5 for understanding the comparison between MDP, POMDP, and Dec-POMDP.Belief  So how does the agent know which state it is in, in POMDPs?Since it is the state that affects payoffs and the state transitions (thus the future payoffs) rather than the observation, the agent needs to estimate the current state $s_t$ by its previous observations before choosing $a_t$.The state is Markovian by assumption, meaning that maintaining a belief over the current states $s_t$ solely requires knowledge of  the previous belief state $b(\\cdot\\mid a_{t-1}, o_{t})$,          the taken action $a_{t-1}$,      and the current observation $o_{t}$.        and the environment’s model          the sets $S$ and $O$,      the observation emission function $\\mathcal{Q}$,      and the state transition function $\\mathcal{P}$.      The belief $b$ is a posterior distribution over states. The conditional probability of a current state $s_t$ can be recursively calculated by Bayes’ rule as follows:If the agent has access to the environment’s model , given $a_{t-1}, o_{t}$, and $b(\\cdot\\mid o_{t-1}, a_{t-2})$ (or $b_{t-1}$ in short), then\\[\\begin{aligned}b\\Big(s_{t}\\mid o_{t}, a_{t-1}, b_{t-1} \\Big)     &amp;= \\frac{        P(o_{t}\\mid s_t, a_{t-1}, b_{t-1})\\cdot         P(s_t\\mid a_{t-1}, b_{t-1})    }    {        P(o_{t}\\mid a_{t-1}, b_{t-1})    }\\\\    &amp;= \\frac{        P(o_{t}\\mid s_t, a_{t-1})\\cdot         \\sum\\limits_{s_{t-1}}         P(s_{t}\\mid s_{t-1}, a_{t-1})         \\cdot b_{t-1}(s_{t-1})    }    {        \\sum\\limits_{s_t}        P(o_{t}\\mid s_t, a_{t-1})\\cdot         \\sum\\limits_{s_{t-1}}         P(s_{t}\\mid s_{t-1}, a_{t-1})         \\cdot b_{t-1}(s_{t-1})    }\\\\    &amp;= \\frac{        \\mathcal{Q}(o_{t}\\mid s_t, a_{t-1})         \\sum\\limits_{s_{t-1}}         \\mathcal{P}(s_{t}\\mid s_{t-1}, a_{t-1})         \\cdot b_{t-1}(s_{t-1})    }    {        \\sum\\limits_{s_{t}}        \\mathcal{Q}(o_{t}\\mid s_t, a_{t-1})         \\sum\\limits_{s_{t-1}}         \\mathcal{P}(s_{t}\\mid s_{t-1}, a_{t-1})        \\cdot b_{t-1}(s_{t-1})    }.\\end{aligned}\\]Note that\\[P(o_{t}\\mid a_{t-1}, b_{t-1}) = \\sum\\limits_{s_{t}}        \\mathcal{Q}(o_{t}\\mid s_t, a_{t-1})         \\sum\\limits_{s_{t-1}}         \\mathcal{P}(s_{t}\\mid s_{t-1}, a_{t-1})        \\cdot b_{t-1}(s_{t-1}),\\]and we will catch it later.  This definition is adapted from that of Wikipedia6. And I found its original definition is a bit of confusing, for agent observing $o$ after reaching $s’$. I suppose that $o$ is ought to be $o_{t+1}$ or $o’$.Theory of Mind in Dec-POMDPsFuchs et al. proposed nested beliefs for deep RL in Hanabi.7 And in this section, I will focus on the settings of their paper rather than their method, since the method is specifically designed to tackle the Hanabi problem.Consider a two-player game. Each agent makes decisions based on its belief of the other’s policy. So the two agents’ polices are recursively dependent:  $i$ makes decisions based on $j$’s policy.  And $j$ acts the same way.  If $i$ become aware of the second step, then $i$ will speculate how $j$ is guessing it and make decisions based on that.  And so forth.Formally, a belief at depth $k$ is given by $b_k^{ij\\ldots}=\\Delta(S_k)$, such that $\\vert ij\\ldots \\vert = k+1$, and the subscript of $S$ indicates the depth. The beliefs at even depth are self-reflective, and the ones at odd depth model the other’s modeling. E.g.,  $b_0^{i}$ is $i$’s prior knowledge of states.  $b_1^{ij}$ is $i$’s belief about $j$’s prior knowledge that $i$ models. It is still a distribution over states.  $b_2^{iji}$ is $i$’s belief about $b_1^{ji}$. It is still a distribution over states.My questions  I am not sure why the agent’s high-level beliefs are still distributions over states rather than distributions over the former level beliefs.  After we have the tool of belief, what can we do? How should agents make decisions based on their beliefs?An exampleSimplified Action Decoder (SAD). See my other note.  Hu, Hengyuan, and Jakob N. Foerster. “Simplified Action Decoder for Deep Multi-Agent Reinforcement Learning.” International Conference on Learning Representations. 2019.Belief MDPGiven a POMDP $(S, A, \\mathcal{P}, R, O, \\mathcal{Q}, \\gamma)$, the corresponding belief MDP is $(B, A, \\tau, R, \\gamma)$, where  $B$ is the belief states set, and each element in it is a distribution over the states of the POMDP,  $A$ is the same as the one of the POMDP,  $\\tau: B\\times A\\to\\Delta(B)$ is the belief state transition function,  $\\mathcal{R}: B\\times A \\to \\mathbb{R}$ is the reward function on belief states,  $\\gamma$ is the same as the one of the POMDP.More specifically,\\[\\tau(b_{t+1} \\mid b_t, a_t) = \\sum\\limits_{o_{t+1}} P(b_{t+1}\\mid b_t, a_t, o_{t+1}) \\cdot P(o_{t+1}\\mid b_t, a_t),\\]where $P(o_{t+1}\\mid b_t, a_t)$ is defined in the previous section and\\[P(b_{t+1}\\mid b_t, a_t, o_{t+1}) = \\begin{cases}  1 &amp; \\text{if the belief update with } b_t, a_t, o_{t+1} \\text{ returns } b_{t+1}, \\\\  0 &amp; \\text{otherwise}.\\end{cases}\\]And $\\mathcal{R}(b, a) = \\sum\\limits_{s} b(s) \\cdot R(s,a)$.Compared to the original POMDP, the corresponding belief MDP is not partially observable anymore, and the agent makes decisions at each timestep based on the current belief state. Its policy is denoted as $\\pi(b)$, and its goal is $\\max\\limits_{\\pi} V_{\\pi}(b_0)$, where $V_{\\pi}(b_0) = \\mathbb{E}_\\pi\\left[    \\sum\\limits_{t=0}^{\\infty}\\gamma^t\\cdot     \\mathcal{R}(b_t,a_t)\\mid b_0\\right].$6References            Wikipedia: Theory of Mind. &#8617; &#8617;2              Wikipedia: Dec-POMDP. &#8617;              Daniel S Bernstein, Robert Givan, Neil Immerman, Shlomo Zilberstein. “The complexity of decentralized control of markov decision processes.” Mathematics of operations research (2002). &#8617;              Frans A Oliehoek, Christopher Amato. “A concise introduction to decentralized POMDPs.” Springer (2016). &#8617;              Alina Vereshchaka’s slides about MDPs. &#8617;              Wikipedia: POMDP. &#8617; &#8617;2              Andrew Fuchs, Michael Walton, Theresa Chadwick, Doug Lange. “Theory of mind for deep reinforcement learning in hanabi.” NeurIPS Workshop (2019). &#8617;      "
  },
  
  {
    "title": "Theoretical Computer Science (TCS)",
    "url": "/posts/TCS-Basics/",
    "categories": "Mathematics",
    "tags": "tech, math, theoretical computer science, computational complexity theory, reduction, complexity classes",
    "date": "2023-06-17 12:00:01 +0000",
    





    
    "snippet": "  This note will be consistently updated.What is TCS?(Wikipedia) Theoretical computer science (TCS) is a subset of general computer science and mathematics that focuses on mathematical aspects of c...",
    "content": "  This note will be consistently updated.What is TCS?(Wikipedia) Theoretical computer science (TCS) is a subset of general computer science and mathematics that focuses on mathematical aspects of computer science such as the theory of computation, lambda calculus, and type theory.Topics that I might come across:  Algorithms  Computational complexity theory  Computational learning theory  Information-based complexity  Information theory  Machine learning  currently I mainly focus on the computational complexity theory.What is Computational Complexity Theory?(Wikipedia)  Computational complexity theory is a branch of the theory of computation that focuses on classifying comiputational problems according to their inherent diffculty, and relating those classes to each other.  A computational problem is understood to be a task that is in principle amenable to being solved by a computer, which is equivalent to stating that the problem may be solved by mechanical application of mathematical steps, such as an algorithm.  A problem is regarded as inherently difficult if its solution requires significant resources, whatever the algorithm used. The theory formalizes this intuition, by introducing mathematical models of computation to study these problems and quantifying the amount of resources needed to solve them, such as time and storage. Other complexity measures are also used, such as the amount of communication (used in communication complexity), the number of gates in a circuit (used in circuit complexity) and the number of processors (used in parallel computing).  One of the roles of computational complexity theory is to determine the practical limits on what computers can and cannot do.Computational ProblemsProblem instancesRepresenting problem instancesComplexity classesMisc Important ConceptsReduction(Wikipedia)In computability theory and computational complexity theory, a reduction is an algorithm for transforming one problem into another problem. A sufficiently efficient reduction from one problem to another may be used to show that the second problem is at least as difficult as the first.IntractabilityBig $O$ notation  This part partially uses material from this website and Wikipedia.  Big-oh is about finding an asymptotic upper bound.$f(x) = O\\left(g(x)\\right)$, iff (if and only if) $\\exists 0&lt;k, 0&lt;x_0$, s.t.$f(x)\\le k\\cdot g(x), \\forall x_0\\le x$.After $x_0$, there is a $k\\cdot g(x)$ which is the upper bound of $f(x)$. (So in my understanding, the complexity means the upper bound.)It is read “$f(x)$ is big O of $g(x)$”.Comparison:  $f(x) = O\\left(g(x)\\right)$, iff $\\exists 0&lt;k, 0&lt;x_0$, s.t.$f(x)\\le k\\cdot g(x), \\forall x_0\\le x$.          The upper bound of $f(x)$ after $x_0$.      $\\lim\\limits_{x\\to\\infty} \\frac{f(x)}{g(x)} &lt; \\infty$.        $f(x) = \\Omega\\left(g(x)\\right)$, iff $\\exists 0&lt;k, 0&lt;x_0$, s.t.$f(x)\\ge k\\cdot g(x), \\forall x_0\\le x$.          The lower bound of $f(x)$ after $x_0$.      $\\lim\\limits_{x\\to\\infty} \\frac{f(x)}{g(x)} &gt; 0$.        $f(x) = \\Theta\\left(g(x)\\right)$, iff $f(x) = O\\left(g(x)\\right)$ and $f(x) = \\Omega\\left(g(x)\\right)$.          The exact bound of $f(x)$.      $\\lim\\limits_{x\\to\\infty} \\frac{f(x)}{g(x)} \\in \\mathbb{R}_{&gt;0}$.        $f(x) = o\\left(g(x)\\right)$, iff $f(x) = O\\left(g(x)\\right)$ and $f(x)$ is not $\\Theta\\left(g(x)\\right)$.          The upper bound of $f(x)$ excluding the exact bound.      $\\lim\\limits_{x\\to\\infty} \\frac{f(x)}{g(x)} = 0$.      Big O can also be used to describe the error term in an  approximation to a mathematical function.\\[\\begin{aligned}e^x =&amp; 1+x+\\frac{x^2}{2!}+\\frac{x^3}{3!}+\\frac{x^4}{4!}+\\dots &amp; \\forall x \\\\=&amp; 1+x+\\frac{x^2}{2!}+O(x^3) &amp; x\\to 0\\\\=&amp; 1+x+O(x^2) &amp; x\\to 0  \\\\\\end{aligned}\\]Compared with $x^2$, $x^3$ is closer to $0$, when $x\\to 0$.  Disclaimer: The above content is summarized from Wikipedia and other sources. Corresponding links or references have been provided."
  },
  
  {
    "title": "Principal Component Analysis",
    "url": "/posts/PCA/",
    "categories": "Mathematics",
    "tags": "tech, math, PCA",
    "date": "2023-06-16 06:40:00 +0000",
    





    
    "snippet": "  很久以前的笔记介绍PCA is a widely used dimensionality reduction technique that projects high-dimensional data into a lower-dimensional space, while retaining as much of the data’s variance as possible.My ...",
    "content": "  很久以前的笔记介绍PCA is a widely used dimensionality reduction technique that projects high-dimensional data into a lower-dimensional space, while retaining as much of the data’s variance as possible.My understanding is that the constraint on variance in PCA is intended to allow us to distinguish each point as effectively as possible in the new lower-dimensional space.算法有个$m$维的随机变量$X$，有$n$个数据，那么$X$是个$m\\times n$的矩阵。现在要把维度从$m$降到$k$，让这些样本点在降低后的维度上和原来的点一一对应，且点之间尽可能远离  把数据中心化：把这$n$个数据算个均值，然后每个数据都减去均值（这一步的目的是快速地算协方差矩阵）  算协方差矩阵：中心化后的数据矩阵，自己乘自己，就是协方差矩阵，$\\frac{1}{n-1}XX^\\intercal$或者$\\frac{1}{n}XX^\\intercal$          这一步是为了求协方差矩阵的特征向量，除以$n$或者$n-1$不影响求特征向量      按理说是除以$n-1$，因为是求的样本方差，除以这个是无偏估计（根据Bessel’s correction，求方差时用到了均值，所以得少一个自由度）      而且均值是0了，也没有用到均值，所以可以除以$n$        求协方差矩阵的特征值和特征向量  把特征值最大的那$k$个特征向量最为新的空间的基坐标，然后把原空间投影上去例子  Generated by ChatGPT 3.5PCA（主成分分析）的计算步骤如下，我将为你提供一个简化的例子和详细的计算过程：假设我们有以下二维数据集：            数据点      x      y                  A      1      2              B      2      3              C      3      4              D      4      5              E      5      6      步骤 1：数据中心化计算每个维度的均值并从每个数据点中减去均值。\\[\\mathrm{Mean}(x) = \\frac{1 + 2 + 3 + 4 + 5}{5} = 3\\]\\[\\mathrm{Mean}(y) = \\frac{2 + 3 + 4 + 5 + 6}{5} = 4\\]            数据点      x - Mean(x)      y - Mean(y)                  A      -2      -2              B      -1      -1              C      0      0              D      1      1              E      2      2      步骤 2：计算协方差矩阵计算数据的协方差矩阵。协方差矩阵的元素$(i, j)$表示第 $i$ 维和第 $j$ 维之间的协方差。\\[\\mathrm{Covariance}(x, x) = \\frac{\\sum{(x - \\mathrm{Mean}(x))^2}}{n-1} = 2.5\\]\\[\\mathrm{Covariance}(y, y) = \\frac{\\sum{(y - \\mathrm{Mean}(y))^2}}{n-1} = 2.5\\]\\[\\mathrm{Covariance}(x, y) = \\frac{\\sum{(x - \\mathrm{Mean}(x))(y - \\mathrm{Mean}(y))}}{n-1} = 2.5\\]协方差矩阵：\\[\\begin{bmatrix}\\mathrm{Cov}(x, x) &amp; \\mathrm{Cov}(x, y) \\\\\\mathrm{Cov}(y, x) &amp; \\mathrm{Cov}(y, y) \\\\\\end{bmatrix}=\\begin{bmatrix}2.5 &amp; 2.5 \\\\2.5 &amp; 2.5 \\\\\\end{bmatrix}\\]步骤 3：计算特征向量和特征值对协方差矩阵进行特征分解，得到特征向量和特征值。特征方程 $\\mathrm{det}(\\mathrm{Covariance} - \\lambda I) = 0$特征值：$\\lambda_1 = 5$, $\\lambda_2 = 0$对于 $\\lambda_1 = 5$，解方程 $\\mathrm{Covariance} - \\lambda_1 I = 0$，得到特征向量 $v_1 = \\begin{bmatrix} 1 \\ 1 \\end{bmatrix}$。对于 $\\lambda_2 = 0$，解方程 $\\mathrm{Covariance} - \\lambda_2 I = 0$，得到特征向量 $v_2 = \\begin{bmatrix} -1 \\ 1 \\end{bmatrix}$。步骤 4：选择主成分选择特征值最大的特征向量作为主成分。在这个例子中，选择 $v_1 = \\begin{bmatrix} 1 \\ 1 \\end{bmatrix}$ 作为主成分，因为它对应的特征值 $\\lambda_1 = 5$ 最大。步骤 5：投影将数据点投影到主成分上。投影 A: $[1, 2]\\cdot [1, 1]^T=3$投影 B: $[2, 3]\\cdot [1, 1]^T = 5$投影 C: $[3, 4] \\cdot [1, 1]^T = 7$投影 D: $[4, 5] \\cdot [1, 1]^T = 9$投影 E: $[5, 6] \\cdot [1, 1]^T = 11$这样，我们得到了在主成分方向上的投影值，它们可以用于表示降维后的数据。在实际应用中，通常会选择多个主成分来保留更多的数据方差。我的理解之所以是求协方差矩阵并让其对角化，把使其对角化的那个正交矩阵拿来对原来的维度进行变化，是因为形式刚刚好一致首先目的是两个  一个是找到新的一组线性无关的维度（从旧的维度作变换得来）          要找到个正交单位矩阵$P$，对原来的空间作变化，也就是$P$乘在原来空间的左边      不改变原空间的大小（模长和内积），只旋转，所以要找正交单位矩阵        一个是这个新的维度要能描述最大的数据之间的方差，保持点之间的距离够大，好区分数据中心化之后，$\\frac{1}{n} XX^\\intercal$就是协方差矩阵，因为均值是0\\[\\text{Cov}(X, Y) = \\frac{\\sum_{i=1}^{n} (x_i \\cdot y_i)}{n-1} = \\frac{\\sum_{i=1}^{n} (x_i \\cdot y_i)}{n}\\]求协方差矩阵的目的是，对角线上是自己的方差，其他是两个的相关性，那么要变换的维度上尽可能无关那是最好的所以要让协方差对角化，$PX(PX)^\\intercal$=新的对角矩阵，可以看成是P对X变换后的新向量的协方差矩阵，那么P就是刚好我要的这个正交单位矩阵了，因为它让X变换后的协方差矩阵是对角的同时对角化完后又是从大到小排的，大的表示特征向量贡献大的，更多得描述了这个矩阵，那么留$P$前面$k$行就可以降到$k$维了  参考：主成分分析PCA算法：为什么去均值以后的高维矩阵乘以其协方差矩阵的特征向量矩阵就是“投影”？ - 郝熊升的回答 - 知乎 "
  },
  
  {
    "title": "Information Design",
    "url": "/posts/Information-Design/",
    "categories": "Economics & Game Theory",
    "tags": "tech, game theory, information design, Bayesian persuasion, Bayes plausible, concavification, obedience, Bayes correlated equilibrium, Bayes nash equilibrium, multi agents, incentive compatibility",
    "date": "2023-06-01 12:00:01 +0000",
    





    
    "snippet": "What is Information Design?Communication does not just happen in fully cooperative scenarios. In some cases, the sender can persuade the receiver by “strategically deceiving”, to increase its own e...",
    "content": "What is Information Design?Communication does not just happen in fully cooperative scenarios. In some cases, the sender can persuade the receiver by “strategically deceiving”, to increase its own expected payoff. Actually, “one quarter of gdp is persuasion.” (McCloskey &amp; Klamer 1995).Nontrivially, “successful partially deceiving” is a better equilibrium than “saying nothing” and “revealing all information”.Information design is the study of this persuasion. And Bayesian persuasion is a special case of information design, which consists of a sender and a receiver.The problem of information design can be equivalently approached from various perspectives:  $\\max_{\\varphi} \\mathbb{E}_{s \\sim \\mu_0}\\left[\\mathbb{E}_{\\sigma \\sim \\varphi(\\cdot\\mid s)}\\left[r^i(s,a^*(\\sigma))\\right]\\right]$.  $\\max_{\\varphi} \\mathbb{E}_{\\mu\\sim\\tau}\\Big[\\mathbb{E}_{s\\sim\\mu}\\big[r^i(s,a^*(\\mu))\\big]\\Big]$, s.t. $\\mathbb{E}_{\\mu\\sim\\tau}(\\mu) = \\mu_0$.  In a two-signal case: The sender manipulates the receiver’s posterior beliefs (each corresponding to a sent signal) to find the highest intersection point of the line segment $(\\mu_1 - \\mu_2, \\hat{v}_1 - \\hat{v}_2)$ and $x = \\mu_0$.  Denote the concavification of $\\hat{v}(\\mu) = \\mathbb{E}_{s\\sim\\mu}\\big[r^i(s,a^{*}(\\mu))\\big]$ as $f(\\mu)$. The sender’s optimal expected payoff is $f(\\mu_0)$.  The sender select an optimal Bayes correlated equilibrium given an objective function.  $\\max_{\\varphi} \\mathbb{E}_{s \\sim \\mu_0}\\left[\\mathbb{E}_{\\sigma \\sim \\varphi(\\cdot\\mid s)}\\left[r^i(s,a^*(\\sigma))\\right]\\right]$, s.t. Obedience.PapersThe following part of this note is to summarize the essence of these papers:  Bayesian Persuasion (Kamenica &amp; Gentzkow 2011)  An equilibrium perspective:          Correlated Equilibrium in Games with Incomplete Information (Bergemann &amp; Morris 2011a)      Robust Predictions in Games with Incomplete Information (Bergemann &amp; Morris 2011b)      Bayes Correlated Equilibrium and the Comparison of Information Structures in Games (Bergemann &amp; Morris 2016)        Surveys:          Bayesian Persuasion and Information Design (Kamenica 2019)      Algorithmic Information Structure Design: A Survey (Dughmi 2019)      Information Design: A Unifified Perspective (Bergemann &amp; Morris 2019)      TimingConsider a persuasion between a sender and a receiver (named as Bayesian persuasion). The timing of it is:  The sender chooses a signaling scheme $\\varphi$. (Given a state $s$, $\\varphi(\\cdot \\mid s)$ is a distribution of $\\sigma$.)  The receiver observes this $\\varphi$.  The environment generates a state $s$.  A signal $\\sigma$ is sampled accroding to the commited $\\varphi(\\cdot\\mid s)$.  The receiver calculates its posterior belief $\\mu(\\cdot \\mid \\sigma)$, and then chooses an action $a$ that maximizes $\\mathbb{E}_{\\mu}(r^j)$. (Thus the term Bayesian persuasion.)  The sender and the receiver get rewards $r^i(s,a)$ and $r^j(s,a)$, respectively.  The core of information design is to find the optimal signaling scheme $\\varphi$ for the sender and to study the nature of this problem. But the sender’s payoff is determined by the receiver’s action, so it has to use its information advantage to influence the receiver’s behavior (by designing $\\varphi$), thereby indirectly optimizing its own expected payoff.Important Examples  Recommendation Letter (Example 2.1 in Dughmi 2019).  Courtroom (The first example in Section 2.2 of Kamenica 2019).  Routing Game (Example 3.2 in Dughmi 2019) or Braess’s paradox.Assumptions  The prior $\\mu_0$ of states is common knowledge.  The receiver is self-interested.          The receiver has its objective function to optimize (In my understanding it does not need to be consistent with its epected environmental reward), and the sender wants to influence the receiver’s behavior.      The sender’s objective function does not need to be consistent with its environmental reward, e.g., it can be social welfare $r^i+r^j$. (In this way, the sender is altruistic.)      Anyway, there is no limit to the goals of the sender and the receiver. They may be fully cooperative, mixed motived or adversarial.        The receiver’s strategy is based on Bayes’ rule (i.e. Bayesian rational). (Timing 5)  Commitment: The sender will honestly commit a signaling scheme to the receiver before the interaction with the receiver. (Timing 1-2)          It is this that makes Bayesian persuasion (or information design) different from other communication models, e.g. cheap talk, verifiable message, signaling games. (Kamenica 2019)      Justifications:                  Reputation          The signaling schemes are common knowledge, e.g., the sender (prosecutor) can choose investigative means (forensic tests, calling witness, etc.) to partially reveal the truth, which are public to everyone. (Courtroom)          (Justifications vary across applications. See Section 2.2 in Kamenica 2019)                      An analysis analogues to the revelation principle: The optimal scheme needs no more signals than the number of states of nature.          Proof: probability simplex + Caratheodory’s theorem. (Dughmi 2019)      Simplification and Geometric InterpretationsReformulation  The receiver’s strategy:          Given a sent $\\sigma$, its posterior belief is $\\mu(\\cdot\\mid \\sigma)$.      Then it chooses $a^{*}(\\sigma) = a^{*}(\\mu(\\cdot\\mid\\sigma)) = \\arg\\max_{a}\\mathbb{E}_{s\\sim\\mu(\\cdot\\mid\\sigma)}\\left[r^j(s,a)\\right]$.      The mapping from signals to posterior beliefs is many-to-one: All the $\\sigma$ that induce the same posterior belief $\\mu(\\cdot\\mid \\sigma)$, the receiver will behave the same way (thus the payoffs of both agents).        The distribution of posterior beliefs $\\tau$          Every sent signal induces a specific posterior belief: Given a committed signaling scheme $\\varphi$ and a sent signal $\\sigma$, the receiver’s posterior belief is $\\mu(\\cdot\\mid\\sigma)$. Calculated as $\\mu(s_i \\mid\\sigma) = \\frac{\\mu_0(s_i)\\cdot \\varphi(\\sigma\\mid s_i)}{\\sum\\limits_{s_j}\\mu_0(s_j)\\cdot \\varphi(\\sigma\\mid s_j)}$.      Again: The mapping from signals to posterior beliefs is many-to-one.      A distribution of signals corresponds to a distribution of posterior beliefs: Before the signal $\\sigma$ realized, the receiver can only estimate its distribution by the committed signaling scheme: $\\sigma\\sim p^1_{\\mu_0, \\varphi}(\\cdot)  = \\sum\\limits_{s}\\mu_0(s)\\cdot \\varphi(\\cdot\\mid s)$. Then $\\mu\\sim p^2_{\\mu_0, \\varphi}(\\cdot) = \\sum\\limits_{\\sigma: \\mu(\\cdot\\mid \\sigma) = \\mu}\\sum\\limits_{s} \\mu_0(s)\\cdot\\varphi(\\sigma\\mid s)$.      A $\\tau$ induced by a $\\varphi$ is denoted as $\\tau_{\\mu_0,\\varphi}$.        The sender’s optimization problem          Original optimization: $\\max_{\\varphi} \\mathbb{E}_{s\\sim\\mu_0}\\Big[\\mathbb{E}_{\\sigma\\sim\\varphi(\\cdot\\mid s)}\\big[r^i(s,a^*(\\sigma))\\big]\\Big]$.      Reformulation:                  An equivalent objective function: $\\mathbb{E}_{\\mu\\sim\\tau}\\Big[\\mathbb{E}_{s\\sim\\mu}\\big[r^i(s,a^*(\\mu))\\big]\\Big]$.          The only contraint: $\\mathbb{E}_{\\mu\\sim\\tau}(\\mu) = \\mu_0$. (i.e. $\\tau = \\tau_{\\mu_0,\\varphi}$. This is named as Bayes plausible. See the next subsection.)                    Bayes plausible  The sender’s optimization problem can be approached from the perspective of any arbitrary distribution of induced posterior beliefs that satisfies the Bayes plausible. Understanding this is important for grasping the concepts of geometric interpretation (concavification) in the next subsection.If an arbitrary $\\tau$ satisfies $\\mathbb{E}_{\\mu\\sim\\tau}(\\mu) = \\mu_0$, then this $\\tau$ is Bayes plausible. (Kamenica 2019)  What is necessary and sufficient:          Every $\\tau_{\\mu_0,\\varphi}$ (a $\\tau$ induced by the $\\varphi$) is Bayes plausible. (It can be proved by the law of iterated expectations. Kamenica 2019)      If a $\\tau$ is Bayes plausible, then it can be induced by a $\\varphi$. (Kamenica 2019. Proved in Kamenica &amp; Gentzkow 2011.)        A geometric interpretation: probability simplex (Figure 2 of Dughmi 2019).          A point in the probability simplex represents a distribution of singals (a convex combination of signals).      The blue point is the prior belief. The red points are posterior beliefs.      Bayes plausible = The blue point is inside the simplex of the red points.      Concavification  Does the sender benefit from persuasion? What is an optimal mechanism?This technique is best described with a two-signal example (Figure 1 of Kamenica &amp; Gentzkow 2011, Kamenica 2019):  Assume that there are $n$ states. Then a posterior belief $\\mu$ is a point in $\\mathbb{R}^{n-1}$. (2011)  The values of $x$-axis are in $\\mathbb{R}^{n-1}$. Each represents a posterior belief $\\mu$. (2011)  A $\\mu$ induces an expected payoff of the sender $\\hat{v}(\\mu) = \\mathbb{E}_{s\\sim\\mu}\\big[r^i(s,a^{*}(\\mu))\\big]$. The black line in the figure denotes this function. (2011,2019)  $\\sigma_1$ and $\\sigma_2$ induce posterior beliefs $\\mu_1(\\cdot\\mid\\sigma_1)$ and $\\mu_2(\\cdot\\mid\\sigma_2)$ respectively. These two $\\mu$ correspond to two values on the $x$-axis, and indicate two expected payoff of the sender $\\hat{v}_1$ and $\\hat{v}_2$. (2019)  $\\mu_1$ and $\\mu_2$ are almost arbitrary: The distribution of $\\mu_1$ and $\\mu_2$ should be Bayes plausible, i.e., their expectation should be $\\mu_0$.          Assume that the distribution of $\\mu$ is $\\tau = (k, 1-k)$. Then $k\\cdot\\mu_1+(1-k)\\cdot\\mu_2 = \\mu_0$. Thus if $\\mu_{i}$ is on the left side of $\\mu_0$ on the $x$-axis, then $\\mu_{1-i}$ is on the right side. Unless they both coincide with $\\mu_0$.      Also, $\\mathbb{E}_{\\mu\\sim\\tau}(\\hat{v}(\\mu)) = k\\cdot \\hat{v}_1+(1-k)\\cdot\\hat{v}_2$, which is the sender’s expected payoff (obejective). It means that: When the receiver’s posterior beliefs are $\\mu_1,\\mu_2$ (after receiving $\\sigma_1, \\sigma_2$ respectively), the sender will gets $\\mathbb{E}_{\\mu\\sim\\tau}(\\hat{v}(\\mu))$.      Since the coordinates are proportional, connecting $(\\mu_1, \\hat{v}_1)$ and $(\\mu_2, \\hat{v}_2)$ to get a line segment $l$, $(\\mu_0, \\mathbb{E}_{\\mu\\sim\\tau}(\\hat{v}(\\mu)))$ is on $l$. In this way, the sender needs to manipulate the receiver’s posterior beliefs to find the highest intersection point.        A concavification of $\\hat{v}(\\mu)$ is the smallest concave function everywhere greater than $\\hat{v}(\\mu)$. (The red line in Figure 1 of 2011, 2019)          The concavification of $\\hat{v}(\\mu)$ evaluated at $\\mu_0$ equals $\\max\\set{y\\mid(\\mu_0, y)\\in co(\\hat{v})}$, where $co(\\hat{v})$ denotes the convex hull of the graph of $\\hat{v}$, i.e., the light blue region in Figure 1 of 2011.        In this case, a signaling scheme with more than two signals cannot improve the sender’s expected payoff. (Kamenica 2019)Corollaries and propositions(Kamenica &amp; Gentzkow 2011):  Corollary 1: The sender benefits from persuasion if and only if there exists a Bayes plausible distribution $\\tau$ of posterior belief $\\mu$ such that $\\mathbb{E}_{\\mu\\sim\\tau}\\Big[\\mathbb{E}_{s\\sim\\mu}\\big[r^i(s,a^{*}(\\mu))\\big]\\Big] &gt; \\mathbb{E}_{s\\sim\\mu_0}\\big[r^i(s,a^{*}(\\mu_0))\\big]$. The sender’s optimal expected payoff is $\\max_{\\varphi} \\mathbb{E}_{\\mu\\sim\\tau}\\Big[\\mathbb{E}_{s\\sim\\mu}\\big[r^i(s,a^*(\\mu))\\big]\\Big]$, s.t. $\\mathbb{E}_{\\mu\\sim\\tau}(\\mu) = \\mu_0$.  Corollary 2: The sender benefits from persuasion if and only if $f(\\mu_0)&gt;\\hat{v}(\\mu_0)$, where $\\hat{v}(\\mu) = \\mathbb{E}_{s\\sim\\mu}\\big[r^i(s,a^{*}(\\mu))\\big]$, and $f(\\mu)$ is the concavification of $\\hat{v}(\\mu)$. The sender’s optimal expected payoff is $f(\\mu_0)$.  Proposition 2: If $\\hat{v}(\\mu)$ is concave, the sender does not benefit from persuasion for any prior $\\mu_0$. If $\\hat{v}(\\mu)$ is convex and not concave, the sender benefits from persuasion for every prior $\\mu_0$.  Proposition 3: If there is no information the sender would share, the sender does not benefit from persuasion.          “There is information the sender would share” is defined as $\\exists \\mu$, s.t. $\\hat{v}(\\mu)&gt; \\sum\\limits_{s} \\mu(s)\\cdot r^j(s, a^{*}(\\mu_0))$.      In other words, there must exist a $\\mu$ such that, if the sender had private information that led it induces $\\mu$, it would prefer to share this information with the receiver rather than have the receiver act based on $\\mu_0$.      Otherwise, the sender does not have the motivation to share its information.        Proposition 4: If there is information the sender would share, and the receiver’s preference is discrete at the prior, the sender benefits from persuasion. (sufficient)          “The receiver’s preference is discrete at belief $\\mu$” is defined as: If there is an $\\epsilon&gt;0$ s.t. $\\forall a\\ne a^{*}(\\mu)$, $\\sum\\limits_{s}\\mu(s)\\cdot r^j(s, a^{*}(\\mu)) &gt; \\sum\\limits_{s} \\mu(s)\\cdot r^j(s, a)+\\epsilon$.      In other words, the receiver’s expected payoff from its preferred action $a^{*}(\\mu)$ is bounded away from its expected payoff from any other action.      Otherwise, the sender does not have the ability to influence the receiver.        Proposition 5: If $A$ is finite, the receiver’s preference is discrete at the prior generically.  Proposition 7: An optimal mechanism exists.  Proposition 8: For any prior,          if $\\hat{v}(\\mu)$ is (strictly) concave, no disclosure is (uniquely) optimal;                  no disclosure: $\\mu = \\mu_0$.                    if $\\hat{v}(\\mu)$ is (strictly) convex, full disclosure is (uniquely) optimal;                  full disclosure: $\\mu(\\cdot\\mid\\sigma)$ is degenerate for all $\\sigma$ sent in equilibirum. $\\mu$ is degenerate if there is $s$ s.t. $\\mu(s) = 1$.                    if $\\hat{v}(\\mu)$ is convex and not concave, strong disclosure is (uniquely) optimal.                  strong disclosure: $\\mu$ is at the boundary of $\\Delta(s)$ for all $\\sigma$ sent in equilibirum. (?)                    (Dughmi 2019):  If reward functions are identical (i.e. $r^i = r^j$), then the sender’s objective function is convex. The optimal signaling scheme is to reveal all the information.  If $(r^i+r^j)(s,a) = k, \\forall s,a$, where $k \\in \\mathbb{R}$, then the sender’s objective function is concave. The optimal signaling scheme is to reveal nothing (In this case, $\\mu = \\mu_0$).An Equilibrium Perspective(Bergemann &amp; Morris 2016)Settings in my understanding  This paper studies the persuasion of a group of receivers with private channels.  A game of incomplete information can be decomposed into a basic game and an observation structure (or information structure).          The basic game defines the set of actions, the set of payoff states, the payoff functions, and the common prior over the payoff states.      The observation structure refers to the function of partial observations that the players receive in the game.        An incomplete information game $\\mathcal{G} = (G, M)$.          A basic game $G = ((A^j, r^j)_{j=1}^{J}, \\mu_0)$.                  $A = \\prod_{j} A^j$.          $r^j: S\\times A \\to \\mathbb{R}$.          $\\mu_0 \\in \\Delta_{++}(S)$.                    An observation structure $M = ((O^j)_{j=1}^{J}, q)$.                  Partial observation (type) $O = \\prod_{j} O^j$.          Emission function $q: S \\to \\Delta(O)$.          A possible (and natural) observation structure is the null observation structure in which each player’s set of ovservations (tyes) $O^j$ is a singleton, $O^j = \\set{t^j}$.                      A signaling scheme (decision rule) $\\varphi: S\\times O \\to \\Delta(A)$.          There is an omniscient mediator, who first observes the sampled $s\\sim\\mu_0$ and the sampled $o\\sim q(\\cdot\\mid s)$, and then picks actons and privately announces $a^j$ to each receiver $j$.        A policy (behavioral strategy) for receiver $j$ in $(G, M)$ is $\\pi^j: O^j \\to \\Delta(A^j)$.Obedience &amp; BCEA decision rule $\\varphi$ is obedient for $(G, M)$ if, for each $j = 1, \\ldots, J$, $o^j\\in O^j$, and $a^j\\in A^j$, we have\\[\\begin{aligned}  &amp; \\sum\\limits_{s, o^{-j}, a^{-j}} \\mu_0(s)   \\cdot q\\Big((o^j, o^{-j})\\mid s \\Big)   \\cdot \\varphi\\Big( (a^j, a^{-j})\\mid s, (o^j, o^{-j}) \\Big)  \\cdot r^j\\Big(s, (a^j, a^{-j})\\Big) \\ge \\\\  &amp; \\sum\\limits_{s, o^{-j}, a^{-j}} \\mu_0(s)   \\cdot q\\Big((o^j, o^{-j})\\mid s \\Big)   \\cdot \\varphi\\Big( (a^j, a^{-j})\\mid s, (o^j, o^{-j}) \\Big)  \\cdot r^j\\Big(s, (a^{j\\prime}, a^{-j})\\Big)\\end{aligned}\\]for all $a^{j\\prime}\\in A^j$.  The definitions of obedience in (Bergemann &amp; Morris 2011a, 2011b) is a bit different from this.A decision rule $\\varphi$ is a Bayes correlated equilibrium (BCE) of $(G,M)$ if it is obedient for $(G,M)$.Degenerated BCE  There is complete information, i.e., if $S$ is a singleton: this definition reduces to the definition of correlated equilibrium (CE) for a complete information game. (Aumann 1987)      If $M$ is the null observation function (a singleton), then this is essentially the “universal Bayesian solution” of (Forges 1993).    A decision rule $\\varphi$ is obedient for $(G)$ if, for each $j = 1, \\ldots, J$ and $a^j\\in A^j$, we have\\[\\begin{aligned}   &amp; \\sum\\limits_{s, a^{-j}} \\mu_0(s)    \\cdot \\varphi\\Big( (a^j, a^{-j})\\mid s \\Big)   \\cdot r^j\\Big(s, (a^j, a^{-j})\\Big) \\ge \\\\   &amp; \\sum\\limits_{s, a^{-j}} \\mu_0(s)    \\cdot \\varphi\\Big( (a^j, a^{-j})\\mid s \\Big)   \\cdot r^j\\Big(s, (a^{j\\prime}, a^{-j})\\Big) \\end{aligned}\\]    for all $a^{j\\prime}\\in A^j$.        If $M$ is the null observation function, and if there is only one receiver, then this definition reduces to behavior in the concavification problem of (Aumann et al. 1995) and the Bayesian persuasion of (Kamenica &amp; Gentzkow 2011).    A decision rule $\\varphi$ is obedient for $(G)$ if, for each $a\\in A$, we have\\[\\sum\\limits_{s} \\mu_0(s)  \\cdot \\varphi( a\\mid s ) \\cdot \\Big( r^j(s, a) - r^j(s, a') \\Big) \\ge 0\\]    for all $a^{\\prime}\\in A$.    Why is it called obedience? Why do the receivers follow the recommendation?(Bergemann &amp; Morris 2019)\\[\\begin{aligned}  &amp; \\Leftrightarrow  \\sum\\limits_{s} \\frac{\\mu_0(s) \\cdot \\varphi( a\\mid s )}  { \\sum\\limits_{s'}\\mu_0(s') \\cdot \\varphi( a\\mid s')}  \\cdot \\Big( r^j(s, a) - r^j(s, a') \\Big) \\ge 0 , \\forall a'\\in A.\\\\  &amp; \\Leftrightarrow  \\sum\\limits_{s} \\mu(s\\mid a)  \\cdot \\Big( r^j(s, a) - r^j(s, a') \\Big) \\ge 0 , \\forall a'\\in A.\\end{aligned}\\]BNEA strategy profile (joint policy) $\\pi$ is a Bayes Nash equilibrium (BNE) of $(G, M)$ if for each $j = 1 , \\ldots, J$, $\\sigma^j\\in\\Sigma^j$, and $a^j\\in A^j$ with $\\pi^j(a^j \\mid o^j) &gt; 0$, we have\\[\\begin{aligned}  &amp; \\sum\\limits_{s, o^{-j}, a^{-j}} \\mu_0(s)   \\cdot q\\Big((o^j, o^{-j})\\mid s \\Big)   \\cdot \\Bigg( \\prod_{j \\ne k} \\pi^k(a^k \\mid o^k) \\Bigg)  \\cdot r^j\\Big(s, (a^j, a^{-j})\\Big) \\ge \\\\  &amp; \\sum\\limits_{s, o^{-j}, a^{-j}} \\mu_0(s)   \\cdot q\\Big((o^j, o^{-j})\\mid s \\Big)   \\cdot \\Bigg( \\prod_{j \\ne k} \\pi^k(a^k \\mid o^k) \\Bigg)  \\cdot r^j\\Big(s, (a^{j\\prime}, a^{-j})\\Big)\\end{aligned}\\]for all $a^{j\\prime}\\in A^j$.  The following part has not been finished yet. One may check my writing schedule.Extensions  Multiple senders  Multiple receivers  Dynamic environment  Others:          The receiver has private information      An Ethical JustificationI do not think information design is immoral. Information is a kind of property of the sender, and it is legal for it to profit from its information. (Unless the utilized information is a public resource.)Furthermore, in those cases where the sender can improve its own expected payoff through information design, the receiver’s payoff is not worse than that of the sender not reveal information at all.Nevertheless, practice use of information design should take the sender’s objective function into some serious consideration.  Disclaimer: The above content is summarized from the mentioned papers. Corresponding links or references have been provided."
  },
  
  {
    "title": "MARL Tasks",
    "url": "/posts/MARL-Tasks/",
    "categories": "Artificial Intelligence, Multi-Agent Reinforcement Learning",
    "tags": "tech, multi agents, environment",
    "date": "2023-05-31 12:00:01 +0000",
    





    
    "snippet": "  This note will be consistently updated.List  StarCraft II          SMAC (StarCraft Multi-Agent Challenge). SMAC is WhiRL’s environment for research in the field of collaborative multi-agent reinf...",
    "content": "  This note will be consistently updated.List  StarCraft II          SMAC (StarCraft Multi-Agent Challenge). SMAC is WhiRL’s environment for research in the field of collaborative multi-agent reinforcement learning (MARL) based on Blizzard’s StarCraft II RTS game.      PySC2 (StarCraft II Learning Environment). PySC2 is DeepMind’s Python component of the StarCraft II Learning Environment (SC2LE).      gym-starcraft. An OpenAI Gym interface to StarCraft.        MAagent          persuit: predators pursuit preys      gather: agents rush to gather food      battle: battle between two armies      arrange: arrange agents into some characters      A demo video        Overcooked  MiniRTS          Paper      GitHub        Cards &amp; Chess          Go      Texas Hold’em      Mahjong      Hanabi [code] [paper]        Communication  Melting Pot (Deep Mind)          Sequential Social Dilemma (SSD)      Harvest      Clean Up        Level-Based ForagingOvercookedThis environment is designed to test the human-ai coordination, or used as a zero-shot coordination task.Resources  [GitHub Repo: overcooked_ai]  [Online Game]  [An Environment Wrapper: PantheonRL]HanabiObjectivePlayers work collaboratively to put on a firework show by placing a series of cards in the correct order. The objective is to complete a series of five cards of the same color in ascending numerical order (1 to 5) for each color.  Cards Facing Outwards: One distinctive rule of Hanabi is that players hold their cards facing outwards, meaning they can see everyone else’s cards but their own. This facilitates a cooperative environment where players rely on each other’s hints to figure out their cards.  Communication: Communication is restricted to the formal hint-giving process to maintain the game’s difficulty and collaborative spirit.Components  Deck: Consists of 50 cards, distributed into 5 different colors (red, yellow, green, blue, white), with each color having numbers from 1 to 5 (1s x3, 2s x2, 3s x2, 4s x2, and 5s x1).  Hint tokens: 8 in total, used to give hints to other players.  Fuse tokens: 3 in total, representing the players’ “lives”.Setup  Number of players: 2-5 players.  Hand size: Depending on the number of players, each player starts with a hand of cards (5 cards for 2-3 players, 4 cards for 4-5 players).  Initial layout: Players hold their cards facing outwards so they can’t see their cards but can see others’.GameplayPlayers take turns clockwise and can choose to perform one of the following actions on their turn:      Give a hint: A player may give a hint to another player about the contents of their hand. The hint must be about either the color or the number of the cards, not both. It consumes a hint token. It must relate to at least one card in the other player’s hand, and it must give information about all the cards of the chosen characteristic.        Discard a card: A player may choose to discard a card from their hand to regain a hint token. The discarded card is placed in the discard pile and the player draws a new card from the draw pile.        Play a card: A player may choose to play a card from their hand onto the table. If the card successfully fits into a fireworks display (i.e., it is the next number in sequence of the same color), it stays on the table. Otherwise, it goes to the discard pile and a fuse token is removed.  Game Ending ConditionsThe game can end in the following ways:  Successful Completion: Players successfully play all five cards in each color.  Fuse Tokens Exhausted: Players make three mistakes and lose all fuse tokens.  Deck Exhaustion: The draw pile is exhausted. Players get one final round before the game ends.ScoringAt the end of the game, the score is calculated based on the number of cards successfully played in the fireworks displays. The highest possible score is 25 (if all fireworks are successfully completed). The score is determined by summing the highest value of cards played for each color.Strategy and Tips  Memory and Deduction: Players must rely heavily on their memory and deduction skills to remember the hints given and to figure out which cards they hold.  Non-verbal cues: Players are not allowed to give extra hints through non-verbal cues or suggestive comments. All hints must be given using hint tokens officially in one’s turn.  Hint Efficiency: Given the limited number of hint tokens, players should strive to give hints that convey the maximum amount of information."
  },
  
  {
    "title": "Personality",
    "url": "/posts/Personality/",
    "categories": "Efficiency, Psychology",
    "tags": "life, efficiency, psychology, personality",
    "date": "2023-05-19 18:40:00 +0000",
    





    
    "snippet": "ForewordOne may choose to disregard their own labels (and bear the consequences, good or bad, of doing so), believing that the labeling system is flawed. However, they cannot claim to be without la...",
    "content": "ForewordOne may choose to disregard their own labels (and bear the consequences, good or bad, of doing so), believing that the labeling system is flawed. However, they cannot claim to be without labels or cannot be defined.This post aims to briefly introduce three personality models: The Big Five, Myers-Briggs Type Indicator (MBTI), and Enneagram. The Big Five is generally well-regarded and supported by research in the psychology community, whereas the MBTI and Enneagram are viewed more critically, with concerns about their scientific validity and reliability.But I believe that these tools are all efficient communication protocols, and using them allows me to easily introduce my profile.Because if these models are common knowledge between me and my readers, then my test results will encapsulate a wealth of information about my behavioral habits and thought preferences.After understanding these models, analyzing the characters in film and literary works will also yield many interesting new perspectives.Big Five Personality TestThe Big Five personality traits system is a widely accepted framework in psychology, consisting of five broad dimensions that describe human personality:  Openness (inventiveness and curiosity vs. consistency and caution),  Conscientiousness (efficiency and organization vs. easy-going and careless),  Extraversion (outgoing and energetic vs. solitary and reserved),  Agreeableness (friendly and compassionate vs. challenging and detached), and  Neuroticism (sensitive and nervous vs. secure and confident).This model is empirically driven and is considered robust for understanding personality due to its extensive research base, cross-cultural validity, and ability to predict various life outcomes. It’s praised for capturing the complexity of human personality traits in a comprehensive yet flexible manner.My test result[My test result in 2023]: SCOAI.                   Abbr.      High      Low                  Extraversion      E      S      R              Neuroticism      N      L      C              Conscientiousness      C      O      U              Agreeableness      A      A      E              Openness To Experience      O      I      N                  Openness To Experience (101)      Conscientiousness (96)      Agreeableness (76)      Neuroticism (67)      Extraversion (76)                  Imagination (19)      Self-Efficacy (16)      Trust (9)      Anxiety (14)      Friendliness (11)              Artistic Interests (15)      Orderliness (18)      Morality (15)      Anger (9)      Gregariousness (9)              Emotionality (16)      Dutifulness (13)      Altruism (14)      Depression (9)      Assertiveness (15)              Adventurousness (17)      Achievement-Striving (17)      Cooperation (15)      Self-Consciousness (12)      Activity Level (16)              Intellect (20)      Self-Discipline (16)      Modesty (10)      Immoderation (12)      Excitement-Seeking (14)              Liberalism (14)      Cautiousness (16)      Sympathy (13)      Vulnerability (11)      Cheerfulness (11)      In the first row, each item has a full score of 120, while in the other rows, each item has a full score of 20.  The detailed description of each dimension can be found in [My Test Result in 2023].Myers-Briggs Type Indicator (MBTI)The Myers-Briggs Type Indicator (MBTI) is another popular personality framework, particularly in non-academic settings like business and career counseling. It categorizes people into 16 distinct personality types based on four dichotomies:  Introversion vs. Extraversion. This dimension describes how individuals draw and expend their energy. Extraverts (E) are energized by interaction with others and the external world. They tend to be sociable, outgoing, and more comfortable in groups or social situations. Introverts (I), in contrast, are energized by spending time alone or in quiet environments. They are often reflective, reserved, and more comfortable working independently or in smaller groups. This preference doesn’t necessarily determine social skills or shyness; it’s more about where one gets their energy from.  Sensing vs. Intuition. This dimension indicates how individuals prefer to gather information. People who prefer Sensing (S) focus on the present and concrete information gained from their senses. They are detail-oriented, practical, and prefer to deal with facts and real-world applications. Intuitive types (N), however, are more focused on possibilities and what might be. They enjoy thinking about the future, abstract concepts, and the big picture. They value imagination and innovation more than practical applications.  Thinking vs. Feeling. This dimension describes how people prefer to make decisions. Thinkers (T) make decisions based on logic and objective analysis. They value principles, consistency, and impersonal truth, often focusing on the task rather than people. Feelers (F), on the other hand, are more concerned with harmony and the well-being of others. Their decisions are influenced more by personal values and the impact on people. They are empathetic, compassionate, and tend to prioritize relationships in their decision-making process.  Judging vs. Perceiving. This dimension relates to how individuals prefer to organize their lives. Judging types (J) like structure, firm decisions, and clear rules. They prefer planning and organizing their lives and tend to be decisive, efficient, and task-focused. Perceiving types (P), conversely, prefer to remain open to new information and options. They are more spontaneous, flexible, and adaptable. Perceivers are less structured and more comfortable adapting to changes and exploring various possibilities.Furthermore, there is a fifth letter in some MBTI representations, such as “INTJ-T,” originates from the 16Personalities framework and represents an additional dimension known as “Identity,” which is not part of the original MBTI model.  Assertive vs. Turbulent. This dimension is divided into “Assertive” (A) and “Turbulent” (T) traits. Assertive individuals are typically confident, relaxed, and resistant to stress, maintaining a stable level of emotional resilience. In contrast, Turbulent individuals are more likely to be self-conscious, sensitive to stress, and driven by a desire for improvement, often experiencing wider emotional fluctuations and a strong sense of ambition. This Assertive/Turbulent aspect, blending elements from the Big Five personality traits, adds another layer to personality assessment, offering a nuanced view of one’s overall disposition and reaction to stress and change.Developed from Carl Jung’s theory of psychological types, MBTI is often used for personal development, career planning, and team building. However, it has faced criticism from the academic psychology community due to concerns about its scientific validity, reliability, and lack of empirical support, as people may receive different type designations upon retaking the test.My test result[My test result in 2023]: INTJ-T (The Turbulent Architect).  Introverted, Intuitive, Thinking, Judging, and Turbulent;  Strategy: Constant Improvement;  Dominant (Jungian) Cognitive Functions: Introverted Intuition (Ni) and Extraverted Thinking (Te).Jungian cognitive functionsThe MBTI is heavily influenced by the theories of Swiss psychiatrist Carl Jung, particularly his concept of cognitive functions.Jung proposed a theory of psychological types in his 1921 work “Psychological Types.”He introduced the concepts of introversion and extraversion as the main orientations of personality and added four psychological functions through which people experience the world: thinking, feeling, sensation, and intuition.These functions could operate in either an extraverted or introverted mode, leading to a total of eight distinct functions in his theory.  Cognitive functions, also referred to as psychological functions, as described by Carl Jung in his book Psychological Types, are particular mental processes within a person’s psyche that are present regardless of common circumstance. This is a concept that serves as one of the foundations for his theory on personality type. In his book, he noted four main psychological functions: thinking, feeling, sensation, and intuition. He introduced them with having either an internally focused (introverted) or externally focused (extraverted) tendency which he called “attitudes”. He also categorizes the functions as either rational (thinking and feeling) or irrational (intuition and sensation).— Wikipedia.The MBTI was developed by Katharine Cook Briggs and her daughter Isabel Briggs Myers in the 1940s. They were influenced by Jung’s theory and sought to apply it in a more practical, accessible way.While they retained Jung’s basic concepts of extraversion/introversion and the four functions (thinking, feeling, sensing, and intuition), they added a new dimension: Judging vs. Perceiving. This was not explicitly part of Jung’s original theory but was inferred from his descriptions of how people use their judging (thinking and feeling) and perceiving (sensing and intuition) functions.In MBTI, the combination of Extraversion vs. Introversion with the four functions results in eight possible cognitive functions (e.g., extraverted thinking, introverted feeling, etc.).These functions are used to determine the 16 MBTI types, with each type having a primary function that dominates their personality. The MBTI also considers secondary and tertiary functions in its personality descriptions, attempting to create a more holistic view of each personality type.In summary, the MBTI is an adaptation and extension of Carl Jung’s theory of cognitive functions. It maintains the core of Jung’s ideas but simplifies and expands them to create a more structured and easily applicable personality typology. However, it’s important to note that the MBTI’s interpretation and application of Jung’s theories are not without criticism, particularly from academic psychologists who question the MBTI’s scientific validity and reliability.Table 1 from Wikipedia.Table 2 from Wikipedia.Table 3 from Wikipedia.Introverted Intuition (Ni) is an information-gathering function, but unlike its counterpart, Extroverted Intuition, it processes information in a more internal and reflective manner. Individuals who primarily use Introverted Intuition tend to focus on the future, relying on insights and impressions rather than concrete facts. They often perceive patterns and connections that are not immediately obvious to others. Ni users are typically deep thinkers, valuing concepts, theories, and abstract ideas. They have a natural inclination towards understanding complex systems and are often driven by a vision or a sense of where things are heading. This introspective function can sometimes lead them to seem detached from the present, as they are more engaged with their internal world of ideas and possibilities.Extroverted Thinking (Te) is an extroverted function, meaning it is oriented towards the external world. Te is logical, systematic, and focused on organizing and structuring the environment. Individuals with a dominant Extroverted Thinking function are typically very good at setting goals, making plans, and implementing strategies. They prioritize efficiency, productivity, and are often drawn to positions of management or leadership due to their ability to make objective, logical decisions and to create order and consistency in their surroundings. Te users are practical and straightforward, preferring clear communication and measurable outcomes. They excel in taking complex ideas and organizing them in a way that is understandable and actionable, often thriving in environments where their skills in organization and leadership can be put to good use.Introverted Feeling (Fi) is featured in the Myers-Briggs Type Indicator (MBTI). Fi is a decision-making function that is focused inwardly, dealing with emotions and values. Individuals who primarily use Introverted Feeling process their feelings internally and place a high emphasis on personal, often subjective, values and morals. They tend to be guided by these internal values when making decisions, rather than external sources of morality or societal expectations. Fi users are often very aware of their own emotions and seek to understand and align their actions with their inner sense of integrity and authenticity. This can make them seem reserved or private, as they prefer to reflect deeply on their feelings rather than express them openly. They are often empathetic and compassionate, with a strong sense of individualism and a deep commitment to living in harmony with their personal ethical code.Extroverted Sensing (Se) is a perceiving function that focuses on the external world, emphasizing the present moment and concrete sensory experience. Individuals with a dominant Extroverted Sensing function are typically very aware of their physical environment and enjoy engaging with it. They are often pragmatic and realistic, preferring direct interaction with the world around them. Se users tend to be action-oriented, enjoying activities that involve sensory stimulation and physical engagement. They are often adept at responding to immediate situations and can be very spontaneous and adaptable. This focus on the here and now can sometimes lead to a lesser emphasis on planning for the future or reflecting on the past. Extroverted Sensing is associated with a love for aesthetics, a keen awareness of their surroundings, and a preference for hands-on experiences.EnneagramThe Enneagram is a personality system that describes nine primary personality types, each with its own set of characteristics, motivations, fears, and desires. Its origins are more spiritual and mystical, tracing back to ancient traditions. The Enneagram has become popular in various self-help and spiritual contexts and is sometimes used in counseling for personal and relational growth. Despite its popularity in these areas, the Enneagram is viewed with skepticism by the mainstream psychology community due to its lack of empirical evidence and scientific grounding. Critics point out that its categories are too vague and subjective, making it difficult to validate and study scientifically.This website provides a great introduction to the various types of the Enneagram.My test result[My test result in 2023]: Type 5 with a 4-Wing.  My test result at this website.  Fives (The Investigators) are alert, insightful, and curious. They are able to concentrate and focus on developing complex ideas and skills. Independent, innovative, and inventive, they can also become preoccupied with their thoughts and imaginary constructs. They become detached, yet high-strung and intense. They typically have problems with eccentricity, nihilism, and isolation. At their Best: visionary pioneers, often ahead of their time, and able to see the world in an entirely new way.      Basic Fear: Being useless, helpless, or incapable    Basic Desire: To be capable and competent    Enneagram Five with a Four-Wing: “The Iconoclast”    Enneagram Five with a Six-Wing: “The Problem Solver”    Key Motivations: Want to possess knowledge, to understand the environment, to have everything figured out as a way of defending the self from threats from the environment.  When moving in their Direction of Disintegration (stress), detached Fives suddenly become hyperactive and scattered at Seven. However, when moving in their Direction of Integration (growth), avaricious, detached Fives become more self-confident and decisive, like healthy Eights.  — From this webpage.TritypeThe tritype theory suggests that while individuals have a primary Enneagram type, they also exhibit significant characteristics of two other types.These three types – one from each of the Enneagram’s centers of intelligence (Heart/Feeling, Head/Thinking, Gut/Instinct) – combine to form a person’s tritype. For example, someone could have a tritype of 4-7-1, meaning their primary type is in the Heart center (type 4), but they also strongly identify with a type in the Head center (type 7) and the Gut center (type 1).My tritye is 548.  Fours (The Individualists) are self-aware, sensitive, and reserved. They are emotionally honest, creative, and personal, but can also be moody and self-conscious. Withholding themselves from others due to feeling vulnerable and defective, they can also feel disdainful and exempt from ordinary ways of living. They typically have problems with melancholy, self-indulgence, and self-pity. At their Best: inspired and highly creative, they are able to renew themselves and transform their experiences.  Basic Fear: That they have no identity or personal significanceBasic Desire: To find themselves and their significance (to create an identity)Enneagram Four with a Three-Wing: “The Aristocrat”Enneagram Four with a Five-Wing: “The Bohemian”  Key Motivations: Want to express themselves and their individuality, to create and surround themselves with beauty, to maintain certain moods and feelings, to withdraw to protect their self-image, to take care of emotional needs before attending to anything else, to attract a “rescuer.”  When moving in their Direction of Disintegration (stress), aloof Fours suddenly become over-involved and clinging at Two. However, when moving in their Direction of Integration (growth), envious, emotionally turbulent Fours become more objective and principled, like healthy Ones.  — From this webpage.  Eights (The Challengers) are self-confident, strong, and assertive. Protective, resourceful, straight-talking, and decisive, but can also be ego-centric and domineering. Eights feel they must control their environment, especially people, sometimes becoming confrontational and intimidating. Eights typically have problems with their tempers and with allowing themselves to be vulnerable. At their Best: self- mastering, they use their strength to improve others’ lives, becoming heroic, magnanimous, and inspiring.  Basic Fear: Of being harmed or controlled by othersBasic Desire: To protect themselves (to be in control of their own life and destiny)Enneagram Eight with a Seven-Wing: “The Maverick”Enneagram Eight with a Nine-Wing: “The Bear”  Key Motivations: Want to be self-reliant, to prove their strength and resist weakness, to be important in their world, to dominate the environment, and to stay in control of their situation.  When moving in their Direction of Disintegration (stress), self-confident Eights suddenly become secretive and fearful at Five. However, when moving in their Direction of Integration (growth), lustful, controlling Eights become more open-hearted and caring, like healthy Twos.  — From this webpage."
  },
  
  {
    "title": "For Prospective Undergraduates",
    "url": "/posts/A-Document-for-Prospective-Undergraduates/",
    "categories": "Misc Notes",
    "tags": "misc note",
    "date": "2023-05-18 12:00:00 +0000",
    





    
    "snippet": "  Welcome! 🎉Preliminariesif you are wondering whether you are …capable:  PyTorch  GitHub  Weights &amp; Biases  Advantage Actor Critic (A2C)  You have a relatively large amount of time.  A straight...",
    "content": "  Welcome! 🎉Preliminariesif you are wondering whether you are …capable:  PyTorch  GitHub  Weights &amp; Biases  Advantage Actor Critic (A2C)  You have a relatively large amount of time.  A straightforward test:Reproduce A2C to complete any RL task (development needs to take place within a GitHub repository), and utilize Weights &amp; Biases for experiment visualization and hyperparameter tuning.interested in our topic:Currently, we are focusing on information design (a subfield of computational economics and game theory) and multi-agent reinforcement learning (MARL). Our next few works will be based on:  Information Design in Multi-Agent Reinforcement Learning.    Yue Lin, Wenhao Li, Hongyuan Zha, Baoxiang Wang.    arXiv 2023.  For more specific details, please contact me so that we can discuss in person.Recommended ReadingsOur communication will be smoother if you have read some of the following papers.MARL in Sequential Social Dilemmas  MARL in Sequential Social Dilemmas  Melting PotInformation Design  Bayesian Persuasion  Bayes Correlated Equilibrium and the Comparison of Information Structures in Games  Surveys:          Bayesian Persuasion and Information Design      Algorithmic Information Structure Design: A Survey      Mechanism Design in MARL  LIO  The AI Economist: TaxationMARL Algorithms  MADDPG  COMA  QMIXMARL Communication  RIAL,DIAL  CommNet"
  },
  
  {
    "title": "RL Toolbox",
    "url": "/posts/RL-Toolbox/",
    "categories": "Artificial Intelligence, Reinforcement Learning",
    "tags": "tech, toolbox, reinforcement learning",
    "date": "2023-04-10 18:40:00 +0000",
    





    
    "snippet": "  This note will be consistently updated.PPO TricksThere are a total of 37 tricks, among which 13 are relatively core.      PPO paper    The 37 Implementation Details of Proximal Policy Optimizatio...",
    "content": "  This note will be consistently updated.PPO TricksThere are a total of 37 tricks, among which 13 are relatively core.      PPO paper    The 37 Implementation Details of Proximal Policy Optimization.    影响PPO算法性能的10个关键技巧（附PPO算法简洁Pytorch实现） - Beaman的文章 - 知乎  Adam Optimizer Epsilon Parameterself.actor_optim = torch.optim.Adam(self.actor.parameters(), lr=config.lr_actor, eps=1e-5)self.critic_optim = torch.optim.Adam(self.critic.parameters(), lr=config.lr_critic, eps=1e-5)Gradient Clipself.critic_optim.zero_grad()critic_loss.backward()torch.nn.utils.clip_grad_norm_(self.critic.parameters(), config.clip_range) # hereself.critic_optim.step()self.actor_optim.zero_grad()loss_actor.mean().backward()torch.nn.utils.clip_grad_norm_(self.actor.parameters(), config.clip_range) # hereself.actor_optim.step()Tanh Activation Function# A continuous actorclass Actor(torch.nn.Module):    def __init__(self):        super(Actor, self).__init__()        self.mlp = torch.nn.Sequential(            torch.nn.Linear(config.state_size, config.mlp_dim), torch.nn.Tanh(),            torch.nn.LayerNorm(config.mlp_dim),            torch.nn.Linear(config.mlp_dim, config.action_num),            torch.nn.Tanh()        )        self.log_std = torch.nn.Parameter(torch.zeros(1, config.action_num))  # Gaussian std, learnable    def forward(self, state):        mean_raw = self.mlp(state)  # [-1, 1]        mean = mean_raw * config.action_space_range  # [-max_a, max_a]        std = torch.exp(self.log_std)  # std=exp(log_std)&gt;0        distribution = torch.distributions.Normal(mean, std)        return distributionPolicy Entropydef choose_action(self, state):    state_tensor = torch.tensor(state).to(torch.float32).squeeze()    distribution = self.actor(state_tensor)    dist_entropy = distribution.entropy()    action = distribution.sample().squeeze(dim=0)    log_prob = distribution.log_prob(action)    return action.detach().numpy(), log_prob, dist_entropynegative_loss_actor = log_prob * TD_error.detach() + dist_entropy * config.entropy_coeloss_actor = - negative_loss_actorself.actor_optim.zero_grad()loss_actor.mean().backward()self.actor_optim.step()Reward ScalingIncremental meanI have a dataset with $n$ samples ${x_1, x_2, \\ldots, x_n}$. The expectation of $X$ is calculated as\\[\\mu_n = \\frac{1}{n}\\sum\\limits_{i=1}^n x_i\\]Then I get a new sample $x_{n+1}$, then the expectation of $X$ should be updated. And it can be represented by the current expectation:\\[\\mu_{n+1} = \\mu_n + \\frac{1}{n+1}\\left(x_{n+1} - \\mu_n \\right)\\]Derivation:\\[\\begin{aligned}  \\mu_{n+1} =&amp; \\frac{1}{n+1}\\sum\\limits_{i=1}^{n+1} x_i   =  \\frac{1}{n+1}\\left(x_{n+1} + \\sum\\limits_{i=1}^n x_i \\right) \\\\  =&amp;  \\frac{1}{n+1}x_{n+1} + \\frac{n}{n+1} \\sum\\limits_{i=1}^n x_i \\\\  =&amp;  \\frac{1}{n+1}x_{n+1} + \\left(1 - \\frac{1}{n+1}\\right) \\mu_n \\\\  =&amp; \\mu_n + \\frac{1}{n+1}\\left(x_{n+1} - \\mu_n \\right)\\end{aligned}\\]To reduce the impact of previous samples, the coefficient is fixed as a constant $\\alpha$:\\[\\begin{aligned}  \\mu_{n+1}   =&amp; \\mu_n + \\alpha\\left(x_{n+1} - \\mu_n \\right) \\\\  =&amp; \\alpha\\cdot x_{n+1} - \\left(1-\\alpha\\right)\\cdot\\mu_n \\end{aligned}\\]Incremental Variance\\[s_n^2 = s_{n-1}^2 + \\frac{(x_n - \\mu_{n-1})(x_n - \\mu_n)}{n}\\]\\[s_n^2 = \\frac{1}{n} \\sum_{i=1}^n (x_i - \\mu_n)^2\\]\\[\\mu_n = \\mu_{n-1} + \\frac{1}{n}(x_n - \\mu_{n-1})\\]\\[\\begin{aligned}s_n^2 =&amp; \\frac{1}{n} \\sum_{i=1}^n \\left(x_i - \\mu_{n-1} - \\frac{1}{n}(x_n - \\mu_{n-1})\\right)^2 \\\\=&amp; \\frac{1}{n} \\sum_{i=1}^{n-1}(x_i - \\mu_{n-1})^2 + \\frac{1}{n}(x_n - \\mu_{n-1})^2 - 2\\frac{1}{n}(x_n - \\mu_{n-1})\\sum_{i=1}^{n-1}(x_i - \\mu_{n-1}) + \\frac{1}{n^2}(x_n - \\mu_{n-1})^2\\sum_{i=1}^{n-1}1\\end{aligned}\\]\\[\\sum_{i=1}^{n-1}(x_i - \\mu_{n-1}) = 0\\]\\[\\begin{aligned}s_n^2 =&amp; \\frac{1}{n} \\sum_{i=1}^{n-1}(x_i - \\mu_{n-1})^2 + \\frac{1}{n}(x_n - \\mu_{n-1})^2 - \\frac{n-1}{n^2}(x_n - \\mu_{n-1})^2 \\\\=&amp; \\frac{n-1}{n}s_{n-1}^2 + \\frac{1}{n}(x_n - \\mu_{n-1})^2 - \\frac{n-1}{n^2}(x_n - \\mu_{n-1})^2 \\\\=&amp; s_{n-1}^2 + \\frac{1}{n}(x_n - \\mu_{n-1})(x_n - \\mu_n)\\end{aligned}\\]Embedding for the Q-value Critic  Check the implementation of DIAL.In my understanding, after going through the embedding, inputs with different ranges can be considered as linearly independent quantities in the same space, so they can be added directly.# From the CoLab: https://colab.research.google.com/gist/MJ10/2c0d1972f3dd1edcc3cd17c636aac8d2/dial.ipynb#scrollTo=G5e0IeqmIJJjclass CNet(nn.Module):    def __init__(self, opts):        \"\"\"        Initializes the CNet model        \"\"\"        super(CNet, self).__init__()        self.opts = opts        self.comm_size = opts['game_comm_bits']        self.init_param_range = (-0.08, 0.08)        ## Lookup tables for the state, action and previous action.        self.action_lookup = nn.Embedding(opts['game_nagents'], opts['rnn_size'])        self.state_lookup = nn.Embedding(2, opts['rnn_size'])        self.prev_action_lookup = nn.Embedding(opts['game_action_space_total'], opts['rnn_size'])        # Single layer MLP(with batch normalization for improved performance) for producing embeddings for messages.        self.message = nn.Sequential(            nn.BatchNorm1d(self.comm_size),            nn.Linear(self.comm_size, opts['rnn_size']),            nn.ReLU(inplace=True)        )        # RNN to approximate the agent’s action-observation history.        self.rnn = nn.GRU(input_size=opts['rnn_size'], hidden_size=opts['rnn_size'], num_layers=2, batch_first=True)        # 2 layer MLP with batch normalization, for producing output from RNN top layer.        self.output = nn.Sequential(            nn.Linear(opts['rnn_size'], opts['rnn_size']),            nn.BatchNorm1d(opts['rnn_size']),            nn.ReLU(),            nn.Linear(opts['rnn_size'], opts['game_action_space_total'])        )    def forward(self, state, messages, hidden, prev_action, agent):        \"\"\"        Returns the q-values and hidden state for the given step parameters        \"\"\"        state = Variable(torch.LongTensor(state))        hidden = Variable(torch.FloatTensor(hidden))        prev_action = Variable(torch.LongTensor(prev_action))        agent = Variable(torch.LongTensor(agent))        # Produce embeddings for rnn from input parameters        z_a = self.action_lookup(agent)        z_o = self.state_lookup(state)        z_u = self.prev_action_lookup(prev_action)        z_m = self.message(messages.view(-1, self.comm_size))        # Add the input embeddings to calculate final RNN input.        z = z_a + z_o + z_u + z_m        z = z.unsqueeze(1)        rnn_out, h = self.rnn(z, hidden)        # Produce final CNet output q-values from GRU output.        out = self.output(rnn_out[:, -1, :].squeeze())        return h, outGumbel-Softmax  Reparameterization.  Maintain gradients from the sampled variables.  Commonly used in communication methods.What is gumbel-softmax for?If $a_t\\sim \\pi_\\theta(\\cdot \\mid s_t)$, then how to calculate $\\nabla_\\theta a_t$?What is reparameterization?This trick decouples the deterministic part and the random part of a variable.This concept can be best illustrated with the example of the Gaussian distribution.If $z\\sim \\mathcal{N}(\\mu,\\sigma^2)$, then $z = \\mu + \\sigma \\cdot \\epsilon$, where $\\epsilon\\sim \\mathcal{N}(0,1)$. In this way, $\\frac{\\partial z}{\\partial \\mu} = 1$ and $\\frac{\\partial z}{\\partial \\sigma} = \\epsilon$. Usually $\\mu$ and $\\sigma$ are estimated by a neural network, and the following gradient can be automatically calculated by deep frameworks.What does Gumbel-Softmax do?We often use neural networks to generate a probability simplex, i.e., a profile of probability where $0\\le p_i$ and $\\sum\\limits_{i} p_i = 1$. Then we will sample an $x$ based on this distribution.An example scenario is in RL, where an agent needs to choose an action $a_t$. We output a distribution $\\pi(\\cdot \\mid s_t)$ and then sample an action $a_t\\sim \\pi(\\cdot \\mid s_t)$ based on this distribution to execute.Gumbel-Softmax is used to reparameterization this kind of categorical distribution. This technique allows samples to be drawn according to the original distribution and enables gradient computation.\\[z\\sim \\arg\\max\\limits_i (\\log(p_i) + g_i),\\]where $g_i = -\\log(-\\log (u_i)), u_i\\sim U(0,1)$.The argmax is non-differentiable, it can be replaced with softmax. $i = \\arg\\max\\limits_{j} (x_j)$.\\[\\mathrm{softmax}_T (x) = \\frac{e^{x_j/T}}{\\sum_k e^{x_k/T}}.\\]If temperature $T$ is small enough, then the output of the softmax can be seen as a one-hot vector which indicates $i$.$x\\ne \\log(\\mathrm{softmax}(x))$\\[\\mathrm{softmax}(x_i) = \\frac{e^{x_i}}{\\sum_{j=1}^{n} e^{x_j}}\\]\\[\\begin{aligned}    \\log(\\mathrm{softmax}(x_i))     =&amp; \\log\\left(\\frac{e^{x_i}}{\\sum_{j=1}^{n} e^{x_j}}\\right) \\\\    =&amp; x_i - \\log\\left(\\sum_{j=1}^{n} e^{x_j}\\right)\\end{aligned}\\]import torchx = torch.rand(5)x1 = torch.nn.Softmax(dim=0)(x)x2 = torch.nn.functional.softmax(x, dim=0)x3 = torch.nn.functional.log_softmax(x, dim=0)print(x1)print(x2)print(torch.log(x1))print(x3)tensor([0.1385, 0.1978, 0.2231, 0.2861, 0.1543])tensor([0.1385, 0.1978, 0.2231, 0.2861, 0.1543])tensor([-1.9766, -1.6204, -1.4999, -1.2512, -1.8686])tensor([-1.9766, -1.6204, -1.4999, -1.2512, -1.8686])Example codeimport torchif __name__ == '__main__':    batch_size = int(1e7)    logits_distribution = [2, 3]    logits_batch = torch.tensor(logits_distribution, dtype=torch.float64) \\        .unsqueeze(dim=0).expand(batch_size, len(logits_distribution))    softmax = torch.nn.Softmax(dim=-1)    pi = softmax(logits_batch)    # -----    # The standard way.    temperature = 1    actions_sampled = torch.nn.functional.gumbel_softmax(logits_batch, tau=temperature, hard=True)    a0_num = torch.sum(actions_sampled[:, 0])    a1_num = torch.sum(actions_sampled[:, 1])    print(pi[0], a0_num, a1_num, sep=\"\\n\")    # -----    # In RL, the common epsilon-greedy is a operation on the policy space.    # To sample it, we need to edit the policy first, and then put log(pi) into the gumbel-softmax.    # See https://stackoverflow.com/questions/64980330/input-for-torch-nn-functional-gumbel-softmax    print('===============')    temperature = 1    actions_sampled = torch.nn.functional.gumbel_softmax(torch.log(pi), tau=temperature, hard=True)    a0_num = torch.sum(actions_sampled[:, 0])    a1_num = torch.sum(actions_sampled[:, 1])    print(pi[0], a0_num, a1_num, sep=\"\\n\")tensor([0.2689, 0.7311], dtype=torch.float64)tensor(2687766., dtype=torch.float64)tensor(7312234., dtype=torch.float64)===============tensor([0.2689, 0.7311], dtype=torch.float64)tensor(2690092., dtype=torch.float64)tensor(7309908., dtype=torch.float64)  Applying Gumbel-Softmax may cause NaN during training. Changing the data type of the variable to float64 seems to have avoided this issue.Computation graphCheck my note on computation graph.Social Influence  A MARL method.  An intrinsic reward.  Agent $i$ chooses the action that has the most impact on others.\\[\\begin{aligned}    r_t^i     =&amp; \\sum\\limits_{j\\ne i} D_{KL}\\left[\\pi^j(a_t^j \\mid s_t, a_t^i) \\Big\\Vert \\sum\\limits_{a_t^{i\\prime}} \\pi^j(a_t^j \\mid s_t, a_t^{i\\prime})\\cdot \\pi^i(a_t^{i\\prime}\\mid s_t) \\right] \\\\    =&amp; \\sum\\limits_{j\\ne i} D_{KL}\\left[\\pi^j(a_t^j \\mid s_t, a_t^i) \\Big\\Vert P(a_t^j\\mid s_t) \\right]\\end{aligned}\\]In the principal-agent communication:\\[r^i = D_{KL}\\left[ \\pi^j(a^j\\mid\\sigma^i) \\Big\\Vert \\sum\\limits_{\\sigma'}\\varphi^i(\\sigma^{i\\prime}\\mid s)\\cdot \\pi^j(a^j\\mid\\sigma^{i\\prime})\\right]\\]BasicsTD(0)Resampling techniques are a class of statistical methods that involve creating new samples by repeatedly drawing observations from the original data sample.Bootstrapping is a method where new “bootstrap samples” are created by drawing observations with replacement from the original sample.In RL, a common example is the Temporal Difference (TD) learning. This method bootstraps from the current estimate of the value function. The value function is defined as\\[V(s) = \\mathbb{E}\\left[\\sum\\limits_{t=0}^\\infty \\gamma^t \\cdot r_t | s_0 = s\\right]\\]But if the trajectory will never end, then we cannot get all the $r_t$ that we need to calculate the expectation.According to the Bellman equation, the value function can be calculated as\\[V(s) = \\mathbb{E}\\left[r_{t+1} + \\gamma V(s_{t+1}) | s_t = s\\right]\\]Now I get a new sample of $R_{t+1}$, I can use it to update $V(s_t)$, using the incremental mean trick.\\[V(s_t) \\gets V(s_t) + \\alpha\\left(x_{n+1} - V(s_t) \\right),\\]where $x_{n+1} = r_{t+1} + \\gamma V(s_{t+1}).$ The $V(s_{t+1})$ is not the ground true value, but we can used it. (Proving convergence is another thing to do.)So we can say that the value function is updated based on itself. And this method uses $V(s_{t+1})$ instead of $\\sum\\limits_{k=t}^\\infty \\gamma^{k-t}\\cdot r_{k+2}.$ And that’s what bootstrapping means."
  },
  
  {
    "title": "Code Toolbox",
    "url": "/posts/Code-Toolbox/",
    "categories": "Code",
    "tags": "tech, toolbox",
    "date": "2023-04-09 18:40:00 +0000",
    





    
    "snippet": "  This note will be consistently updated.Per-Sample Gradient  $\\mathrm{batch_size} = n$,  $\\boldsymbol x \\to \\mathrm{net}(\\boldsymbol w) \\to \\boldsymbol y \\to \\boldsymbol L \\to L_{scalar}$         ...",
    "content": "  This note will be consistently updated.Per-Sample Gradient  $\\mathrm{batch_size} = n$,  $\\boldsymbol x \\to \\mathrm{net}(\\boldsymbol w) \\to \\boldsymbol y \\to \\boldsymbol L \\to L_{scalar}$          $\\boldsymbol w \\gets \\boldsymbol w + \\frac{\\alpha}{n}\\cdot \\frac{\\partial L_{scalar}}{L_i} \\cdot \\frac{L_i}{\\partial \\boldsymbol w}$        Accomplishing it by for costs lots of time.Hook  PyTorch中，可以自己定一个hook函数，给nn.Module登记          登记完后，nn.Module在forward的时候会触发这个hook函数      也可以选择让其在backward的时候触发hook函数        hook函数的参数是固定的：(module, grad_input, grad_output)          hook函数被触发后，自动搜集当前触发状态下的这3个参数，因此可以用hook实现搜集一些中间量      grad_input是反向传播的量对module的input的梯度        $\\frac{\\partial L}{\\partial w} = \\sum\\limits_i \\frac{\\partial L}{\\partial L_i} \\cdot\\frac{\\partial L_i}{\\partial y_i}\\cdot\\frac{\\partial y_i}{\\partial w}$          $\\frac{\\partial L}{\\partial L_i} \\cdot\\frac{\\partial L_i}{\\partial y_i}=\\mathrm{grad_output}$      Opacus  让PyTorch训练模型时能做差分隐私的一个库  DP-SGD (Differentially-Private Stochastic Gradient Descent)          要让Loss对每个sample的grad都做一个clip，再加个噪声      所以要求per-sample gradient        他们也是用的hook来做的，但是是封装好了，可以直接用vmap  v = vectorization  新函数 = vmap(要做的批量操作的函数，输入的量按哪个维度作分割)  批量操作的结果 = 新函数(批量的原函数的输入)  现在要批量求梯度，那么要给vmap传入个求梯度的函数  vmap不支持autograd，但有函数代替  具体写在了22.9.14的实验进展里Python Profile  Used to find performance bottlenecks.  Can be easily done by clicking the button in the upper right corner, if you are using PyCharm (Professional Edition).  Check this website.Tmux  tmux ls  tmux attach-session -t 0Terminal Python Environment Initializationsource ~/.bash_profileconda activate rlbasicCheck the status of GPU or CPU(… in a terminal)  GPU: nvidia-smi  CPU: topGithubCreate a repo  Click the green button New on the GitHub repo website.  Do not check the Add a README file.  Copy the link with the .git extension.  Create a directory locally and enter it in a terminal.  git init  git remote add origin [xxx.git]  The content inside [] is a variable.  The origin is a default name that refers to the original location (i.e., the remote repository’s URL) from which you cloned the repository. When you use the git clone [URL] command to clone a repository, Git automatically names the remote repository’s URL as origin.Lazy commitCreate a snippet in the software Termius:git add .git commit -m \"[commit_info]\"git push origin [branch_name]Then enter your github name and your git temporary token.Download  Create a new terminal at the folder where you want to download the repo. The downloaded repo will be a subfolder, and its contents are what you see on the webpage.  git clone [repo_URL(xxx.git)] (Download.)  Enter the subfolder.The git clone will create a subfolder (named after the repo) in your current folder.Branch  git branch -a (List all the braches.)  git checkout [branch_name] (Switch to a branch.)  git checkout -b [branch_name] (Create a branch.)Get updatedWay 1  git fetch origin (Retrieve the changes from all branches.)  git merge origin/[remote_branch_name] [local_branch_name]Way 2pull = fetch + mergegit pull origin [remote_branch_name] (Update the code in your current local branch.)RandomIntegerimport torchtorch.randint(1,5,[2])\t# [1,4]的整数，生成2个数，可重复torch.randint(5,[1,2])\t# [0,4]的整数，结果是类似这样的：tensor([[2, 4]])import randomrandom.randint(-1,2) # [-1,2]的整数，生成1个import randomnumbers = random.choices(range(101), k=10) # [0,100]，生成10个，可以重复print(numbers)import randomnumbers = random.sample(range(101), 10) # [0,100]的整数，生成10个，不会重复print(numbers)import randomitems = [i for i in range(1, 6)] # [1,5]的整数列表random.shuffle(items)\t#打乱顺序print(items[:k])\t\t#保留前k个；k自己取RealUniform distributiontorch.rand(2, 3) # size (2,3)，每一个数都是从[0,1)均匀采样的Normal distributiontorch.randn(2, 3) # size (2,3)，每一个数都是从正态分布N(0,1)采样的  Check the PyTorch documentation.Customized Module Templateimport torchimport torch.nn as nnimport osclass net_base(nn.Module):    def __init__(self, n_channels, config, name, device=None):        super().__init__()        self.n_channels = n_channels        # padding for keeping the width and height of input unchanged: kernel=3, padding=1; kernel=5, padding= 2; ...        self.conv_layer = nn.Sequential(            nn.Conv2d(n_channels, config.nn.n_filters, config.nn.kernel, config.nn.stride,                      padding=int((config.nn.kernel - 1) / 2), dtype=torch.double), nn.ReLU(),        )        obs_vector = config.env.map_height * config.env.map_width * config.nn.n_filters        self.mlp = nn.Sequential(            nn.Linear(obs_vector, config.nn.hidden_width, dtype=torch.double), nn.ReLU(),            nn.Linear(config.nn.hidden_width, config.nn.hidden_width, dtype=torch.double), nn.ReLU(),        )        self.name = name        self.checkpoint_file = os.path.join(config.path.saved_models, config.main.exp_name, name)        # print(os.getcwd())        if not os.path.exists(os.path.join(config.path.saved_models, config.main.exp_name)):            os.makedirs(os.path.join(config.path.saved_models, config.main.exp_name), exist_ok=True)        self.device = device        self.to(self.device)    def forward(self, x):        return self.mlp(self.conv_layer(x).view(x.shape[0], -1))    def save_checkpoint(self):        torch.save(self.state_dict(), self.checkpoint_file)        return    def load_checkpoint(self, path=None):        if not path:            self.load_state_dict(torch.load(self.checkpoint_file, map_location=self.device))        else:            load_path = os.path.join(path, self.name)            self.load_state_dict(torch.load(load_path, map_location=self.device))Web CrawlerResources  A nice blog cuiqingcai (in Chinese).ML TricksDropout  Generated by ChatGPT 4Dropout的原理：Dropout是一种正则化技巧，用于防止神经网络过拟合。它在训练期间随机地”丢弃”或“关闭”一部分神经元，即将它们的输出设置为0。这样做可以减少神经元之间的相互依赖，从而鼓励每个神经元独立地学习特征。具体来说，Dropout操作如下：  对于每个训练样本，在前向传播时，每个神经元都有概率p被设置为0。  在反向传播时，被设置为0的神经元不会更新其权重。  在测试或验证时，不使用dropout，但为了平衡因dropout导致的输出变化，我们将神经元的输出乘以(1-p)进行缩放。PyTorch中的Dropout代码示例：以下是使用PyTorch的nn.Dropout模块的简单示例：import torchimport torch.nn as nnclass SimpleNN(nn.Module):    def __init__(self, input_dim, hidden_dim, output_dim):        super(SimpleNN, self).__init__()                # 定义一个简单的三层神经网络，其中包含一个dropout层        self.fc1 = nn.Linear(input_dim, hidden_dim)        self.dropout = nn.Dropout(p=0.5)  # 设置dropout概率为0.5        self.fc2 = nn.Linear(hidden_dim, output_dim)    def forward(self, x):        x = torch.relu(self.fc1(x))        x = self.dropout(x)  # 在隐藏层后应用dropout        x = self.fc2(x)        return x# 创建一个简单的模型实例model = SimpleNN(input_dim=10, hidden_dim=20, output_dim=2)input_tensor = torch.randn(5, 10)  # 创建一个5x10的随机输入张量output = model(input_tensor)print(output)MaskAs indicesimport torchdata = torch.arange(5)  # tensor([0, 1, 2, 3, 4])mask = data &lt;= 2  # tensor([ True,  True,  True, False, False]); any condition is okdata[mask] = 0  # tensor([0, 0, 0, 3, 4])Retain gradientsimport torchdata_shape = 5, 3data = torch.arange(15, dtype=torch.float64).view(data_shape).requires_grad_(True)mask = data &lt;= 6  # any condition is okdata_masked = data * maskloss = data_masked.sum()loss.backward()grad1 = data_masked.gradgrad2 = data.grad'''data_masked: tensor([[0., 1., 2.],        [3., 4., 5.],        [6., 0., 0.],        [0., 0., 0.],        [0., 0., 0.]], dtype=torch.float64, grad_fn=&lt;MulBackward0&gt;)data_masked.grad: Nonedata.grad:tensor([[1., 1., 1.],        [1., 1., 1.],        [1., 0., 0.],        [0., 0., 0.],        [0., 0., 0.]], dtype=torch.float64)'''argparse      A tutorial.    Documentation.  argparse_test.py:import argparseif __name__ == '__main__':    parser = argparse.ArgumentParser()    parser.add_argument(\"positional_arg1\", help=\"A message to the user.\")    parser.add_argument(\"positional_arg2\")    parser.add_argument(\"-o1\", \"--optional_arg1\")    parser.add_argument(\"-o2\", \"--optional_arg2\", action=\"store_true\", help=\"Input -o2 will set it True.\")    parser.add_argument(\"-o3\", \"--optional_arg3\", default=\"wuwu\")    args = parser.parse_args()    print(args, args.positional_arg1, args.positional_arg2, sep=\"\\n\")Run (help):  python3 argparse_test.py -h  python3 argparse_test.py --helpResult:usage: argparse_test.py [-h] [-o1 OPTIONAL_ARG1] [-o2] [-o3 OPTIONAL_ARG3] positional_arg1 positional_arg2positional arguments:  positional_arg1       A message to the user.  positional_arg2optional arguments:  -h, --help            show this help message and exit  -o1 OPTIONAL_ARG1, --optional_arg1 OPTIONAL_ARG1  -o2, --optional_arg2  Input -o2 will set it True.  -o3 OPTIONAL_ARG3, --optional_arg3 OPTIONAL_ARG3Run:python3 argparse_test.py xixi hahaResult:Namespace(optional_arg1=None, optional_arg2=False, optional_arg3='wuwu', positional_arg1='xixi', positional_arg2='haha')xixihahaRun:python3 argparse_test.py xixi haha -o2 -o1 heiheiResult:Namespace(optional_arg1='heihei', optional_arg2=True, optional_arg3='wuwu', positional_arg1='xixi', positional_arg2='haha')xixihahaChange Dir to the Project Dir  Make sure that there is a README.md file under the project directory. This file is used as an identifier.import osdef find_project_directory(identifier_file):    current_path = \".\"    while True:        if identifier_file in os.listdir(current_path):            return os.path.abspath(current_path)        parent_path = os.path.join(current_path, \"..\")        if os.path.abspath(parent_path) == os.path.abspath(current_path):            return None        current_path = parent_pathdef cd_project_directory():    project_directory = find_project_directory(\"README.md\")    if project_directory:        os.chdir(project_directory)    else:        print(\"Project directory not found.\")    return project_directoryif __name__ == '__main__':    project_directory = cd_project_directory()    print('All done.')printSeparator lineimport shutilterminal_columns = shutil.get_terminal_size().columnsprint('=' * terminal_columns)Set Seeddef all_seed(env, seed=1):    env.seed(seed)  # env config    np.random.seed(seed)    random.seed(seed)    torch.manual_seed(seed)  # config for CPU    torch.cuda.manual_seed(seed)  # config for GPU    os.environ['PYTHONHASHSEED'] = str(seed)  # config for python scripts    # config for cudnn    torch.backends.cudnn.deterministic = True    torch.backends.cudnn.benchmark = False    torch.backends.cudnn.enabled = False"
  },
  
  {
    "title": "Paper Toolbox",
    "url": "/posts/Paper-Toolbox/",
    "categories": "Misc Notes",
    "tags": "tech, toolbox",
    "date": "2023-04-08 18:40:00 +0000",
    





    
    "snippet": "  This note will be consistently updated.Frequently Referenced PapersClassic RL milestones  Atari  Go  Poker  video games  bioinformatics  economicsMARLExpressionsCool  The canonical formulation co...",
    "content": "  This note will be consistently updated.Frequently Referenced PapersClassic RL milestones  Atari  Go  Poker  video games  bioinformatics  economicsMARLExpressionsCool  The canonical formulation considers …  Domain-specific  The ego agent (for which we design the algorithm)Avoid the following usages  can’t (better: cannot)  can not (correct: cannot)  what’s / it’s (better: what is / it is)  what’s more (better: additionally)  traditional (better: classic, vanilla)  he/she  Firstly, … (better: First, …)"
  },
  
  {
    "title": "Math Toolbox",
    "url": "/posts/Math-Toolbox/",
    "categories": "Mathematics",
    "tags": "tech, math, toolbox",
    "date": "2023-04-07 18:40:00 +0000",
    





    
    "snippet": "  This note will be consistently updated.OptimizationBasicsThe standard form for an optimization problem (the primal problem) is the following:\\[\\begin{aligned}&amp;\\min\\limits_{x} \\quad f_0(x)  \\\\...",
    "content": "  This note will be consistently updated.OptimizationBasicsThe standard form for an optimization problem (the primal problem) is the following:\\[\\begin{aligned}&amp;\\min\\limits_{x} \\quad f_0(x)  \\\\&amp;\\begin{array}{cc}\\mathrm{s.t.} \t&amp;f_i(x) \\le 0,\t&amp; i=1,2,\\ldots,m\\\\\t\t\t\t&amp;h_i(x) = 0,\t\t&amp; i=1,2,\\ldots,p\\\\\\end{array}\\end{aligned}\\]      optimization variable: $x\\in \\mathbb{R}^n$        $\\mathrm{dom}(x)=D=\\bigcap\\limits_{i=1}^m \\mathrm{dom}(f_i) \\cap \\bigcap\\limits_{i=1}^p\\mathrm{dom}(h_i)$        objective function (cost function): $f_0(x):\\mathbb{R}^n\\to \\mathbb{R}$        $x^*=\\arg\\min\\limits_{x\\in D} f_0(x),$ subjects to the constraints.        $p^*=f_0(x^*)$  ConvexConvex set:Epigraphs: ${(x,y):y\\ge f(x)}$Convex function: a function is convex if and only if its epigraph is convex, and the epigraph of a pointwise supremum is the intersection of the epigraphs. Hence, the pointwise supremum of convex func tions is convex.DualityThe Lagrangian function $L:\\mathbb{R}^n \\times \\mathbb{R}^m \\times \\mathbb{R}^p \\to \\mathbb{R}$\\(L(x,\\lambda,v)=f_0(x) + \\sum\\limits_{i=0}^m \\lambda_i f_i(x) + \\sum\\limits_{i=0}^p v_i h_i(x)\\)The Lagrange dual function $g:\\times \\mathbb{R}^m \\times \\mathbb{R}^p \\to \\mathbb{R}$\\(g(\\lambda,v)=\\inf\\limits_{x\\in D} L(x,\\lambda,v)\\)The dual function is concave even when the optimization problem is not convex, since the dual function is the pointwise infimum of a family of affine functions of $(\\lambda,v)$.Infimum = Greatest Lower Bound: the infimum of a subset $S$ partially ordered set $P$ is a greatest element in $P$ that is less than or equal to all elements of $S$, if such an element exists.The Lagrange dual problem, then, is to maximize this dual function:\\[\\begin{align*}\\max\\limits_{\\lambda,v} \\quad &amp; g(\\lambda,v) \\\\\\mathrm{s.t.} \\quad &amp; \\lambda_i \\ge 0, \\, i = 1, \\ldots, m \\\\\\end{align*}\\]Formulating a dual problem gives a lower bound for the primal problem.\\[f_0(x) \\ge L(x, \\lambda, \\nu) \\ge g(\\lambda, \\nu)\\]Solving the dual problem can provide useful insights into the primal problem, and in some circumstances, it is easier to find the solution to the primal problem through solving its dual.In certain cases (under certain conditions such as the Slater condition), the solutions to the original and dual problems will match, showcasing “strong duality”.Slater’s ConditionSlater’s condition is a criterion in convex optimization used to ensure strong duality. It imposes requirements on the existence of interior points concerning the constraints of the primal problem. Specifically, it requires there to be a point $x \\in \\text{relint}(D)$ (where $\\text{relint}(D)$ is the relative interior of the domain $D$) such that:\\[\\begin{align*}f_i(x) &amp; &lt; 0, &amp; i = 1, \\ldots, m \\\\h_i(x) &amp; = 0, &amp; i = 1, \\ldots, p \\\\\\end{align*}\\]Meeting Slater’s condition ensures that there is zero duality gap between the primal and dual problems, affirming strong duality.Slater’s condition ensures that there is a point that lies strictly within all inequality constraints and satisfies all equality constraints. This condition is employed to assure strong duality, i.e., the optimal value of the primal problem equals the optimal value of the dual problem.      Existence: By assuring there is a feasible point that meets all constraint conditions, Slater’s condition guarantees that both the primal and dual problems are solvable.        Gap-Free: Slater’s condition ensures a zero duality gap, i.e., there is no “gap” between the optimal solution of the primal problem and that of the dual problem, thus assuring strong duality.  KKT ConditionsThe KKT (Karush-Kuhn-Tucker) conditions are a set of equations and inequalities necessary for finding the optimal solutions to a nonlinear constrained optimization problem. The KKT conditions comprise the following equations and inequalities:      Stationarity condition:\\(\\nabla f_0(x) + \\sum_{i=1}^m \\lambda_i \\nabla f_i(x) + \\sum_{i=1}^p v_i \\nabla h_i(x) = 0\\)        Primal feasibility:\\(\\begin{align*}f_i(x) &amp; \\leq 0, &amp; i = 1, \\ldots, m \\\\h_i(x) &amp; = 0, &amp; i = 1, \\ldots, p \\\\\\end{align*}\\)        Dual feasibility:\\(\\lambda_i \\geq 0, \\, i = 1, \\ldots, m\\)        Complementary slackness:\\(\\lambda_i f_i(x) = 0, \\, i = 1, \\ldots, m\\)  If a set of solutions $x^$, $\\lambda^$, and $v^$ satisfy these conditions, then $x^$ is a local optimal solution to the primal problem.      Primal Feasibility and Dual Feasibility: These two conditions ensure that the solution we find satisfies all constraint conditions of the primal and dual problems, respectively.        Complementary Slackness: This condition implies that for each inequality constraint, either it is tight (i.e., the equality holds) or its corresponding Lagrange multiplier is zero. This ensures that at the optimal solution, the solutions to the primal and dual problems are “aligned,” thereby affirming strong duality.        Stationarity Condition: This condition, by setting the gradient of the Lagrangian to zero, provides us with a system to solve for the potential optimal solutions.  Notation &amp; OperatorsMisc  $[[n]] = \\set{1, \\ldots, n}$.  $A := B$ means $A$ is defined as $B$.  $(f\\circ g)(x) = f(g(x))$. Function composition. \\circ.  $A^\\intercal$. vector/matrix transpose. \\intercal.  Norm $\\Vert x \\Vert$. \\Vert x \\Vert.Greek alphabet  Check here.  $\\alpha$ (\\alpha). /ˈælfə/.  $\\beta$ (\\beta). UK: /ˈbiːtə/, US: /ˈbeɪtə/.  $\\gamma$ (\\gamma), $\\Gamma$ (\\Gamma), $\\varGamma$ (\\varGamma). /ˈɡæmə/.  $\\delta$ (\\delta), $\\Delta$ (\\Delta), $\\varDelta$ (\\varDelta). /ˈdɛltə/.  $\\epsilon$ (\\epsilon), $\\varepsilon$ (\\varepsilon). /ˈɛpsɪlɒn, ɛpˈsaɪlən/.  $\\zeta$ (\\zeta). UK: /ˈziːtə/,[1] US: /ˈzeɪtə/.  $\\eta$ (\\eta). /ˈiːtə, ˈeɪtə/.  $\\theta$ (\\theta), $\\Theta$ (\\Theta), $\\vartheta$ (\\vartheta), $\\varTheta$ (\\varTheta). UK: /ˈθiːtə/, US: /ˈθeɪtə/.  $\\iota$ (\\iota). /aɪˈoʊtə/.  $\\kappa$ (\\kappa), $\\varkappa$ (\\varkappa). /ˈkæpə/.  $\\lambda$ (\\lambda), $\\Lambda$ (\\Lambda), $\\varLambda$ (\\varLambda). /ˈlæmdə/.  $\\mu$ (\\mu). /ˈm(j)uː/.  $\\nu$ (\\nu). /ˈnjuː/.  $\\xi$ (\\xi), $\\Xi$ (\\Xi), $\\varXi$ (\\varXi). /zaɪ, ksaɪ/.  $o$ (o), $O$ (O). Omicron, /ˈoʊmɪkrɒn, ˈɒmɪkrɒn, oʊˈmaɪkrɒn/.  $\\pi$ (\\pi), $\\Pi$ (\\Pi), $\\varpi$ (\\varpi), $\\varPi$ (\\varPi). /ˈpaɪ/.  $\\rho$ (\\rho), $\\varrho$ (\\varrho). /ˈroʊ/.  $\\sigma$ (\\sigma), $\\Sigma$ (\\Sigma), $\\varsigma$ (\\varsigma), $\\varSigma$ (\\varSigma). /ˈsɪɡmə/.  $\\tau$ (\\tau). /ˈtɔː, ˈtaʊ/.  $\\upsilon$ (\\upsilon), $\\Upsilon$ (\\Upsilon), $\\varUpsilon$ (\\varUpsilon). /ˈʌpsɪˌlɒn, ˈ(j)uːp-, -lən/.  $\\phi$ (\\phi), $\\Phi$ (\\Phi),  $\\varphi$ (\\varphi), $\\varPhi$ (\\varPhi). /faɪ/.  $\\chi$ (\\chi). /ˈkaɪ, ˈxiː/.  $\\psi$ (\\psi), $\\Psi$ (\\Psi), $\\varPsi$ (\\varPsi). /ˈ(p)saɪ, ˈ(p)siː/.  $\\omega$ (\\omega), $\\Omega$ (\\Omega), $\\varOmega$ (\\varOmega). /oʊˈmiːɡə, oʊˈmɛɡə, oʊˈmeɪɡə, əˈmiːɡə/.var indicates that this quantity is a variable.Fonts  Adapted from this site.  \\mathnormal{} is the normal math italic font. It is the default font.  \\mathbb{}.          $\\mathbb{E}[X]$, the expectation of $X$.      $\\mathbb{R}$, the set of real numbers.        \\mathcal{} is the special calligraphic font for uppercase letters only.          The standard normal distribution $\\mathcal{N}(0,1)$.        \\mathbf{} gives upright Roman boldface letters.          $\\mathbf{v}$, a vector.      $\\mathbf{A}$, a matrix.        \\mathrm{} is the normal upright Roman font.          $\\mathrm{e}$, the constant “e”.      $\\mathrm{sin}$, the sine function.      $\\mathrm{softmax}$, an operators.      $\\mathrm{m}$, the unit “meter”.      $\\mathrm{cov}(X, Y)$, the covariance of $X$ and $Y$.        \\mathtt{} gives upright letters from the typewriter type font.  It is typically used to represent computer code, variable names, function names, and other elements that need to be displayed in a fixed-width font.          \\mathtt{print(\"Hello, World!\")}      $\\mathtt{print(“Hello, World!”)}$        \\mathit{} gives text italic letters.  \\mathsf{} gives upright sans serif letters.Multiple variables  Addition and subtraction of matrices are element-wise.\\[\\begin{bmatrix}1 &amp; 2 \\\\3 &amp; 4 \\end{bmatrix}+\\begin{bmatrix}5 &amp; 6 \\\\7 &amp; 8 \\end{bmatrix}=\\begin{bmatrix}6 &amp; 8 \\\\10 &amp; 12 \\end{bmatrix}\\]  Multiplication of a matrix with a scalar:\\[2 \\cdot\\begin{bmatrix}1 &amp; 2 \\\\3 &amp; 4 \\end{bmatrix}=\\begin{bmatrix}2 &amp; 4 \\\\6 &amp; 8 \\end{bmatrix}\\]  Multiplication of matrices.          $AB$ or $A\\cdot B$.      $A$ is of $m\\times n$ shape, and $B$ is of $n\\times k$.      \\[\\begin{bmatrix}1 &amp; 2 \\\\3 &amp; 4 \\end{bmatrix}\\cdot\\begin{bmatrix}5 &amp; 6 \\\\7 &amp; 8 \\end{bmatrix}=\\begin{bmatrix}1\\times 5+2\\times 7 &amp; 1\\times 6+2\\times 8 \\\\3\\times 5+4\\times 7 &amp; 3\\times 9+7\\times 8\\end{bmatrix}\\]  Hadamard (element-wise) product: $\\odot$ (\\odot) or $\\circ$ (\\circ).\\[\\begin{bmatrix}1 &amp; 2 \\\\3 &amp; 4 \\end{bmatrix}\\odot\\begin{bmatrix}5 &amp; 6 \\\\7 &amp; 8 \\end{bmatrix}=\\begin{bmatrix}5 &amp; 12 \\\\21 &amp; 28 \\end{bmatrix}\\]  Dot product of vectors. Or inner product, scalar product.          $\\mathbf{u} \\cdot \\mathbf{v} = \\sum\\limits_{i}^n u_i\\cdot v_i$        Outer product of vectors.          $\\mathbf{u} = [u_1,\\ldots, u_m]^\\intercal$      $\\mathbf{v} = [v_1,\\ldots, v_n]^\\intercal$      $\\mathbf{A} = \\mathbf{u} \\otimes \\mathbf{v} = \\mathbf{u} \\mathbf{v}^\\intercal$.      $\\mathbf{A}$ is $m\\times n$.      \\[\\mathbf{u} \\otimes \\mathbf{v}=\\begin{bmatrix}   u_1\\cdot v_1 &amp; u_1\\cdot v_2 &amp; \\ldots &amp; u_1\\cdot v_n \\\\   u_2\\cdot v_1 &amp; u_2\\cdot v_2 &amp; \\ldots &amp; u_2\\cdot v_n \\\\   \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\   u_m\\cdot v_1 &amp; u_m\\cdot v_2 &amp; \\ldots &amp; u_m\\cdot v_n \\end{bmatrix}\\]  Outer product of tensors.Given two tensors $\\mathbf{u}$ and $\\mathbf{v}$ with dimensions $(k_1, k_2, \\ldots, k_m)$ and $l_1,l_2, \\ldots, l_n$, their outer product is a tensor with dimensions $(k_1, k_2, \\ldots, k_m, l_1,l_2, \\ldots, l_n)$. and entries\\[(\\mathbf{u} \\otimes \\mathbf{v})_{i_1,\\ldots,i_m, j_1,\\ldots, j_m} = u_{i_1,\\ldots,i_m}\\cdot v_{j_1, \\ldots, j_n}\\]InequalityLog-sum inequality\\[\\sum\\limits_{i=1}^n a_i \\cdot \\log \\frac{a_i}{b_i} \\ge \\left( \\sum\\limits_{i=1}^n a_i \\right) \\cdot\\log \\frac{\\sum\\limits_{i=1}^n a_i}{\\sum\\limits_{i=1}^n b_i}\\]Jensen’s inequality\\[\\varphi\\left(\\mathbb{E}[X]\\right) \\le \\mathbb{E}[\\varphi(X)],\\]where $X$ is a random variable and $\\varphi$ is a convex function.Jensen gap: $\\mathbb{E}[\\varphi(X)] - \\varphi\\left(\\mathbb{E}[X]\\right)$.\\[\\varphi\\left(\\frac{\\sum a_i\\cdot x_i}{\\sum a_i}\\right) \\le \\frac{\\sum a_i \\cdot \\varphi(x_i)}{\\sum a_i}\\]Equality holds iif. $x_1 = \\ldots =x_n$ or $\\varphi$ is linear.ProbabilityExpectation\\[\\mathbb{E}[X] = \\sum\\limits_{i}p_i\\cdot x_i\\]\\[\\mathbb{E}[X] = \\int_{-\\infty}^{\\infty} x\\cdot f(x) \\, dx\\]Linearity:\\[\\mathbb{E}[X+Y] = \\mathbb{E}[X]+\\mathbb{E}[Y],\\]\\[\\mathbb{E}[aX] = a\\mathbb{E}[X].\\]If $X$ and $Y$ are independent:\\[\\mathbb{E}[XY] = \\mathbb{E}[X]\\cdot \\mathbb{E}[Y].\\]If $X$ and $Y$ are dependent:\\[\\mathbb{E}[XY] \\ne \\mathbb{E}[X]\\cdot \\mathbb{E}[Y].\\]If $X = c$, where $c\\in \\mathbb{R}$, then $\\mathbb{E}[X] = c$. Thus\\[\\mathbb{E}[\\mathbb{E}[X]] = \\mathbb{E}[X].\\]Variance\\[\\begin{aligned}   \\mathbb{V}\\left[X \\right] =&amp; \\mathbb{E}\\left[ (X - \\mathbb{E}[X])^2 \\right] \\\\   =&amp; \\mathbb{E}\\left[ X^2 - 2\\,X\\cdot \\mathbb{E}[X] +\\mathbb{E}[X]^2 \\right]\\\\   =&amp; \\mathbb{E}\\left[ X^2 - 2\\,\\mathbb{E}[X]\\cdot \\mathbb{E}[X] +\\mathbb{E}[X]^2 \\right]\\\\   =&amp; \\mathbb{E}\\left[X^2\\right] - \\mathbb{E}\\left[X\\right]^2\\end{aligned}\\]\\[\\mathbb{V}\\left[X \\right] = \\sum\\limits_{i} p_i\\cdot (x_i - \\mathbb{E}[X])^2\\]\\[\\mathbb{V}\\left[X \\right] = \\int_{\\mathbb{R}} f(x) \\cdot (x - \\mathbb{E}[X])^2 \\, dx\\]\\[\\mathbb{V}\\left[X \\right] = \\int_{\\mathbb{R}} x^2\\cdot f(x)\\, dx - \\mathbb{E}\\left[X\\right]^2\\]\\[\\mathbb{V}\\left[X \\right] = \\mathrm{Cov}(X,X)\\]\\[\\mathbb{V}[X] \\ge 0\\]\\[\\mathbb{V}[X+a] = \\mathbb{V}[X]\\]\\[\\mathbb{V}[aX] = a^2\\mathbb{V}[X]\\]\\[\\mathbb{V}[aX+bY] = a^2\\, \\mathbb{V}[X] + b^2\\, \\mathbb{V}[Y] + 2ab\\, \\mathrm{Cov}[X]\\]If there is a set of random variables $\\set{X_1,\\ldots X_N}$,then\\[\\begin{aligned}   \\mathbb{V}\\left[\\sum\\limits_i X_i\\right] =&amp; \\sum\\limits_{i,j} \\mathrm{Cov}(X_i, X_j) \\\\   =&amp; \\sum\\limits_{i}\\mathbb{V} [X_i] +\\sum\\limits_{i\\ne j}\\mathrm{Cov} (X_i, X_j).\\end{aligned}\\]Covariance\\[\\mathrm{Cov}(X,Y) = \\mathbb{E}\\left[ X - \\mathbb{E}[X] \\right] \\cdot \\mathbb{E}\\left[ Y - \\mathbb{E}[Y] \\right]\\]MomentInformation TheorySelf-information\\(I(x) = -\\log p(x)\\)信息源发消息，信息是消息中的语义，是抽象的，信号是消息的物理表示；信息源发什么消息是不确定的（确定的话就没有通信的必要了），所以消息有一个样本空间，和一个对应的概率分布；信息描述事件的不确定性。一个事件发生，会带来信息；事件发生的概率越小，则其出现后所带来的信息越大。事件的信息=事件的发生概率的某个函数，$I(a_i) = f[P(a_i)]$，这个叫自信息，自信息度量了随机事件信息量的大小，the amount of information；根据信息的特性找出这个函数：  事件$x_i$发生的概率越小，其发生后带来的信息越大  事件$x_i$发生概率为1，则其发生不带来信息  事件$x_i$发生概率为0，则其发生带来无穷大的信息  信息是关于事件发生概率的递减函数  两个事件都发生的概率为两个事件的发生概率的乘积，两个事件都发生的信息为这两个事件发生的信息的和然后一些证明，自信息的函数为$-\\log$，即 $I(x_i) = -\\log P(x_i)$我看到有说这个也叫log-perplexity，困惑程度。  概率越大，越有可能发生，越符合习惯，困惑程度低，比如”Mary had a little lamb.”  概率越小，越不可能发生，越奇怪，困惑程度高，比如”Correct horse battery stapler.”Entropy\\(\\begin{aligned}   H(X) =&amp; \\mathbb{E}\\_{X}\\left[I(x)\\right]\\\\   =&amp; -\\sum\\limits_{x} p(x) \\cdot \\log p(x)\\end{aligned}\\)  信源的样本空间是$X$，也就是能发送的信号（事件的集合）  信源的概率空间是$\\set{X, P(X)}$  事件$x_i$以$P(x_i)$的概率发生，发生后带来$I(x_i)$的信息  熵 = 信源能带来的平均自信息  熵一定是正的（概率大于0小于1看看就知道）  熵是凸函数，用Jensen不等式证明  均匀分布的时候，熵最大一个事件的自信息表示了该事件发生的不确定性，该事件发生的概率越小，则其发生的不确定性越大，则其发生带来的信息越大，则其自信息越大。信息熵是自信息的期望，表示一个消息的概率分布确定时这个信源能带来的信息的多少，也是平均每个信源符号（发送一次信息，出现一次发信息事件）所携带的信息量。The uniform distribution has the max entropy要证明在给定条件下均匀分布具有最大熵，我们可以使用拉格朗日乘数法来找到概率分布的最优解。假设我们有一个离散的概率分布$p(x)$，其中$x$可以取$n$个不同的值，优化问题是\\[\\begin{aligned}\\max\\limits_{p}\\quad &amp;H(p) = - \\sum_{i=1}^{n} p(x_i) \\log p(x_i) \\\\\\textrm{s.t.}\\quad &amp;\\sum_{i=1}^{n} p(x_i) = 1, \\\\\\quad &amp;p(x_i) &gt; 0, \\forall i.\\end{aligned}\\]首先构造拉格朗日函数如下：\\[L(p, \\lambda) = -\\sum_{i=1}^{n} p(x_i) \\log p(x_i) - \\lambda \\left( \\sum_{i=1}^{n} p(x_i) - 1 \\right),\\]其中 $\\lambda$ 是拉格朗日乘数。接下来，我们将对$L$分别对$p(x_i)$和$\\lambda$求偏导，并将其设为0以找到驻点。得到：\\[\\frac{\\partial L}{\\partial p(x_i)} = -\\log p(x_i) -1 - \\lambda = 0, \\quad \\forall i,\\]\\[\\frac{\\partial L}{\\partial \\lambda} = - \\sum_{i=1}^{n} p(x_i) + 1 = 0.\\]从第一个偏导数方程中我们可以解出：\\[-\\log p(x_i) -1 - \\lambda = 0 \\implies \\log p(x_i) = -\\lambda - 1 \\implies p(x_i) = e^{-1-\\lambda}.\\]接着，我们可以将这个解代回约束条件\\[\\sum_{i=1}^{n} p(x_i) = 1 \\implies \\sum_{i=1}^{n} e^{-1-\\lambda} = 1 \\implies n e^{-1-\\lambda} = 1 \\implies e^{-1-\\lambda} = \\frac{1}{n}.\\]现在我们找到了$\\lambda$的值：\\[-\\lambda -1 = \\log \\frac{1}{n} \\implies \\lambda = -\\log \\frac{1}{n} + 1.\\]然后我们可以找到$p(x_i)$的解：\\[p(x_i) = e^{-1-\\lambda} \\implies p(x_i) = e^{-1 - (-\\log \\frac{1}{n} + 1)} = \\frac{1}{n}.\\]我们验证这确实是一个最大点，通过证明Hessian矩阵是负定的。最终我们得到最优解是均匀分布：\\[p(x_i) = \\frac{1}{n}, \\quad \\forall i.\\]The more random the signal is, the less informative it will be熵和KL散度是信息论中的两个核心概念。熵是用来衡量一个随机变量的不确定性的量，而KL散度用来衡量两个概率分布之间的差异。我们可以使用KL散度来证明一个信号的分布越随机，则其熵越高。以下是证明步骤：      为了证明一个分布越随机其熵越高，我们可以假设有一个分布 $P$ 与一个完全均匀分布 $U$，其中 $U$ 的每个状态的概率都是 $\\frac{1}{n}$。        然后我们计算 $P$ 与 $U$ 之间的 KL 散度：\\(D_{\\text{KL}}(P||U) = \\sum_{i=1}^{n} p(x_i) \\log \\frac{p(x_i)}{\\frac{1}{n}} = \\sum_{i=1}^{n} p(x_i) \\log (n p(x_i)) - \\log n \\sum_{i=1}^{n} p(x_i)\\)        我们可以发现：\\(D_{\\text{KL}}(P||U) = H(U) - H(P) + \\log n\\)        由于KL散度总是非负的，我们有：\\(H(U) - H(P) + \\log n \\geq 0 \\quad \\Rightarrow \\quad H(P) \\leq H(U) + \\log n\\)        由于均匀分布的熵是最大的，所以我们可以得出结论：一个分布越随机，其熵就越高。  Joint entropy\\(H(X, Y) = \\sum\\limits_{i,j} P(x_i,y_j)\\cdot [- \\log P(x_i,y_j)]\\)把空间改成二维的，$x_i \\in X$ 变成 $(x_i,y_j)\\in X\\times Y$Conditional entropy (channel equivocation)假设我们原本对$X$的认识是先验概率$P$，算的这个熵也叫先验熵\\[H(X) = \\sum\\limits_{i} P(x_i)\\cdot [- \\log P(x_i)]\\]接收到一个信号$y_j$，我们就有了后验概率，对$X$的认识就改变了，可以算一个后验熵\\[H(X\\mid y_j) =  \\sum\\limits_{i} P(x_i\\mid y_j)\\cdot [- \\log P(x_i\\mid y_j)]\\]可以对随机变量$Y$求期望，把后验熵变成条件熵，也叫信道疑义度或者损失熵\\[\\begin{aligned}   H(X\\mid Y)   =&amp; \\sum\\limits_{j} P(y_j) \\sum\\limits_{i} P(x_i\\mid y_j)\\cdot [- \\log P(x_i\\mid y_j)] \\\\   =&amp; \\sum\\limits_{i,j} P(x_i,y_j)\\cdot [- \\log P(x_i\\mid y_j)] \\end{aligned}\\]  如果发的信号$y_j$能唯一确定一个$x_i$，那么$P(x_i\\mid y_j)=1$，$H(X\\mid Y)=0$  如果发的信号$y_j$和$x_i$无关/独立，则￼$P(x_i\\mid y_j) = P(x_i)$，则：\\[H(X\\mid Y)= \\sum\\limits_{i,j} P(x_i)\\cdot P(y_j)\\cdot [- \\log P(x_i)] = H(X)\\]结果和先验熵一样，我们对$X$的认识没有在收到$Y$后而改变Conditioning never increases entropy条件增益性，等号只有$X$和$Y$独立时取到\\[H(X\\mid Y) \\le H(X)\\]Chain rule for entropy\\[H(X,Y) = H(X) + H(Y\\mid X)\\]\\[\\begin{aligned}&amp; H(X) + H(Y\\mid X) \\\\=&amp; \\sum\\limits_{i} P(x_i)\\cdot [- \\log P(x_i)]+ \\sum\\limits_{i,j} P(x_i,y_j)\\cdot [- \\log P(y_i\\mid x_j)]  \\\\=&amp; \\sum\\limits_{i,j} P(x_i,y_j) \\cdot [- \\log P(x_i)]+ \\sum\\limits_{i,j} P(x_i,y_j)\\cdot [- \\log P(y_i\\mid x_j)]  \\\\=&amp; \\sum\\limits_{i,j} P(x_i,y_j)\\cdot [- \\log P(x_i,y_j)] \\\\=&amp; H(X,Y)\\end{aligned}\\]Mutual information平均互信息：接收到信号$Y$消除掉的不确定性，也是获得的信息，也是$X$和$Y$的相关性\\[\\begin{aligned}I(X;Y) &amp;= H(X) - H(X\\mid Y)\\\\&amp;=\\sum\\limits_{i} P(x_i)\\cdot [- \\log P(x_i)] - \\sum\\limits_{i,j} P(x_i,y_j)\\cdot [- \\log P(x_i\\mid y_j)] \\\\&amp;= \\sum\\limits_{i,j} P(x_i,y_j)\\cdot [\\log P(x_i\\mid y_j) - \\log P(x_i)] \\\\&amp;= \\sum\\limits_{i,j} P(x_i,y_j)\\cdot \\log \\frac{P(x_i,y_j)}{P(x_i)\\cdot P(y_j)} \\\\&amp;= D_{KL} (P_{(X,Y)} \\mid \\mid  P_X \\otimes P_Y)\\end{aligned}\\]  可以看到是对称的  可以写成KL散度  互信息就是先验概率的熵减去后验概率的熵，所以先验和后验差距越大，越相关KL divergence\\(\\begin{aligned}   D_{KL} \\left[p(X) \\Vert q(X)\\right]   =&amp; \\sum\\limits_{x\\in X} - p(x)\\cdot \\log q(x)   -p(x)\\cdot \\log p(x) \\\\   =&amp; \\sum\\limits_{x\\in X} p(x)\\cdot \\log \\frac{p(x)}{q(x)}\\end{aligned}\\)#            发送的信号是哪个/事件：$X$      $a_1$      $a_2$                  信源1发送信号的概率/事件发生的概率1：$P_1(X)$      0.99      0.01              信源2发送信号的概率/事件发生的概率2：$P_2(X)$      0.2      0.8      不同的信源发不同的消息带来的自信息不同，因为他们发信息的概率不一样。（相同事件在不同概率分布下发生，带来的自信息不同，因为事件在不同概率分布下的概率不一样）KL散度就是描述这种差异，衡量两个信源发信号带来信息差的期望，也是两个概率分布的差异程度假如知道这个表，那么信源1发$a_1$信号/$a_1$事件发生，带来的信息/消除的不确定性/自信息是$-\\log P_1(a_1)$，而信源2发$a_1$信息，自信息是$-\\log P_2(a_1)$            KL散度：$D_{KL}(P             Q) = \\mathbb{E}{x\\sim P}\\left[-\\log Q(x) - \\left(-\\log P(x)\\right) \\right] = \\mathbb{E}{x\\sim P} \\left[\\log\\frac{P(x)}{Q(x)}\\right]$                  上述例子中：$D_{KL}(P_1             P_2) = P_1(a_1)\\cdot \\log \\frac{P(a_1)}{Q(a_1)} + P_1(a_2)\\cdot \\log \\frac{P(a_2)}{Q(a_2)}$                  在ML中，一般$D_{KL}(P             Q)$是以$P$分布为真实分布/目标分布，衡量$Q$分布离$P$分布差多少。事件发生的概率是按$P$发生的，所以以$P$分布的概率作为求期望的分布。在真实分布/目标分布$P$下，事件按这样的分布$P$发生，发生后会带来xxx的自信息，而$Q$则带来了xxx的自信息，所以是相减然后对$P$求分布                  KL散度虽然是代表差异，但不是真正意义上的距离，因为它不是对称的：$D_{KL}(P             Q)\\ne D_{KL}(Q             P)$      在ML中优化时，是以最小化KL散度作为优化目标，让模型输出的$Q$分布接近目标分布$P$。那么其实可以发现，式子中的一部分是不用优化的，所以引出了交叉熵：$H(P,Q)=-\\mathbb{E}_{x\\sim P}\\log Q(x)$。“最小化KL散度”等同于“最小化交叉熵”AlgebraLearning resources  Linear Algebra Done Right (Book).  Videos by Gilbert Strang (reposted on bilibili).  The Matrix Cookbook (Book).Inverse  A square matrix $A$ is invertible  = $A$ has n pivots  = $A$ is not singular  = the columns/rows of $A$ are independent  = the columns/rows are linearly independent  = elimination can be completed: $PA=LDU$, with all n pivots  = the nullspace of these vectors are ${\\mathbf 0}$  = the determinant of $A$ is not 0  = the rank of $A$ is n  = 0 is not an eigenvalue of $A$  = $A^TA$ is positive definiteRankrank of a matrix = the number of independent columns/rows of this matrix = the number of pivots of this matrix方程组求解 -&gt; 写成矩阵形式 -&gt; 知道什么是线性组合 -&gt; 知道什么是生成子空间 -&gt; 矩阵的秩是生成子空间的维度方程组求解 -&gt; 写成矩阵形式：\\(a_{11}x_1+a_{12}x_2+a_{13}x_3=b_1\\\\ a_{21}x_1+a_{22}x_2+a_{23}x_3=b_2\\\\ a_{31}x_1+a_{32}x_2+a_{33}x_3=b_3\\)可以写成$\\mathbf{A}\\mathbf{x}=\\mathbf{b}$，矩阵和向量的乘法就是这么定义的，方便表达知道什么是线性组合 -&gt; 知道什么是生成子空间：\\[\\begin{bmatrix} a_{11} \\\\ a_{21} \\\\ a_{31}  \\end{bmatrix} x_1 + \\begin{bmatrix} a_{12} \\\\ a_{22} \\\\ a_{32}  \\end{bmatrix} x_2 + \\begin{bmatrix} a_{13} \\\\ a_{23} \\\\ a_{33}  \\end{bmatrix} x_3  = \\begin{bmatrix} b_{1} \\\\ b_{2} \\\\ b_{3}  \\end{bmatrix}\\]矩阵拆成列向量，一个列向量代表一个方向，每个列向量的乘积代表沿着这个列向量的方向走多远。解方程组的问题变成：在空间中给出几个方向，也给出了终点和起点（相对位置），要分别沿着这些方向走多远，可以从起点到达终点？看几个情况，欠定方程组的系数矩阵为矮矩阵，即系数矩阵$\\mathbf{A}\\in \\mathbb{R}^{m\\times n}$的行数$m$小于列数$n$，方程组有无穷多解，比如：\\[\\begin{bmatrix} a_{11} \\\\ a_{21} \\end{bmatrix} x_1 + \\begin{bmatrix} a_{12} \\\\ a_{22} \\end{bmatrix} x_2 + \\begin{bmatrix} a_{13} \\\\ a_{23}  \\end{bmatrix} x_3  = \\begin{bmatrix} b_{1} \\\\ b_{2}  \\end{bmatrix}\\]如果这三个列向量互相不成比例，那么他们是线性无关的，由于每个列向量是两个元素，可以在一个平面中画出来。我们知道在平面中任意两个不共线的向量的线性组合，就可以表示这个平面上的任意一个向量了，那么现在有3个，多了，想怎么走就怎么走了另一种情况，稀疏矩阵的列数小于行数，比如：\\[\\begin{bmatrix} a_{11} \\\\ a_{21} \\\\ a_{31}  \\end{bmatrix} x_1 + \\begin{bmatrix} a_{12} \\\\ a_{22} \\\\ a_{32}  \\end{bmatrix} x_2  = \\begin{bmatrix} b_{1} \\\\ b_{2} \\\\ b_{3}  \\end{bmatrix}\\]给的方向还有目标点都是在三维空间中的，但是只给了两个方向，怎么组合那也只能是在一个平面里折腾，如果恰好$b$这个目标点在这个平面上，那么可以完成任务，否则是不能完成的，方程组也就无解每一种$(x_1,x_2)$的取值都对应了一种$\\mathbf{A}$的列向量的线性组合 (linear combination)，线性组合是对向量的操作，矩阵的列向量的线性组合 = 这个矩阵的列空间（column space） 或者是这个矩阵的值域（range）$\\mathbf{A}\\mathbf{x}=\\mathbf{b}$，如果$\\mathbf{b}$不在$\\mathbf{A}$的列空间里，那么这个问题就无解了。所以如果要有解，那么首先维度得对得上，即$\\mathbf{A}$的列空间的维度要大于等于$\\mathbf{b}$的维度####一组向量的生成子空间（span）就是关于它的线性组合能到达的点，一个矩阵的列空间（列向量的生成子空间）的维度就是秩（rank）Linear IndependencyIf $\\exists k\\in \\mathbb{R}$, s.t. $\\mathbf{x_1} = k\\cdot\\mathbf{x_2}$, then $x_1$ and $x_2$ are  linear dependent.如果一个矩阵中有两个列向量是线性相关的，那么其中一个被删掉了也不会改变这个矩阵的列空间EigenvalueDeterminantSingularCramer’s ruleAdjugate matrixSetMapping      单射 (Injective)： 一个函数$f: A \\to B$是单射的，当且仅当对于$A$中的任意两个不同的元素$a_1$和$a_2$（即$a_1 \\ne a_2$），我们都有$f(a_1) \\neq f(a_2)$。简单来说，不同的输入被映射到不同的输出。        满射 (Surjective)： 一个函数$f: A \\to B$是满射的，当且仅当对于集合$B$中的每一个元素$b$，都存在集合$A$中的某个元素$a$使得$f(a) = b$。简单来说，$B$中的每一个元素都是$f$的某个输出。        双射 (Bijective)： 一个函数$f: A \\to B$是双射的，当且仅当它既是单射的又是满射的。这意味着$f$为$A$中的每一个元素与$B$中的某一个元素建立了一一对应的关系，而且每一个元素在这种对应中都是独一无二的。如果存在这样的双射函数，则我们可以说集合$A$和$B$具有相同的势（cardinality）。  Isomorphic 同构映射首先定义“同构映射”是一个双射，它还保持某些基本的代数或其他结构特性。我们可以根据具体的结构来定义它。这样的映射通常被表示为 $f: A \\rightarrow B$，其中 A 和 B 是我们所考虑的集合。在不同的数学结构中的同构  群同构：在群论中，如果存在一个映射 $f: G \\rightarrow H$ 使得对所有的 $g_1, g_2 \\in G$，都有 $f(g_1 \\cdot g_2) = f(g_1) \\cdot f(g_2)$，其中“$\\cdot$”表示群的操作，则称$G$和$H$是同构的。  环同构：在环论中，一个环同构是一个保持加法和乘法操作的双射。即如果存在一个映射 $f: R \\rightarrow S$ 使得对所有的 $r_1, r_2 \\in R$，都有 $f(r_1 + r_2) = f(r_1) + f(r_2)$ 和 $f(r_1 \\cdot r_2) = f(r_1) \\cdot f(r_2)$，则称 R 和 S 是同构的。  向量空间同构：在线性代数中，一个线性变换可以是一个向量空间到另一个向量空间的同构，如果它是双射且保持向量加法和标量乘法。性质和结果  唯一性：如果两个结构是同构的，那么它们在结构上是“相同”的，这意味着它们的性质是相同的。因此，一个结构中的定理也将适用于另一个结构。  反对称性和传递性：同构具有反对称性和传递性。这意味着如果 $A$ 是与 $B$ 同构的，那么 $B$ 也是与 $A$ 同构的；如果 $A$ 是与 $B$ 同构的，并且 $B$ 是与 $C$ 同构的，那么 $A$ 是与 $C$ 同构的。例子  整数与偶数的集合：我们可以构建一个映射 $f: \\mathbb{Z} \\rightarrow 2\\mathbb{Z}$，通过 $f(n) = 2n$ 来定义。这里 $2\\mathbb{Z}$ 是偶数的集合。这个映射是一个同构，因为它是双射且保持加法运算。TopologyLogicCalculusCombinatorics  Permutation: Permutation refers to the arrangement of a certain number of elements from a set in a specific order. The number of permutations of $k$ elements taken from a set of $n$ elements is denoted as $P(n, k)$, and it is calculated using the formula: \\(P(n, k) = \\frac{n!}{(n-k)!}\\)  Combination: Combination refers to the selection of a certain number of elements from a set without considering the order. The number of combinations of $k$ elements taken from a set of $n$ elements is denoted as $C(n, k)$, and it is calculated using the formula: \\(C(n, k) = \\binom{n}{k} = \\frac{n!}{k!(n-k)!}\\)"
  },
  
  {
    "title": "English Toolbox",
    "url": "/posts/English-Toolbox/",
    "categories": "Misc Notes",
    "tags": "tech, English, toolbox",
    "date": "2023-04-06 18:40:01 +0000",
    





    
    "snippet": "  This note will be consistently updated.5 Principles and 7 Actions  This part is summarized from this talk.Principles1: Focus on langurage content that is relevant to youInformation that helps ach...",
    "content": "  This note will be consistently updated.5 Principles and 7 Actions  This part is summarized from this talk.Principles1: Focus on langurage content that is relevant to youInformation that helps achieve personal goals has relevance. If the content is relevant to you, then you will pay attention to it.This brings us to tools. We master tools by using tools, and we learn tools the fastest when they are relevant to us (e.g. a crisis). Because it was relevant, meaningful and important.2: Use your new language as a tool to communicate from day 1As a kid does.3: When you first understand the message, you will unconsciously acquire the languageComprehension is key. Comprehensible input approach is better than grammar focus language teaching.Language learning is not about accumulating a lot of knowldge. It’s about physiological training.Because we have filters in our brain that filter in the sounds that we are familiar with and they filter out the sounds of languages that we’re not. And if you can’t hear it, you won’t understand it. If you can’t understand it, you’re not going to learn it.4: Talking takes musclePractice coordinating your face muscle in a way that you make sounds that other people will understand. When you face hurts, you are doing it right.5: Psycho-physiological state mattersIf you are sad, angry, worried, upset, you are not going to learn. If you are happy, relaxed, in an Alpha brain state, curious, you are going to learn really quickly.You must learn to tolerate ambiguity.If you are one of those people who needs to understand 100 percent every word you are hearing, you will go nuts. Because you will be incredibly upset all the time, because you are not perfect.If you are comfortable with getting some, not getting some, just pay attention to what you do understand, you will be fine, relaxed, and you will be learning quickly.Actions1: Listen a lotIt is brain soaking.you put yourself in a context where you are hearing tons and tons and tons of a language, and it does not matter if you understand it or not. You are listening to the rhythms, to the patterns that repeat, you are lisening to things that stand out.2: Focus on getting the meaning first (before the words)E.g. by observing the body language.3: Start mixingIf you have got 10 verbs, 10 nouns and 10 adjectives, you can say 1000 different things.Language is a creative process.4: Focus on the coreAny language is high frequency content. In English 1000 words covers 85 percent of anything you are ever going to say in daily communication. And 3000 words gives you 98 percent of anything you are going to say in daily conversation. You got 3000 words, you are speaking the language. The rest is icing on the cake.Start with your tool box:  Week 1          What is this?      How do you say?      I don’t understand…      Repeat that pelase      what does that mean?        Week 2-3: Pronounds, common verbs, adjectives. Communicating like a baby          you      that      me      give      hot        Week 4: Glue words          And      But      Even though      therefore      5: Get yourself a language parentWhen a child is speaking, it will be using simple words, simple combinations. Sometimes quite strange, other people from outside the family don’t understand it. But the parents do.And so the kid has a safe environment, gets confidence. The parents talk to the children with body language, and with simple language they know the child understands. So you have a comprehensible input environment that is safe.So you get yourself a language parent, who is somebody interested in you as a person who will communicate with you, essentially as an equal but pay attention to hel you understand the language. Spouses are not very good at this.Language parent rules:  Works to understand what you are saying.  Does not correct mistakes.  Confirms understanding by using correct language.  Use words the learner knows.6: Copy the faceYou got to get the muscles working right, so you can sound in a way that people will understand you. There is a couple of things you do.You hear how it feels, and feel how it sounds, which means you have a feedback loop operating in your face. If you can look at a native speaker, and just observe how they use their face, and let your unconscious mind absorb the rules, then you are going to be able to pick it up.7: Direct connect to mental imagesDon’t translate it to your familiar language. Everything you know is an image inside your mind, its feelings. You go into that imagery and all of that memory and you come out with another pathway. Same box, different path.Latin expressions (Loanwords)(Organized from this note.)  ad hoc          For this; Formed or done for a particular purpose only.      An ad hoc committee was set up to oversee the matter.        et cetera, etcetera, etc., et cet, &amp;c., or &amp;c          And the rest; And so on; And more.      We urgently need to buy medical equipment, drugs, et cetera.      Et cetera is used when listing groups of nouns and adjectives.        et al. or et alia          And others.      Mike and Carol had six kids named Greg, Marcia, Peter, et al.      Et al. is used to reference other people not specifically named in your list.        per          For each.      This petrol station charges $5.00 per gallon.        per annum or p.a.          For each year.      The population is increasing by about 2% per annum.        per capita          For each person.      The country’s annual income is $5000 per capita.        per se          In itself/themselves; Intrinsically.      These facts per se are not important.        re          About; Concerning; Regarding.      Re: Unpaid Invoice.        status quo          Existing state of affairs.      Monarchies naturally wish to maintain the status quo.        versus, vs or v.          Against.      What are the enefits of organic versus inorganic foods?        vice versa          The other way round.      My phone serves me, and not vice versa.      GrammarGrammarBook.comTOEFLResources  ETS  KMF  Test ResourcesOverview  Sections and time scheduleReadingSpeaking  16 min.  4 questions.          Independent Speaking      Integrated Speaking                  Campus Announcement          General to Specific          Academic Lecture                    Tips  Q1          15s prep. 45s speaking.        Q2          Q2 is about a campus envent. The reading material is a proposal or an announcement. The listening material is usually a conversation about the reading material.      30s prep. 60s speaking.      Reading summary in around 20s: Summarize the subject in a single sentence, followed by a separate sentence for each key point.      Speak while wathcing the timer.        Q3          Q3 requires me to introduce a scientific concept. The reading material is a brief introduction exerpt from the textbook. The listening material is a part of a lecture containing explanation with examples.      30s prep. 60s speaking.      Independent SpeakingTemplate (15s prep, 45s answer)Well, personally, I agree/disagree with the statement that [just read the statement].I feel this way for several reasons.First of all, …For example, … (and a personal example).Additionally, …To be more specific, … (and a few more details).Arguments  Expand horizons          I think it can broaden my horizons because      I think it can enrich citizen’s cultrual life              Learn          I can learn practical skills, in addition to academic knowledge      I can learn lots of vital skills and knowledge      e.g., presentation skills, communication skills, organization skills        Money, finance, economy          Because it can help me become financially independent      I can save money by doing so.      It can generate more revenues in order to enhance city’s image        Relax, enjoy, have fun          I love sports games and outdoor activities. Because they are fun.      I can make new friends, breathe in the fresh air      I can have a good balance between work and leisure.      It can improve quality of life. It can reduce stress.        Emotion          A good friend/mentor can tell me how to distinguish right from wrong.      A good friend will help me establish my value and belief.      I can have lots of emotional support from my friends. They stand by me. They are always there for me when I need help.        Health                  Efficiency  Communication          It can help me make friends with people from different backgrounds.        Trait, characteristic  InterestIntegrated WritingTemplateA’s points are denoted by $X_1, X_2, X_3$,And B’s are denoted by $Y_1, Y_2, Y_3$.Tips:  Identify key points.          Focus on the lecture.      Summarize the passage.        Remember that all the points should be paraphrased.          Vocabulary.      Use synonyms.        225 words.  20 min.In the passage, the author mentions  3 benefits coming from posing high taxes on cigarettes and unhealthy food.  3 potential solutions to the increasing salinity of the Salton Sea in California.However, in the lecture, the speaker casts doubts on them.Initially, the author believes that $X_1$. In contrast, the speaker argues that $Y_1$. (Then focus on the speaker’s argument.)Furthermore, the author claims that $X_2$. However, the speaker disproves it by pointing out that $Y_2$. (Then focus on the speaker’s argument.)Additionally, the author puts forward that $X_3$. Instead, the speaker challenges the author’s opinion and offers a different perspective that $Y_3$. (Then focus on the speaker’s argument.)Academic discussion10 minutes."
  },
  
  {
    "title": "Certification of Rank",
    "url": "/posts/Certification-Rank/",
    "categories": "Misc Notes",
    "tags": "misc note, certification of rank",
    "date": "2023-04-04 12:00:00 +0000",
    





    
    "snippet": "",
    "content": ""
  },
  
  {
    "title": "Swinging Search and Crawling Control",
    "url": "/posts/SSCC/",
    "categories": "Robotics",
    "tags": "tech, my paper, redundant manipulator, reinforcement learning",
    "date": "2023-04-03 12:00:01 +0000",
    





    
    "snippet": "  Please be aware that the video accompanying this article may take some time to load, depending on the speed of your internet connection to GitHub.A snake-inspired path planning algorithm based on...",
    "content": "  Please be aware that the video accompanying this article may take some time to load, depending on the speed of your internet connection to GitHub.A snake-inspired path planning algorithm based on reinforcement learning and self-motion for hyper-redundant manipulatorsSSCC = Swinging Search (self-motion for path planning) and Crawling Control (crawling into the pipe like a snake)  Self-motion is a special kind of motion for redundant manipulators. Given a fixed base and an end target, there can be more than one possible solution configuration due to its redundancy. Searching configurations (for a collision-free path) within these constraints (fixing both ends of a manipulator) can be likened to a rope swinging in the air.VideosCrawling Control: a 16 DoF manipulator crawling into a simple pipe.Sources  PDF  Videos  Codes  Self-Motion demo"
  },
  
  {
    "title": "RHex-T3 (A Mobile Robot, with Hybrid Leg Design)",
    "url": "/posts/RHex-T3/",
    "categories": "Robotics",
    "tags": "tech, my paper, robotic mechanism design, mobile robot, kinematics",
    "date": "2023-04-03 12:00:00 +0000",
    





    
    "snippet": "  Please be aware that the videos accompanying this article may take some time to load, depending on the speed of your internet connection to GitHub.Innovative design and simulation of a transforma...",
    "content": "  Please be aware that the videos accompanying this article may take some time to load, depending on the speed of your internet connection to GitHub.Innovative design and simulation of a transformable robot with flexibility and versatility, RHex-T3RHex-T3 = RHex - To The Top (for it can climb ladders)  RHex is a highly successful robot with excellent properties. The robot has six legs that are shaped like semicircles, with a motor installed at the end of each arc. We have redesigned its legs to perform a wide range of functions.Sources  PDF  VideosA substantial amount of code I wrote (kinematics, simulation, physical control) cannot be made public, because the lab I used to work will continue to conduct related research.VideosThis work is self-explanatory, as the videos that follow are easy to understand.OverviewExploded ViewTransmission SystemWorkspaceWorkspaceHook ModeLeg ModeRHex ModeMode SwitchingWheel Mode"
  },
  
  {
    "title": "Markdown Syntax",
    "url": "/posts/markdown-template/",
    "categories": "Efficiency, Tools",
    "tags": "tech, efficiency, tools, markdown",
    "date": "2023-04-03 07:48:44 +0000",
    





    
    "snippet": "  Adapted from this postPrompts  green tip  blue info  yellow warning  red danger&gt; green tip{: .prompt-tip }&gt; blue info{: .prompt-info }&gt; yellow warning{: .prompt-warning }&gt; red danger{...",
    "content": "  Adapted from this postPrompts  green tip  blue info  yellow warning  red danger&gt; green tip{: .prompt-tip }&gt; blue info{: .prompt-info }&gt; yellow warning{: .prompt-warning }&gt; red danger{: .prompt-danger }My common usage  This note will be consistently updated.  This section is generated by ChatGPT 4.  This section is adapted from the answer generated by ChatGPT 4.  The following part has not been finished yet.Titles# H1## H2### H3ListOrdered List1. 1112. 2223. 333Unordered ListThe indentation determines the structure, and the choice of * or - or + is insignificant.- 111    - 222        - 333TODO ListThe indentation determines the structure, and the choice of * or - or + is insignificant.- [ ] Job    - [x] Step 1    - [x] Step 2    - [ ] Step 3DescriptionSun: the star around which the earth orbitsEmphasis*Italian***Bold**Hyper Link[name](url)Block Quote&gt; blahblahTable            Company      Contact      Country                  Alfreds Futterkiste      Maria Anders      Germany              Island Trading      Helen Bennett      UK              Magazzini Alimentari Riuniti      Giovanni Rovelli      Italy      FootnoteClick the hook will locate the footnote1, and here is another footnote2.Filepath`/path/to/the/file.extend`{: .filepath}.Mathematics(front matter: math: True)\\(\\sum_{n=1}^\\infty 1/n^2 = \\frac{\\pi^2}{6}\\)$$ \\sum_{n=1}^\\infty 1/n^2 = \\frac{\\pi^2}{6} $$Image &amp; VideoDefault (with caption)captionLeft AlignedFloat to LeftPraesent maximus aliquam sapien. Sed vel neque in dolor pulvinar auctor. Maecenas pharetra, sem sit amet interdum posuere, tellus lacus eleifend magna, ac lobortis felis ipsum id sapien. Proin ornare rutrum metus, ac convallis diam volutpat sit amet. Phasellus volutpat, elit sit amet tincidunt mollis, felis mi scelerisque mauris, ut facilisis leo magna accumsan sapien. In rutrum vehicula nisl eget tempor. Nullam maximus ullamcorper libero non maximus. Integer ultricies velit id convallis varius. Praesent eu nisl eu urna finibus ultrices id nec ex. Mauris ac mattis quam. Fusce aliquam est nec sapien bibendum, vitae malesuada ligula condimentum.Float to RightPraesent maximus aliquam sapien. Sed vel neque in dolor pulvinar auctor. Maecenas pharetra, sem sit amet interdum posuere, tellus lacus eleifend magna, ac lobortis felis ipsum id sapien. Proin ornare rutrum metus, ac convallis diam volutpat sit amet. Phasellus volutpat, elit sit amet tincidunt mollis, felis mi scelerisque mauris, ut facilisis leo magna accumsan sapien. In rutrum vehicula nisl eget tempor. Nullam maximus ullamcorper libero non maximus. Integer ultricies velit id convallis varius. Praesent eu nisl eu urna finibus ultrices id nec ex. Mauris ac mattis quam. Fusce aliquam est nec sapien bibendum, vitae malesuada ligula condimentum.Video            The footnote source &#8617;              The 2nd footnote source &#8617;      "
  },
  
  {
    "title": "Writing Buffer",
    "url": "/posts/Writing-Buffer/",
    "categories": "Misc Notes",
    "tags": "misc note, writing buffer",
    "date": "2023-04-01 18:40:00 +0000",
    





    
    "snippet": "  This post is used for storing temporary content, which will be classified later. I don’t want my thought process to be interrupted by format-related issues while composing.Eigenvalue另一个介绍ID的思路bat...",
    "content": "  This post is used for storing temporary content, which will be classified later. I don’t want my thought process to be interrupted by format-related issues while composing.Eigenvalue另一个介绍ID的思路battle of sexesmixed strategy Nash equilibrium甚至比任何一个pure strategy Nash equilibrium还差可以引入一个trusted authority，扔硬币，这样结果就会好很多一个角度是：ID可以认为是让trusted authority骗人"
  },
  
  {
    "title": "Writing Schedule",
    "url": "/posts/Schedule/",
    "categories": "Misc Notes",
    "tags": "misc note, writing schedule",
    "date": "2023-04-01 18:40:00 +0000",
    





    
    "snippet": "Writing ScheduleGenerally, I will finish my blogs in the following order:  Zero-Determinant Strategy;  Sequential Social Dilemma  Fictitious Play and Zero-Shot Coordination;  Sequence-to-Sequence M...",
    "content": "Writing ScheduleGenerally, I will finish my blogs in the following order:  Zero-Determinant Strategy;  Sequential Social Dilemma  Fictitious Play and Zero-Shot Coordination;  Sequence-to-Sequence Models;  Details on the Analysis of Policy Gradient Methods;  MARL Basics;  Information Design: Almost finished;Depending on the circumstances, there may be additional blogs added to the queue along the way. The order of completion is not fixed.If I come up with something and have spare time, I will add content to the corresponding blog at any time.Learned a Bit but Forgotten 🥹  Differential Privacy  Meta-Learning  Inverse RL  Generative Models          diffusion      GAN      Variational Autoencoders (VAEs)      Energy-Based Models (EBMs)        Evolutionary RL  CV basics"
  }
  
]

