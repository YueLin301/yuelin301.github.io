---
title: Sequence-to-Sequence Models
date: 2023-07-16 02:40:00 +0800
categories: [Machine Learning Basics]
tags: [NLP, RNN, LSTM, GRU, Seq2Seq, Transformer, BERT, GPT]
math: True
pin: True
---


## NLP Terms

NLP = Natural Language Processing

### Embedding
> In a general sense, "embedding" refers to the process of representing one kind of object or data in another space or format. It involves mapping objects from a higher-dimensional space into a lower-dimensional space while preserving certain relationships or properties.
>
> *— GPT 3.5*

For example, the Principal Component Analysis (PCA) algorithm is an embedding technique. PCA is a widely used dimensionality reduction technique that projects high-dimensional data into a lower-dimensional space, while retaining as much of the data's variance as possible. In my understanding, the constraint on variance in PCA is intended to allow us to distinguish each point as effectively as possible in the new lower-dimensional space. And that is the "preserved property" in this case.

In NLP, the embedding layer is used to **obtain the feature vector for each token in the input sequence**. One-hot is a simple example of the embedding.

#### Items $\to$ one-hot variables:

```python
import torch
import torch.nn.functional as F

A = torch.tensor([3, 0, 2])
output1 = F.one_hot(A)
'''
tensor([[0, 0, 0, 1],
        [1, 0, 0, 0],
        [0, 0, 1, 0]])
'''

output2 = F.one_hot(A, num_classes=5)
'''
tensor([[0, 0, 0, 1, 0],
        [1, 0, 0, 0, 0],
        [0, 0, 1, 0, 0]])
'''
```

#### Items $\to$ learnable embedded variables:

```python
import torch

torch.manual_seed(1)

words = ["hello", "world", "haha"]
vocab_size = len(words)
idx = list(range(vocab_size))
dictionary = dict(zip(words, idx))  # {'hello': 0, 'world': 1, 'haha': 2}
embedding_dim = 2

embedding_layer = torch.nn.Embedding(num_embeddings=vocab_size,     # how many values in a dim (input)
                                     embedding_dim=embedding_dim)   # output_dim
print(list(embedding_layer.parameters()))
'''
[Parameter containing:
tensor([[ 0.6614,  0.2669],
        [ 0.0617,  0.6213],
        [-0.4519, -0.1661]], requires_grad=True)]
'''

lookup_tensor = torch.tensor([dictionary["haha"]], dtype=torch.long)
haha_embed = embedding_layer(lookup_tensor)
print(haha_embed)
# tensor([[-0.4519, -0.1661]], grad_fn=<EmbeddingBackward0>)
# The result is exactly the third row of the embedding layer parameter matrix.
```

[`torch.nn.Embedding`](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html):
- Each element in the weight matrix is sampled from $\mathcal{N}(0,1)$.
- "An Embedding layer is essentially just a Linear layer." (From [this website](https://discuss.pytorch.org/t/how-does-nn-embedding-work/88518/3).)
- The `padding_idx` parameter: (`int`, optional) - If specified, the entries at padding_idx do not contribute to the gradient.
- `PAD_TOKEN`, `<BOS>` and others should be considered in the `num_embeddings` count.


Map back: 
- Given an embedding result $y$, find the corresponding word $x$.
- Calculate the embeddings of all words $(y_1, \ldots, y_n)$, 
- $x = \arg\min_i \Vert y - y_i \Vert$

```python

all_embeddings = embedding_layer(torch.arange(vocab_size))
distances = torch.norm(all_embeddings - hello_embed, dim=1)
min_index = torch.argmin(distances).item()
closest_word = [word for word, idx in dictionary.items() if idx == min_index][0]
print(f"Closest word for the given vector: {closest_word}")
# Closest word for the given vector: haha
```

#### Integer (index) tensors $\to$ learnable embedded variables:

```python
import torch

vocab_size = 10
embedding_dim = 3

embedding_layer = torch.nn.Embedding(vocab_size, embedding_dim)
print(list(embedding_layer.parameters()))
'''
tensor([[-2.3097,  2.8327,  0.2768],
        [-1.8660, -0.5876, -0.5116],
        [-0.6474,  0.7756, -0.1920],
        [-1.2533, -0.7186,  1.8712],
        [-1.5365, -1.0957, -0.9209],
        [-0.0757,  2.3399,  0.9409],
        [-0.9143,  1.3293,  0.8625],
        [ 1.3818, -0.1664, -0.5298],
        [ 2.2011, -0.8805,  1.7162],
        [-0.9934,  0.3914,  0.9149]], requires_grad=True)]
'''

# A batch of 2 samples of 4 indices each
# Each index is in [0, vocab_size - 1]
input = torch.LongTensor([[1, 2, 4, 5],     # Sentence A
                          [4, 3, 2, 9]])    # Sentence B
output = embedding_layer(input)
print(output)
'''
tensor([[[-1.8660, -0.5876, -0.5116],
         [-0.6474,  0.7756, -0.1920],
         [-1.5365, -1.0957, -0.9209],
         [-0.0757,  2.3399,  0.9409]],

        [[-1.5365, -1.0957, -0.9209],
         [-1.2533, -0.7186,  1.8712],
         [-0.6474,  0.7756, -0.1920],
         [-0.9934,  0.3914,  0.9149]]], grad_fn=<EmbeddingBackward0>)
'''
```

#### Differentiable embedding

> "An Embedding layer is essentially just a Linear layer." From [this website].(https://discuss.pytorch.org/t/how-does-nn-embedding-work/88518/1)

```python
embedding_layer = torch.nn.Sequential(
    torch.nn.Linear(input_dim, output_dim, bias=False),
)
```


### Tokenization

> Check [this site](https://neptune.ai/blog/tokenization-in-nlp).
{: .prompt-info }

```python
sentence = 'We do not see things as they are, we see them as we are.'

character_tokenization = list(sentence)
# ['W', 'e', ' ', 'd', 'o', ' ', 'n', 'o', 't', ' ', 's', 'e', 'e', ' ', 't', 'h', 'i', 'n', 'g', 's', ' ', 'a', 's', ' ', 't', 'h', 'e', 'y', ' ', 'a', 'r', 'e', ',', ' ', 'w', 'e', ' ', 's', 'e', 'e', ' ', 't', 'h', 'e', 'm', ' ', 'a', 's', ' ', 'w', 'e', ' ', 'a', 'r', 'e', '.']

word_tokenization = sentence.split()
# ['We', 'do', 'not', 'see', 'things', 'as', 'they', 'are,', 'we', 'see', 'them', 'as', 'we', 'are.']

sentence_tokenization = sentence.split(', ')
# ['We do not see things as they are', 'we see them as we are.']
```

### BoW

BOW = Bag of Words

In my understanding, BoW is a step 
- to make and clean a word-level tokenization, and
- to count and store the occurrences of each word.

> For instance, given the vocabulary `{apple, banana, cherry}`:
> - Text: `"apple banana apple"`
> - BoW representation: `{2, 1, 0}`
> 
> *— ChatGPT 4*

### Word2Vec

Word2Vec = Word to Vector

> Word2Vec is a group of related models that are used to produce word embeddings. Word2Vec takes a large corpus of text as its input and produces a high-dimensional space (typically of several hundred dimensions), with each unique word in the corpus being assigned a corresponding vector in the space.
> 
> *— ChatGPT 4*


In my understanding:
- During training, the process involves presenting a set of words (the context) and predicting the surrounding words. 
- The model learns from reading the corpus to understand the context in which each word occurs in the language. 
- If a certain word at a particular position in a context appears frequently in the corpus, then when given that context, the model's output probability for that word should also be high.

#### CBOW

CBOW = Continuous Bag of Words

The CBOW model predicts the current word based on its context. The objective function to maximize can be expressed as:
$$ J(\theta) = \frac{1}{T} \sum_{t=1}^{T} \log p(w_t | C_t) $$

#### Skip-Gram

The Skip-Gram model predicts the context given a word. Its objective function can be expressed as:
$$ J(\theta) = \frac{1}{T} \sum_{t=1}^{T} \sum_{-c \leq j \leq c, j \neq 0} \log p(w_{t+j} | w_t) $$

Here, $T$ is the total number of words in the training corpus, and $c$ is the size of the context.


![Skip-Gram-CBOW](https://www.baeldung.com/wp-content/uploads/sites/4/2021/03/Screenshot-2021-03-05-at-11.29.31-1024x616-1.png){: width="600" height="600" }
_Illustration of CBOW and Skip-Gram from the paper ["Exploiting Similarities among Languages for Machine Translation"](https://arxiv.org/pdf/1309.4168v1.pdf)._


## RNN Models

### [RNN](https://d2l.ai/chapter_recurrent-neural-networks/rnn.html)

RNN = Recurrent Neural Network

$$
\begin{aligned}
    \begin{cases}
      h_t = f_{w_h}(x_t, h_{t-1}) \\
      o_t = g_{w_o}(h_t) \\
    \end{cases}
\end{aligned}
$$

![RNN](https://d2l.ai/_images/rnn-bptt.svg){: width="500" height="500" }
_Illustration of RNN from the book "Dive into Deep Learning". Boxes represent variables (not shaded) or parameters (shaded) and circles represent operators._


If there are $L$ RNN layers, then

$$
\begin{aligned}
    \begin{cases}
      h_t^1 = \mathrm{Sigmoid}\left(\mathrm{Linear}(x_t) + \mathrm{Linear}(h_{t-1}^1)\right) \\
      h_t^2 = \mathrm{Sigmoid}\left(\mathrm{Linear}(h_t^1) + \mathrm{Linear}(h_{t-1}^2)\right) \\
      \ldots \\
      o_t = \mathrm{Linear}(h_t^L)
    \end{cases}
\end{aligned}
$$

![Stacked-RNN](/assets/img/23-07-16-Seq2Seq/stacked-RNN.jpg){: width="500" height="500" }
_Illustration of Stacked RNN from the paper ["RNNCon: Contribution Coverage Testing for Stacked Recurrent Neural Networks"](https://www.mdpi.com/1099-4300/25/3/520)._


```python
# Adapted from https://pytorch.org/docs/stable/generated/torch.nn.RNN.html
import torch

rnn = torch.nn.RNN(10, 20, 2)   # input_size, hidden_size, num_layers
input = torch.randn(5, 3, 10)   # sequence_length, batch_size, input_size
h0 = torch.randn(2, 3, 20)      # num_layers, batch_size, hidden_size
output, hn = rnn(input, h0)     # h0: Defaults to zeros if not provided.

print(output.size(), hn.size())
# torch.Size([5, 3, 20]) torch.Size([2, 3, 20])
# output size: sequence_length, batch_size, hidden_size

# If parameter batch_first=True,
# then the first parameter should be the batch_size.
```

In NLP:
- An input is a sentence. The `sequence_length` is the number of words in the sentence.
- The `input_size` is the embedding dimension of each word.
- The `output_size` equals the `sequence_length`.
- All the hidden states have the same `hidden_size`.

An example that makes it easy to remember the dimensions of various quantities:

```python
# input_size = embedding_dim = 3
embedded_words = {"I": [0.3, 0.5, 0.1],
                  "am": [0.4, 0.4, 0.3],
                  "so": [0.1, 0.9, 0.2],
                  "happy": [0.5, 0.3, 0.8],
                  "sad": [0.2, 0.6, 0.1],
                  ".": [0.6, 0.2, 0.6],
                  "<EOS>": [0.0, 0.0, 0.0],
                  "<PAD>": [0.9, 0.9, 0.9]}

sentences = ["I am so happy.",
             "I am sad."]

# dataset_size = (batch_size, sequence_length, embedding_dim) = (2, 6, 3,)
# sequence_length = max(sentence_length); Padding
embedded_sentences = [[[0.3, 0.5, 0.1],
                       [0.4, 0.4, 0.3],
                       [0.1, 0.9, 0.2],
                       [0.5, 0.3, 0.8],
                       [0.2, 0.6, 0.1],
                       [0.0, 0.0, 0.0]],

                      [[0.3, 0.5, 0.1],
                       [0.4, 0.4, 0.3],
                       [0.5, 0.3, 0.8],
                       [0.6, 0.2, 0.6],
                       [0.0, 0.0, 0.0],
                       [0.9, 0.9, 0.9]]]

# hidden_size, num_layers: determined by parameter tuning :)
```

### [BPTT](https://d2l.ai/chapter_recurrent-neural-networks/bptt.html)

BPTT = Backpropagation Through Time

$$
L = \frac{1}{T}\sum\limits_{t} l(o_t,y_t)
$$  
$$
\begin{aligned}
    \frac{\partial L}{\partial w_o} =& \frac{1}{T}\sum\limits_{t} \frac{\partial l(y_t,o_t)}{\partial w_o} \\
    =& \frac{1}{T}\sum\limits_{t}
    \frac{\partial l(y_t,o_t)}{\partial o_t}\cdot
     \frac{\partial o_t}{\partial w_o}
\end{aligned}
$$  
$$
\begin{aligned}
    \frac{\partial L}{\partial w_h} =& \frac{1}{T}\sum\limits_{t} \frac{\partial l(y_t,o_t)}{\partial w_h} \\
    =& \frac{1}{T}\sum\limits_{t} \frac{\partial l(y_t,o_t)}{\partial o_t} \cdot
     \frac{\partial o_t}{\partial h_t} \cdot
      \textcolor{red}{\frac{\partial h_t}{\partial w_h}}
\end{aligned}
$$  

$$
\begin{aligned}
    \textcolor{red}{\frac{\partial h_t}{\partial w_h}} =&
    \frac{\partial f(x_t, h_{t-1}, w_h)}{\partial w_h} + \frac{\partial f(x_t, h_{t-1}, w_h)}{\partial h_{t-1}} \cdot 
    \textcolor{red}{\frac{\partial h_{t-1}}{\partial w_h}}
\end{aligned}
$$

$$
\begin{cases}
      z_0 = a_0 = 0 \\
      z_k = a_k + b_k \cdot z_{k-1} \\
\end{cases}
$$

$$
\begin{aligned}
    z_k = a_k + \sum\limits_{i=0}^{k-1}
        \left(\prod\limits_{j=i+1}^k b_j \right) \cdot a_i
\end{aligned}
$$

### [LSTM](https://d2l.ai/chapter_recurrent-modern/lstm.html)

LSTM = Long Short-Term Memory

- To learn long-term dependencies (owing to vanishing and exploding gradients).
- In my understanding, there are two kinds of hidden states, $\mathrm{c}$ and $\mathrm{h}$. And $\mathrm{c}$ is renamed as the **memory cell internal state**.

Each layer is like:

$$
\begin{aligned}
  \begin{cases}
      i_t =& \mathrm{Sigmoid}\left(\mathrm{Linear}(x_t) + \mathrm{Linear}(h_{t-1})\right) \\
      f_t =& \mathrm{Sigmoid}\left(\mathrm{Linear}(x_t) + \mathrm{Linear}(h_{t-1})\right) \\
      g_t =& \mathrm{tanh}\left(\mathrm{Linear}(x_t) + \mathrm{Linear}(h_{t-1})\right) \\
      o_t =& \mathrm{Sigmoid}\left(\mathrm{Linear}(x_t) + \mathrm{Linear}(h_{t-1})\right) \\
      c_t =& f_t \odot c_{t-1} + i_t \odot g_t \\
      h_t =& o_t \odot \mathrm{tanh}(c_t)
  \end{cases}
\end{aligned}
$$

> Adapted from [the PyTorch document](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html).
{: .prompt-info }

- $h_t$: the hidden state.
- $c_t$: the cell state.
- $i_t$: the input gate.
- $f_t$: the forget gate.
- $g_t$: the cell gate.
- $o_t$: the output gate.
- All values of the three gates are in the range of $(0, 1)$ because of the sigmoid function.
- $g_t$ is the vanilla part of an RNN, and it indicates the information that we currently get.
- $i_t$ controls how much we cares about the current information.
- $c$ is an addtional hidden state channel, and it also indicates the memory.
- $f_t$ controls how much we cares about the memory.
- $c_t$ and $h_t$ do not impact the curren output $o_t$.


![LSTM](https://d2l.ai/_images/lstm-3.svg){: width="700" height="700" }
_Illustration of LSTM from the book "Dive into Deep Learning"._

```python
# Adapted from https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html
import torch

rnn = torch.nn.LSTM(10, 20, 2)    # input_size, hidden_size, num_layers
input = torch.randn(5, 3, 10)     # sequence_length, batch_size, input_size
h0 = torch.randn(2, 3, 20)        # num_layers, batch_size, hidden_size
c0 = torch.randn(2, 3, 20)        # num_layers, batch_size, hidden_size
output, (hn, cn) = rnn(input, (h0, c0)) # h0: Defaults to zeros if not provided.

print(output.size(), hn.size(), cn.size())
# torch.Size([5, 3, 20]) torch.Size([2, 3, 20]) torch.Size([2, 3, 20])
# output size: sequence_length, batch_size, hidden_size

# If parameter batch_first=True,
# then the first parameter should be the batch_size.
```

### [GRU](https://d2l.ai/chapter_recurrent-modern/gru.html)

GRU = Gated Recurrent Units

Each layer is like:

$$
\begin{aligned}
  \begin{cases}
      r_t =& \mathrm{Sigmoid}\left(\mathrm{Linear}(x_t) + \mathrm{Linear}(h_{t-1})\right) \\
      z_t =& \mathrm{Sigmoid}\left(\mathrm{Linear}(x_t) + \mathrm{Linear}(h_{t-1})\right) \\
      n_t =& \mathrm{tanh}\left(\mathrm{Linear}(x_t) + r_t \odot \mathrm{Linear}(h_{t-1})\right) \\
      h_t =& (1-z_t)\odot n_t + z \odot h_{t-1}
  \end{cases}
\end{aligned}
$$

- $h_t$: the hidden state. It can be used as the output.
- $r_t$: the reset gate, controls how much we cares about the memory. It is a bit like the forget gate $f_t$ in LSTM
- $z_t$: the update gate, controls how much we cares about the current information. It is a bit like the input gate $i_t$ in LSTM.
- $n_t$: the candidate hidden state, or the new gate.
  - If the reset gate $r_t$ is close to $1$, then it is like the vanilla RNN.
  - If the reset gate $r_t$ is close to $0$, then the new gate $n_t$ is the result of an MLP of $x_t$.

![GRU](https://d2l.ai/_images/gru-3.svg){: width="700" height="700" }
_Illustration of GRU from the book "Dive into Deep Learning"._

```python
# Adapted from https://pytorch.org/docs/stable/generated/torch.nn.GRU.html
import torch

rnn = torch.nn.GRU(10, 20, 2)   # input_size, hidden_size, num_layers
input = torch.randn(5, 3, 10)   # sequence_length, batch_size, input_size
h0 = torch.randn(2, 3, 20)      # num_layers, batch_size, hidden_size
output, hn = rnn(input, h0)     # h0: Defaults to zeros if not provided.

print(output.size(), hn.size())
# torch.Size([5, 3, 20]) torch.Size([2, 3, 20])
# output size: sequence_length, batch_size, hidden_size

# If parameter batch_first=True,
# then the first parameter should be the batch_size.
```


---

## [Encoder-Decoder](https://d2l.ai/chapter_recurrent-modern/seq2seq.html)

### Variable-length inputs

- Truncation and Padding
  - [Dive Into Deep Learning 10.5.3](https://d2l.ai/chapter_recurrent-modern/machine-translation-and-dataset.html#loading-sequences-of-fixed-length)
- Relation Network
  - [A blog](https://medium.com/@andre.holzner/learning-a-function-with-a-variable-number-of-inputs-with-pytorch-c487e35d4dba)
  - [ICLR 2017](https://arxiv.org/pdf/1702.05068.pdf)
- Embedding.
- **Encoder-decoder.**


> In general sequence-to-sequence problems like machine translation (Section 10.5), inputs and outputs are of varying lengths that are unaligned. The **standard** approach to handling this sort of data is to design an encoder–decoder architecture (Fig. 10.6.1) ... *— Dive into Deep Learning.*

### The structure

![Encoder-Decoder](https://d2l.ai/_images/encoder-decoder.svg){: width="600" height="600" }
_Illustration of the encoder-decoder architecture from the book "Dive into Deep Learning"._

- Encoder: `"Hello, world."` $\to$ a hidden state (or context variable) of fixed-shape.
- Decoder 1: the state $\to$ `"你好，世界。"`
- Decoder 2: the state $\to$ `"Hola mundo"`

![Encoder-Decoder-Teacher-Forcing](https://d2l.ai/_images/seq2seq.svg){: width="700" height="700" }
_Illustration of the encoder-decoder architecture (teacher forcing) from the book "Dive into Deep Learning"._

![Encoder-Decoder-Prediction](https://d2l.ai/_images/seq2seq-predict.svg){: width="700" height="700" }
_Illustration of the encoder-decoder architecture (prediction) from the book "Dive into Deep Learning"._

The encoder and the decoder are usually RNNs.
- `<eos>` means the end of the sequence. 
  - Inputting `<eos>` into the encoder indicates the end of this sentence. 
  - In prediction: When the decoder outputs `<eos>`, it will automatically stop and no longer continue generating output.
- `<bos>` means the beginning of the sequence, used to signal the decoder when to begin generating a new sequence.
- The input of the encoder is a variable-length sequence, but its output is of fixed-length, named as the state or the context variable $c$.
-  $c = q(h_1, \ldots, h_t)$, where $q$ is a customized function. In the figures, $c = h_t$.
-  The context variable will be fed into the decoder at evry time step or at the first time step.
-  Teacher Forcing: The input of the decoder is `(<bos>, sequence)`, and the target is `(sequence, <eos>)`.
-  Prediction: The input of the decoder at every time step is the output from the previous time step.
-  When calculating the loss, the padding tokens are masked.

![Encoder-Decoder](https://d2l.ai/_images/seq2seq-details.svg){: width="500" height="500" }
_Illustration of the encoder-decoder architecture where the RNNs are stacked, from the book "Dive into Deep Learning"._

### Teacher forcing

> Without using teacher forcing, the model at each timestep would receive the output from the previous timestep and use this output to predict the next timestep. However, this approach has an inherent problem: early in training, the model is likely to produce incorrect predictions, leading the next timestep prediction to be based on this incorrect output. Such mistakes can accumulate in subsequent timesteps.
> 
> To combat this, the Teacher Forcing technique is introduced during training. Specifically, instead of the model receiving its prediction from the previous timestep, it directly receives the actual output from the previous timestep. In this way, even if the model makes an error at a particular timestep, it can continue making predictions based on the actual data, preventing error accumulation.
> 
> *— ChatGPT 4*

### Code

```python
# Generated by ChatGPT 4

import torch
import torch.nn as nn
import torch.optim as optim


class Encoder(nn.Module):
    def __init__(self, input_dim, emb_dim, hidden_dim, n_layers, dropout):
        super().__init__()
        self.embedding = nn.Embedding(input_dim, emb_dim)
        self.rnn = nn.RNN(emb_dim, hidden_dim, n_layers, dropout=dropout)
        self.dropout = nn.Dropout(dropout)

    def forward(self, src):
        embedded = self.dropout(self.embedding(src))
        outputs, hidden = self.rnn(embedded)
        return hidden


class Decoder(nn.Module):
    def __init__(self, output_dim, emb_dim, hidden_dim, n_layers, dropout):
        super().__init__()
        self.output_dim = output_dim
        self.embedding = nn.Embedding(output_dim, emb_dim)
        self.rnn = nn.RNN(emb_dim, hidden_dim, n_layers, dropout=dropout)
        self.fc_out = nn.Linear(hidden_dim, output_dim)
        self.dropout = nn.Dropout(dropout)

    def forward(self, input, hidden):
        input = input.unsqueeze(0)
        embedded = self.dropout(self.embedding(input))
        output, hidden = self.rnn(embedded, hidden)
        prediction = self.fc_out(output.squeeze(0))
        return prediction, hidden


class Seq2Seq(nn.Module):
    def __init__(self, encoder, decoder, device):
        super().__init__()
        self.encoder = encoder
        self.decoder = decoder
        self.device = device

    def forward(self, src, trg, teacher_forcing_ratio=0.5):
        trg_len = trg.shape[0]
        batch_size = trg.shape[1]
        trg_vocab_size = self.decoder.output_dim
        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)
        hidden = self.encoder(src)
        input = trg[0, :]
        for t in range(1, trg_len):
            output, hidden = self.decoder(input, hidden)
            outputs[t] = output
            teacher_force = torch.rand(1).item() < teacher_forcing_ratio
            top1 = output.argmax(1)
            input = trg[t] if teacher_force else top1
        return outputs


INPUT_DIM = 1000
OUTPUT_DIM = 1000
ENC_EMB_DIM = 256
DEC_EMB_DIM = 256
HID_DIM = 512
N_LAYERS = 2
ENC_DROPOUT = 0.5
DEC_DROPOUT = 0.5
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)
dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT)
model = Seq2Seq(enc, dec, device).to(device)

optimizer = optim.Adam(model.parameters())
criterion = nn.CrossEntropyLoss()


def train(model, iterator, optimizer, criterion, clip):
    model.train()
    epoch_loss = 0
    count = 0
    for i, (src, trg) in enumerate(iterator):
        optimizer.zero_grad()
        output = model(src, trg)
        output_dim = output.shape[-1]
        output = output[1:].view(-1, output_dim)
        trg = trg[1:].view(-1)
        loss = criterion(output, trg)
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)
        optimizer.step()
        epoch_loss += loss.item()
        count = i
    return epoch_loss / (count + 1)


def main():
    src = torch.randint(0, INPUT_DIM, (10, 32)).to(device)  # sequence length 10, batch szie 32
    trg = torch.randint(0, OUTPUT_DIM, (10, 32)).to(device)
    dataset = [(src, trg) for _ in range(100)]  # 100 batch
    iterator = iter(dataset)

    N_EPOCHS = 10
    CLIP = 1
    for epoch in range(N_EPOCHS):
        train_loss = train(model, iterator, optimizer, criterion, CLIP)
        print(f'Epoch: {epoch + 1:02} | Train Loss: {train_loss:.3f}')


def evaluate(model, iterator, criterion):
    model.eval()
    epoch_loss = 0
    count = 0
    with torch.no_grad():
        for i, (src, trg) in enumerate(iterator):
            output = model(src, trg, 0)  # 0 means not using teacher forcing
            output_dim = output.shape[-1]
            output = output[1:].view(-1, output_dim)
            trg = trg[1:].view(-1)
            loss = criterion(output, trg)
            epoch_loss += loss.item()
            count = i
    return epoch_loss / (count + 1)


def main_test():
    src_test = torch.randint(0, INPUT_DIM, (10, 32)).to(device)  # sequence_length 10,  batchsize 32
    trg_test = torch.randint(0, OUTPUT_DIM, (10, 32)).to(device)
    test_dataset = [(src_test, trg_test) for _ in range(50)]  # 50 batch
    test_iterator = iter(test_dataset)

    test_loss = evaluate(model, test_iterator, criterion)
    print(f'Test Loss: {test_loss:.3f}')


if __name__ == "__main__":
    main()
    main_test()

```

---

## [Transformer](https://d2l.ai/chapter_attention-mechanisms-and-transformers/index.html)

> The **Transformer**, **BERT**, and **GPT** architectures do not use **RNNs**. Instead, they rely on the **self-attention** mechanism to process sequences.  *— ChatGPT 4*
{: .prompt-info }

### Queries, Keys, and Values

- A data set $\mathcal{D}:=\{ (k_i, v_i) \mid i\in \{1, \ldots, n\} \}$.
  - $k$ is the key, $v$ is the value.
  - It is a dictionary (in python).
- We input the query $q$ to search the data set.
- The program returns the value most relevant $v_{i^\*}$, where $i^\* = \arg\min_{i} \Vert q - x_i \Vert$.

```python
keys = range(1, 8, 3)  # [1, 4, 7]
values = ["xixi", "haha", "wuwu"]

data_set = dict(zip(keys, values))
# {1: 'xixi', 4: 'haha', 7: 'wuwu'}

def search(query: int):
    distances = [abs(query - key_i) for key_i in keys]
    idx_optimal = distances.index(min(distances))
    key_optimal = keys[idx_optimal]
    value_optimal = data_set[key_optimal]
    return value_optimal

print(search(query=3))  # haha
```


### Attention
> Attention is all you need.

$$
\mathrm{Attention}(q, \mathcal{D}) := \sum\limits_{i=1}^n v_i \cdot \alpha(q, k_i)
$$

- $\alpha(q, k_i)$ is usually a function of the distance between $q$ and $k_i$, reflecting their similarity.
- $\boldsymbol\alpha = (\alpha(q, k_1), \ldots, \alpha(q, k_n))$ should be a convex combination.
  - $\alpha(q, k_i) \ge 0, \forall i$
  - $\sum\limits_{i=1}^n \alpha(q, k_i) = 1$
- If $\boldsymbol\alpha$ is one-hot, then the attention mechanism is just like the traditional database query.


![Attention](https://d2l.ai/_images/qkv.svg){: width="500" height="500" }
_Illustration of the attention mechanism from the book "Dive into Deep Learning"._

### Common similarity kernels

$$
\boldsymbol\alpha(q, \boldsymbol{k}) = \mathrm{softmax}(\textcolor{blue}{(}f(\Vert q - k_1 \Vert), \ldots, f(\Vert q - k_n \Vert)\textcolor{blue}{)})
$$

$f$ is the similarity kernel (or Parzen Windows).
- $f(\Vert q - k \Vert) = \exp\left(-\frac{1}{2}\Vert q-k \Vert^2\right)$ (Gaussian)
- $f(\Vert q - k \Vert) = 1 \mathrm{if} \Vert q-k \Vert \le 1$ (Boxcar)
- $f(\Vert q - k \Vert) = \max(0, 1- \Vert q-k \Vert )$ (Epanechikov)

> The following part has not been finished yet. One may check my [writing schedule](https://yuelin301.github.io/posts/Schedule/).
{: .prompt-warning }

## BERT
BERT = Bidirectional Encoder Representations from Transformers

## GPT

GPT = Generative Pre-trained Transformer
