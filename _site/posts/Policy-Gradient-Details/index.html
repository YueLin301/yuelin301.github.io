<!doctype html>














<!-- `site.alt_lang` can specify a language different from the UI -->
<html lang="en" >
  <!-- The Head -->

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta name="theme-color" media="(prefers-color-scheme: light)" content="#f7f7f7">
  <meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1b1b1e">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta
    name="viewport"
    content="width=device-width, user-scalable=no initial-scale=1, shrink-to-fit=no, viewport-fit=cover"
  >

  

  

  
    <!-- Begin Jekyll SEO tag v2.8.0 -->
<meta name="generator" content="Jekyll v4.3.2" />
<meta property="og:title" content="Details on the Analysis of Policy Gradient Methods" />
<meta property="og:locale" content="en" />
<meta name="description" content="The only way to make sense out of change is to plunge into it, move with it, and join the dance. — Alan Watts." />
<meta property="og:description" content="The only way to make sense out of change is to plunge into it, move with it, and join the dance. — Alan Watts." />
<link rel="canonical" href="http://localhost:4000/posts/Policy-Gradient-Details/" />
<meta property="og:url" content="http://localhost:4000/posts/Policy-Gradient-Details/" />
<meta property="og:site_name" content="Yue Lin" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2023-07-24T18:40:00+00:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Details on the Analysis of Policy Gradient Methods" />
<meta name="google-site-verification" content="d2Lg5eCqHini4wHtZ-c84NNCK97UHcbAhQzVJHekMZQ" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2023-12-19T13:49:18+00:00","datePublished":"2023-07-24T18:40:00+00:00","description":"The only way to make sense out of change is to plunge into it, move with it, and join the dance. — Alan Watts.","headline":"Details on the Analysis of Policy Gradient Methods","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/posts/Policy-Gradient-Details/"},"url":"http://localhost:4000/posts/Policy-Gradient-Details/"}</script>
<!-- End Jekyll SEO tag -->

  

  <title>Details on the Analysis of Policy Gradient Methods | Yue Lin
  </title>

  <!--
  The Favicons for Web, Android, Microsoft, and iOS (iPhone and iPad) Apps
  Generated by: https://realfavicongenerator.net/
-->



<link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png">
<link rel="manifest" href="/assets/img/favicons/site.webmanifest">
<link rel="shortcut icon" href="/assets/img/favicons/favicon.ico">
<meta name="apple-mobile-web-app-title" content="Yue Lin">
<meta name="application-name" content="Yue Lin">
<meta name="msapplication-TileColor" content="#da532c">
<meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml">
<meta name="theme-color" content="#ffffff">


  
    
      <link rel="preconnect" href="https://fonts.googleapis.com" >
      <link rel="dns-prefetch" href="https://fonts.googleapis.com" >
    
      <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
      <link rel="dns-prefetch" href="https://fonts.gstatic.com" crossorigin>
    
      <link rel="preconnect" href="https://fonts.googleapis.com" >
      <link rel="dns-prefetch" href="https://fonts.googleapis.com" >
    
      <link rel="preconnect" href="https://cdn.jsdelivr.net" >
      <link rel="dns-prefetch" href="https://cdn.jsdelivr.net" >
    

    <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Lato&family=Source+Sans+Pro:wght@400;600;700;900&display=swap">
  

  <!-- GA -->
  

  <!-- Bootstrap -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.2/dist/css/bootstrap.min.css">

  <!-- Font Awesome -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.2/css/all.min.css">

  <link rel="stylesheet" href="/assets/css/jekyll-theme-chirpy.css">

  
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/tocbot@4.21.1/dist/tocbot.min.css">
  

  
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/loading-attribute-polyfill@2.1.1/dist/loading-attribute-polyfill.min.css">
  

  
    <!-- Manific Popup -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1.1.0/dist/magnific-popup.min.css">
  

  <!-- JavaScript -->

  
    <!-- Switch the mode between dark and light. -->

<script type="text/javascript">
  class ModeToggle {
    static get MODE_KEY() {
      return 'mode';
    }
    static get MODE_ATTR() {
      return 'data-mode';
    }
    static get DARK_MODE() {
      return 'dark';
    }
    static get LIGHT_MODE() {
      return 'light';
    }
    static get ID() {
      return 'mode-toggle';
    }

    constructor() {
      if (this.hasMode) {
        if (this.isDarkMode) {
          if (!this.isSysDarkPrefer) {
            this.setDark();
          }
        } else {
          if (this.isSysDarkPrefer) {
            this.setLight();
          }
        }
      }

      let self = this;

      /* always follow the system prefers */
      this.sysDarkPrefers.addEventListener('change', () => {
        if (self.hasMode) {
          if (self.isDarkMode) {
            if (!self.isSysDarkPrefer) {
              self.setDark();
            }
          } else {
            if (self.isSysDarkPrefer) {
              self.setLight();
            }
          }

          self.clearMode();
        }

        self.notify();
      });
    } /* constructor() */

    get sysDarkPrefers() {
      return window.matchMedia('(prefers-color-scheme: dark)');
    }

    get isSysDarkPrefer() {
      return this.sysDarkPrefers.matches;
    }

    get isDarkMode() {
      return this.mode === ModeToggle.DARK_MODE;
    }

    get isLightMode() {
      return this.mode === ModeToggle.LIGHT_MODE;
    }

    get hasMode() {
      return this.mode != null;
    }

    get mode() {
      return sessionStorage.getItem(ModeToggle.MODE_KEY);
    }

    /* get the current mode on screen */
    get modeStatus() {
      if (this.isDarkMode || (!this.hasMode && this.isSysDarkPrefer)) {
        return ModeToggle.DARK_MODE;
      } else {
        return ModeToggle.LIGHT_MODE;
      }
    }

    setDark() {
      document.documentElement.setAttribute(ModeToggle.MODE_ATTR, ModeToggle.DARK_MODE);
      sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE);
    }

    setLight() {
      document.documentElement.setAttribute(ModeToggle.MODE_ATTR, ModeToggle.LIGHT_MODE);
      sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE);
    }

    clearMode() {
      document.documentElement.removeAttribute(ModeToggle.MODE_ATTR);
      sessionStorage.removeItem(ModeToggle.MODE_KEY);
    }

    /* Notify another plugins that the theme mode has changed */
    notify() {
      window.postMessage(
        {
          direction: ModeToggle.ID,
          message: this.modeStatus
        },
        '*'
      );
    }

    flipMode() {
      if (this.hasMode) {
        if (this.isSysDarkPrefer) {
          if (this.isLightMode) {
            this.clearMode();
          } else {
            this.setLight();
          }
        } else {
          if (this.isDarkMode) {
            this.clearMode();
          } else {
            this.setDark();
          }
        }
      } else {
        if (this.isSysDarkPrefer) {
          this.setLight();
        } else {
          this.setDark();
        }
      }

      this.notify();
    } /* flipMode() */
  } /* ModeToggle */

  const modeToggle = new ModeToggle();
</script>

  

  <!-- A placeholder to allow defining custom metadata -->

</head>


  <body>
    <!-- The Side Bar -->

<aside aria-label="Sidebar" id="sidebar" class="d-flex flex-column align-items-end">
  <header class="profile-wrapper">
    <a href="/" id="avatar" class="rounded-circle">
      
        
        <img src="/assets/img/meow.png" width="112" height="112" alt="avatar" onerror="this.style.display='none'">
      
    </a>

    <h1 class="site-title">
      <a href="/">Yue Lin</a>
    </h1>
    <p class="site-subtitle fst-italic mb-0">Redemption lies within.</p>
  </header>
  <!-- .profile-wrapper -->

  <nav class="flex-column flex-grow-1 w-100 ps-0">
    <ul class="nav">
      <!-- home -->
      <!-- <li class="nav-item">
        <a href="/" class="nav-link">
          <i class="fa-fw fas fa-home"></i>
          <span>HOME</span>
        </a>
      </li> -->
      <!-- the real tabs -->
      
        <li class="nav-item">
          <a href="/categories/" class="nav-link">
            <i class="fa-fw fa-solid fa-chess"></i>
            

            <span>CATEGORIES</span>
          </a>
        </li>
        <!-- .nav-item -->
      
        <li class="nav-item">
          <a href="/archives/" class="nav-link">
            <i class="fa-fw fa-solid fa-scroll"></i>
            

            <span>ARCHIVES</span>
          </a>
        </li>
        <!-- .nav-item -->
      
        <li class="nav-item">
          <a href="/tech/" class="nav-link">
            <i class="fa-fw fa-solid fa-wand-sparkles"></i>
            

            <span>TECH</span>
          </a>
        </li>
        <!-- .nav-item -->
      
        <li class="nav-item">
          <a href="/life/" class="nav-link">
            <i class="fa-fw fa-solid fa-broom"></i>
            

            <span>LIFE</span>
          </a>
        </li>
        <!-- .nav-item -->
      
        <li class="nav-item">
          <a href="/tags/" class="nav-link">
            <i class="fa-fw fa-solid fa-puzzle-piece"></i>
            

            <span>TAGS</span>
          </a>
        </li>
        <!-- .nav-item -->
      
        <li class="nav-item">
          <a href="/about/" class="nav-link">
            <i class="fa-fw fa-solid fa-hat-wizard"></i>
            

            <span>ABOUT</span>
          </a>
        </li>
        <!-- .nav-item -->
      
    </ul>
  </nav>

  <div class="sidebar-bottom d-flex flex-wrap  align-items-center w-100">
    
      <button type="button" class="mode-toggle btn" aria-label="Switch Mode">
        <i class="fas fa-adjust"></i>
      </button>

      
        <span class="icon-border"></span>
      
    

    
      

      
        <a
          href="https://github.com/YueLin301"
          aria-label="github"
          

          
            target="_blank"
            
          

          

          
            rel="noopener noreferrer"
          
        >
          <i class="fab fa-github"></i>
        </a>
      
    
      

      
        <a
          href="https://scholar.google.com/citations?user=fbvQHX4AAAAJ&hl"
          aria-label="google scholar"
          

          
            target="_blank"
            
          

          

          
            rel="noopener noreferrer"
          
        >
          <i class="fas fa-graduation-cap"></i>
        </a>
      
    
      

      
        <a
          href="javascript:location.href = 'mailto:' + ['linyue3h1','gmail.com'].join('@')"
          aria-label="email"
          

          

          

          
        >
          <i class="fas fa-envelope"></i>
        </a>
      
    
  </div>
  <!-- .sidebar-bottom -->
</aside>
<!-- #sidebar -->


    <div id="main-wrapper" class="d-flex justify-content-center">
      <div class="container d-flex flex-column px-xxl-5">
        <!-- The Top Bar -->

<header id="topbar-wrapper" aria-label="Top Bar">
  <div
    id="topbar"
    class="d-flex align-items-center justify-content-between px-lg-3 h-100"
  >
    <nav id="breadcrumb" aria-label="Breadcrumb">
      

      
        
          
            <span>
              <a href="/">
                Home
              </a>
            </span>

          
        
          
        
          
            
              <span>Details on the Analysis of Policy Gradient Methods</span>
            

          
        
      
    </nav>
    <!-- endof #breadcrumb -->

    <button type="button" id="sidebar-trigger" class="btn btn-link">
      <i class="fas fa-bars fa-fw"></i>
    </button>

    <div id="topbar-title">
      Post
    </div>

    <button type="button" id="search-trigger" class="btn btn-link">
      <i class="fas fa-search fa-fw"></i>
    </button>

    <search class="align-items-center ms-3 ms-lg-0">
      <i class="fas fa-search fa-fw"></i>
      <input
        class="form-control"
        id="search-input"
        type="search"
        aria-label="search"
        autocomplete="off"
        placeholder="Search..."
      >
    </search>
    <button type="button" class="btn btn-link text-decoration-none" id="search-cancel">Cancel</button>
  </div>
</header>


        <div class="row flex-grow-1">
          <!-- <main aria-label="Main Content" class="col-12 col-lg-11 col-xl-9 px-md-4"> -->
            <main aria-label="Main Content" class="col-12 col-sm-9 col-lg-9 col-xl-9 px-md-4">
            
              <!-- Refactor the HTML structure -->



<!--
  In order to allow a wide table to scroll horizontally,
  we suround the markdown table with `<div class="table-wrapper">` and `</div>`
-->



<!--
  Fixed kramdown code highlight rendering:
  https://github.com/penibelst/jekyll-compress-html/issues/101
  https://github.com/penibelst/jekyll-compress-html/issues/71#issuecomment-188144901
-->



<!-- Change the icon of checkbox -->



<!-- Handle images -->





<!-- Add header for code snippets -->



<!-- Create heading anchors -->





  
  

  
    
    

    
      
        
        
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    

    
  

  
  

  
    
    

    
      
        
        
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    

    
  

  
  

  
    
    

    
      
        
        
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    

    
  

  
  

  




<!-- return -->




<article class="px-1">
  <header>
    <h1 data-toc-skip>Details on the Analysis of Policy Gradient Methods</h1>

    <div class="post-meta text-muted">
      <!-- published date -->
      <span>
        Posted
        <!--
  Date format snippet
  See: ${JS_ROOT}/utils/locale-dateime.js
-->




<time
  
  data-ts="1690224000"
  data-df="ll"
  
    data-bs-toggle="tooltip" data-bs-placement="bottom"
  
>
  Jul 24, 2023
</time>

      </span>

      <!-- lastmod date -->
      
        <span>
          Updated
          <!--
  Date format snippet
  See: ${JS_ROOT}/utils/locale-dateime.js
-->




<time
  
  data-ts="1702993758"
  data-df="ll"
  
    data-bs-toggle="tooltip" data-bs-placement="bottom"
  
>
  Dec 19, 2023
</time>

        </span>
      

      

      <div class="d-flex justify-content-between">
        <!-- author(s) -->
        <span>
          

          By

          <em>
            
              <a href="https://github.com/YueLin301">Yue Lin</a>
            
          </em>
        </span>

        <!-- read time -->
        <!-- Calculate the post's reading time, and display the word count in tooltip -->



<!-- words per minute -->










<!-- return element -->
<span
  class="readtime"
  data-bs-toggle="tooltip"
  data-bs-placement="bottom"
  title="2142 words"
>
  <em>11 min</em> read</span>

      </div>
      <!-- .d-flex -->
    </div>
    <!-- .post-meta -->
  </header>

  <div class="content">
    <blockquote>
  <p>The only way to make sense out of change is to plunge into it, move with it, and join the dance. <em>— Alan Watts.</em></p>
</blockquote>

<hr />

<h2 id="policy-gradient-theorem"><span class="me-2">Policy Gradient Theorem</span><a href="#policy-gradient-theorem" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>

<blockquote class="prompt-info">
  <p>The proofs of the stochastic and deterministic policy gradient theorem are mainly summarized from <a href="https://lilianweng.github.io/posts/2018-04-08-policy-gradient/#off-policy-policy-gradient">this blog</a> and the supplementary of the paper “<a href="https://scholar.archive.org/work/v7bb4lgn2zhnta3bassfawrane/access/wayback/https://hal.inria.fr/hal-00938992/file/dpg-icml2014.pdf">Deterministic Policy Gradient Algorithms</a>,” respectively.</p>
</blockquote>

<h3 id="stochastic-policy-gradient"><span class="me-2">Stochastic Policy Gradient</span><a href="#stochastic-policy-gradient" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<h4 id="objectitve"><span class="me-2">Objectitve</span><a href="#objectitve" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4>

\[\max\limits_{\theta} J(\theta) = \mathbb{E}_{s_0\sim d_0}\left[ \textcolor{red}{V^{\pi_\theta} (s_0)} \right].\]

<h4 id="gradient"><span class="me-2">Gradient</span><a href="#gradient" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4>

\[\begin{aligned}
\nabla_\theta V^{\pi_\theta}(s_0) 
=&amp; \frac{1}{(1-\gamma)} \sum\limits_{s} d^{\pi_\theta}(s\mid s_0) \sum\limits_{a} Q^{\pi_\theta}(s,a) \cdot \nabla_\theta \pi_\theta(a\mid s) \\
=&amp; \frac{1}{(1-\gamma)} \sum\limits_{s} d^{\pi_\theta}(s\mid s_0) \sum\limits_{a} \pi(a\mid s) \cdot Q^{\pi_\theta}(s,a) \cdot \nabla_\theta \ln \pi_\theta(a\mid s) \\
=&amp; \frac{1}{(1-\gamma)} \mathbb{E}_{s \sim d^{\pi_\theta}(\cdot \mid s_0)} \mathbb{E}_{a\sim \pi(\cdot \mid s)} \left[Q^{\pi_\theta}(s,a) \cdot \nabla_\theta \ln \pi_\theta(a\mid s)\right] 
\end{aligned}\]

<h4 id="proof"><span class="me-2">Proof</span><a href="#proof" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4>

\[\begin{aligned}
\nabla_\theta V^\pi(s) =&amp; \nabla_\theta \sum\limits_{a} \pi(a\mid s) \cdot Q^\pi(s,a) \\
=&amp; \sum\limits_{a} \left[Q^\pi(s,a) \cdot \nabla_\theta\pi(a\mid s) + \pi(a\mid s) \cdot \nabla_\theta Q^\pi(s,a)\right] \\
=&amp; \sum\limits_{a} \left[Q^\pi(s,a) \cdot \nabla_\theta\pi(a\mid s) + \pi(a\mid s) \cdot \nabla_\theta \sum\limits_{s',r} P(s',r\mid s,a)\cdot \left(r+ \gamma \cdot V^\pi(s')\right)\right] \\
=&amp; \sum\limits_{a} \left[Q^\pi(s,a) \cdot \nabla_\theta\pi(a\mid s) + \gamma \cdot  \pi(a\mid s) \cdot \sum\limits_{s'} P(s'\mid s,a)\cdot \nabla_\theta V^\pi(s')\right] \\
=&amp; \sum\limits_{a} Q^\pi(s,a) \cdot \nabla_\theta\pi(a\mid s) + \gamma \sum\limits_{s',a} \pi(a\mid s) \cdot  P(s'\mid s,a)\cdot \nabla_\theta V^\pi(s') \\
\end{aligned}\]

<p>Note that the action $a$ is sampled from the parameterized policy $\pi_\theta.$ Thus $\nabla_\theta a$ is $0,$ without using the gumbel-softmax technique. In the deterministic policy, $\nabla_\theta a$ is not $0,$ and thus the derivation is different.</p>

\[\begin{aligned}
\nabla_\theta V^\pi(s) 
=&amp; \sum\limits_{a} Q^\pi(s,a) \cdot \nabla_\theta\pi(a\mid s) + \sum\limits_{s',\textcolor{red}{a}} \textcolor{red}{\gamma \cdot \pi(a\mid s) \cdot  P(s'\mid s,a)} \cdot \nabla_\theta V^\pi(s') \\
=&amp; \sum\limits_{a} Q^\pi(s,a) \cdot \nabla_\theta\pi(a\mid s) + \sum\limits_{s'} \textcolor{red}{\gamma \cdot \mathrm{Pr}(s\to s', k=1, \pi_\theta)} \cdot \nabla_\theta V^\pi(s') \\
=&amp; \sum\limits_{a} Q^\pi(s,a) \cdot \nabla_\theta\pi(a\mid s) + \sum\limits_{s'} \gamma \cdot \mathrm{Pr}(s\to s', k=1, \pi_\theta) \left[\sum\limits_{a} Q^\pi(s',a) \cdot \nabla_\theta\pi(a\mid s') + \sum\limits_{s''} \gamma \cdot \mathrm{Pr}(s'\to s'', k=1, \pi_\theta)\cdot \nabla_\theta V^\pi(s'')\right] \\
=&amp; \sum\limits_{a} Q^\pi(s,a) \cdot \nabla_\theta\pi(a\mid s) + \gamma \sum\limits_{s'} \mathrm{Pr}(s\to s', k=1, \pi_\theta) \sum\limits_{a} Q^\pi(s',a) \cdot \nabla_\theta\pi(a\mid s') \\
	&amp;+ \textcolor{red}{\gamma^2 \sum\limits_{s'} \mathrm{Pr}(s\to s', k=1, \pi_\theta) \sum\limits_{s''} \mathrm{Pr}(s'\to s'', k=1, \pi_\theta)}\cdot \nabla_\theta V^\pi(s'') \\
=&amp; \sum\limits_{a} Q^\pi(s,a) \cdot \nabla_\theta\pi(a\mid s) + \gamma \sum\limits_{s'} \mathrm{Pr}(s\to s', k=1) \sum\limits_{a} Q^\pi(s',a) \cdot \nabla_\theta\pi(a\mid s') \\
	&amp;+ \textcolor{red}{\gamma^2 \sum\limits_{s'} \mathrm{Pr}(s\to s'', k=2, \pi_\theta) }\cdot \nabla_\theta V^\pi(s'') \\
=&amp;\ldots \\
=&amp; \sum\limits_{x\in S} \textcolor{blue}{\sum\limits_{k=0}^\infty \gamma^k \cdot \mathrm{Pr}(s\to x, k, \pi_\theta)} \cdot \sum\limits_{a} Q^\pi(x,a) \cdot \nabla_\theta\pi(a\mid x)
\end{aligned}\]

<p>The blue part is defined as the <strong>discounted state visitation distribution</strong> \(d^{\pi_\theta}(s\mid s_0) = (1-\gamma )\cdot \sum\limits_{k=0}^\infty \gamma^k \cdot \mathrm{Pr}(s_0\to s, k, \pi_\theta).\)</p>

\[\begin{aligned}
\sum\limits_{k=0}^\infty \gamma^k \cdot \mathrm{Pr}(s_0\to s, k, \pi_\theta) \le \sum\limits_{k=0}^\infty \gamma^k = \frac{1}{1-\gamma}
\end{aligned}\]

<p>The distribution should beshould lie within the range of $[0,1]$ and thus the coefficient $(1-\gamma)$ is is for normalization.</p>

\[\begin{aligned}
\nabla_\theta V^{\pi_\theta}(s_0) 
=&amp; \textcolor{blue}{\frac{1}{(1-\gamma)}} \sum\limits_{s} \textcolor{blue}{d^{\pi_\theta}(s\mid s_0)} \sum\limits_{a} Q^{\pi_\theta}(s,a) \cdot \nabla_\theta \pi_\theta(a\mid s) \\
=&amp; \frac{1}{(1-\gamma)} \sum\limits_{s} d^{\pi_\theta}(s\mid s_0) \sum\limits_{a} \pi(a\mid s) \cdot Q^{\pi_\theta}(s,a) \cdot \nabla_\theta \ln \pi_\theta(a\mid s) \\
=&amp; \frac{1}{(1-\gamma)} \mathbb{E}_{s \sim d^{\pi_\theta}(\cdot \mid s_0)} \mathbb{E}_{a\sim \pi(\cdot \mid s)} \left[Q^{\pi_\theta}(s,a) \cdot \nabla_\theta \ln \pi_\theta(a\mid s)\right] 
&amp; \blacksquare
\end{aligned}\]

<h3 id="deterministic-policy-gradient"><span class="me-2">Deterministic Policy Gradient</span><a href="#deterministic-policy-gradient" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<h4 id="basics"><span class="me-2">Basics</span><a href="#basics" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4>

<ul>
  <li>$a = \mu_\theta (s)$</li>
  <li>$V^{\mu_\theta}(s) = Q^{\mu_\theta}(s,a) = Q^{\mu_\theta}(s,\mu_\theta(s))$</li>
  <li>$\nabla_\theta a \ne 0$
    <ul>
      <li>$\nabla_\theta r(s,a)\ne 0$</li>
      <li>$\nabla_\theta P(s’\mid s,a)\ne 0$</li>
    </ul>
  </li>
  <li>$P(s’\mid s,\mu_\theta(s)) = \mathrm{Pr}(s\to s’, k=1, \mu_\theta)$</li>
</ul>

<h4 id="gradient-1"><span class="me-2">Gradient</span><a href="#gradient-1" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4>

\[\begin{aligned}
\nabla_\theta V^{\mu_\theta}(s) 
=&amp; \frac{1}{(1-\gamma)}\int_{x\in S} d^{\mu_\theta}(s) \cdot \nabla_\theta \mu_\theta(x)\cdot \nabla_a Q^{\mu_\theta} (x, \mu_\theta(x))\Big\vert_{a=\mu_\theta(x)} \,\mathrm{d} x \\
=&amp;\frac{1}{(1-\gamma)} \mathbb{E}_{s\sim d^{\mu_\theta}} \bigg[ \nabla_\theta \mu_\theta(x)\cdot \nabla_a Q^{\mu_\theta} (x, \mu_\theta(x))\Big\vert_{a=\mu_\theta(x)}\bigg]
\end{aligned}\]

<h4 id="proof-1"><span class="me-2">Proof</span><a href="#proof-1" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4>

\[\begin{aligned}
\nabla_\theta V^{\mu_\theta}(s) 
=&amp; \textcolor{blue}{\nabla_\theta Q^{\mu_\theta} (s, \mu_\theta(s))} \\
=&amp; \nabla_\theta \left( r(s,\mu_\theta(s)) + \gamma \int_S P\left(s'\mid s,\mu_\theta(s)\right)\cdot V^{\mu_\theta}(s') \,\mathrm{d} s' \right) \\
=&amp; \textcolor{red}{\nabla_\theta r(s,\mu_\theta(s))} + \gamma \int_S V^{\mu_\theta}(s') \cdot \textcolor{red}{\nabla_\theta P\left(s'\mid s,\mu_\theta(s)\right)} \,\mathrm{d} s' + \gamma \int_S P\left(s'\mid s,\mu_\theta(s)\right)\cdot \nabla_\theta V^{\mu_\theta}(s') \,\mathrm{d} s' \\
=&amp; \nabla_\theta \mu_\theta(s)\cdot \textcolor{blue}{\nabla_a} \left( r(s,\mu_\theta(s)) + \gamma \int_S \textcolor{blue}{V^{\mu_\theta}(s')} \cdot P\left(s'\mid s,\mu_\theta(s)\right) \,\mathrm{d} s' \right)\Bigg\vert_{a=\mu_\theta(s)} + \gamma \int_S P\left(s'\mid s,\mu_\theta(s)\right)\cdot \nabla_\theta V^{\mu_\theta}(s') \,\mathrm{d} s' \\
=&amp; \textcolor{blue}{\nabla_\theta \mu_\theta(s)\cdot \nabla_a Q^{\mu_\theta} (s, \mu_\theta(s))\Big\vert_{a=\mu_\theta(s)}} + \gamma \int_S \textcolor{green}{P\left(s'\mid s,\mu_\theta(s)\right)}\cdot \nabla_\theta V^{\mu_\theta}(s') \,\mathrm{d} s' \\
=&amp; \nabla_\theta \mu_\theta(s)\cdot \nabla_a Q^{\mu_\theta} (s, \mu_\theta(s))\Big\vert_{a=\mu_\theta(s)} + \gamma \int_S \textcolor{green}{\mathrm{Pr}(s\to s', k=1, \mu_\theta)} \cdot \nabla_\theta V^{\mu_\theta}(s') \,\mathrm{d} s' \\
=&amp; \nabla_\theta \mu_\theta(s)\cdot \nabla_a Q^{\mu_\theta} (s, \mu_\theta(s))\Big\vert_{a=\mu_\theta(s)} + \gamma \int_S \mathrm{Pr}(s\to s', k=1, \mu_\theta) \cdot \left( \nabla_\theta \mu_\theta(s')\cdot \nabla_a Q^{\mu_\theta} (s', \mu_\theta(s'))\Big\vert_{a=\mu_\theta(s')} + \gamma \int_S \mathrm{Pr}(s'\to s'', k=1, \mu_\theta) \cdot \nabla_\theta V^{\mu_\theta}(s'') \,\mathrm{d} s'' \right) \,\mathrm{d} s' \\
=&amp; \nabla_\theta \mu_\theta(s)\cdot \nabla_a Q^{\mu_\theta} (s, \mu_\theta(s))\Big\vert_{a=\mu_\theta(s)} + \gamma \int_S \mathrm{Pr}(s\to s', k=1, \mu_\theta) \cdot \nabla_\theta \mu_\theta(s')\cdot \nabla_a Q^{\mu_\theta} (s', \mu_\theta(s'))\Big\vert_{a=\mu_\theta(s')} \,\mathrm{d} s' \\
	&amp;+ \gamma^2 \int_S \mathrm{Pr}(s\to s', k=1, \mu_\theta) \int_S \mathrm{Pr}(s'\to s'', k=1, \mu_\theta) \cdot \nabla_\theta V^{\mu_\theta}(s'') \,\mathrm{d} s'' \,\mathrm{d} s' \\
=&amp;\ldots \\
=&amp; \int_{x\in S} \sum\limits_{k=0}^\infty \gamma^k \cdot \mathrm{Pr}(s\to x, k, \mu_\theta) \cdot \nabla_\theta \mu_\theta(x)\cdot \nabla_a Q^{\mu_\theta} (x, \mu_\theta(x))\Big\vert_{a=\mu_\theta(x)} \,\mathrm{d} x \\
=&amp; \frac{1}{(1-\gamma)}\int_{x\in S} d^{\mu_\theta}(s) \cdot \nabla_\theta \mu_\theta(x)\cdot \nabla_a Q^{\mu_\theta} (x, \mu_\theta(x))\Big\vert_{a=\mu_\theta(x)} \,\mathrm{d} x \\
=&amp;\frac{1}{(1-\gamma)} \mathbb{E}_{s\sim d^{\mu_\theta}} \bigg[ \nabla_\theta \mu_\theta(x)\cdot \nabla_a Q^{\mu_\theta} (x, \mu_\theta(x))\Big\vert_{a=\mu_\theta(x)}\bigg]
\end{aligned}\]

<p>Note that the Bellman equation here is different from the one in the stochastic case: the reward is not dependent on the next state. $\blacksquare$</p>

<blockquote class="prompt-tip">
  <p>Calculating $\nabla_a Q(s,a)$ is the result of accounting for both $\nabla_\theta r(s,a)$ and $\nabla_\theta p(s’\mid s,a)$.</p>
</blockquote>

<h2 id="performance-difference-lemma"><span class="me-2">Performance Difference Lemma</span><a href="#performance-difference-lemma" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>

<p>For all policies $\pi, \pi^\prime$ and states
$s_0$,</p>

\[\begin{aligned} V^\pi(s_0) - V^{\pi^\prime}(s_0) =&amp; \mathbb{E}_{\tau \sim {\Pr}^\pi(\tau|s_0=s) } \left[\sum_{t=0}^\infty \gamma^t A^{\pi'}(s_t,a_t)\right] \\ =&amp; \frac{1}{1-\gamma}\mathbb{E}_{s\sim d_{s_0}^\pi }\mathbb{E}_{a\sim \pi(\cdot|s) } \left[  A^{\pi^\prime}(s,a)\right]. \end{aligned}\]

<blockquote class="prompt-info">
  <p>Kakade, Sham, and John Langford. “Approximately optimal approximate reinforcement learning.” Proceedings of the Nineteenth International Conference on Machine Learning. 2002.</p>
</blockquote>

<h3 id="proof-2"><span class="me-2">Proof</span><a href="#proof-2" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<p>The proof is provided in the appendix of “On the theory of policy gradient methods: Optimality, approximation, and distribution shift” and I just transcribed it here with additional details.</p>

<p>Let $\Pr^\pi(\tau \mid s_0 = s)$ denote the probability of observing a trajectory $\tau$ when starting in state $s$ and following the policy $\pi$. Using a telescoping argument, we have:</p>

\[\begin{aligned}
&amp;V^\pi(s) - V^{\pi'}(s) \\
=&amp;  \mathbb{E}_{\tau \sim {\Pr}^\pi(\tau|s_0=s) }
\left[\sum_{t=0}^\infty \gamma^t r(s_t,a_t)\right] - V^{\pi'}(s) \\
=&amp; \mathbb{E}_{\tau \sim {\Pr}^\pi(\tau|s_0=s) }
\left[\sum_{t=0}^\infty \gamma^t \left(r(s_t,a_t)+V^{\pi'}(s_t)-V^{\pi'}(s_t) \right)\right]-V^{\pi'}(s)\\
\stackrel{(a)}{=}&amp; \mathbb{E}_{\tau \sim {\Pr}^\pi(\tau|s_0=s) }
    \left[\sum_{t=0}^\infty \gamma^t \left(r(s_t,a_t)+\gamma V^{\pi'}(s_{t+1})-V^{\pi'}(s_t)\right)\right]\\
\stackrel{(b)}{=}&amp;\mathbb{E}_{\tau \sim {\Pr}^\pi(\tau|s_0=s) }
    \left[\sum_{t=0}^\infty \gamma^t \left(r(s_t,a_t)+\gamma \mathbb{E}[V^{\pi'}(s_{t+1})|s_t,a_t]-V^{\pi'}(s_t)\right)\right]\\
\stackrel{(c)}{=}&amp; \mathbb{E}_{\tau \sim {\Pr}^\pi(\tau|s_0=s) }
    \left[\sum_{t=0}^\infty \gamma^t A^{\pi'}(s_t,a_t)\right] \\
=&amp; \frac{1}{1-\gamma}\mathbb{E}_{s'\sim d^\pi_s }\,\mathbb{E}_{a\sim \pi(\cdot | s')}
    \left[ A^{\pi'}(s',a) \right],
\end{aligned}\]

<p>where $(a)$ rearranges terms in the summation and cancels the $V^{\pi’}(s_0)$ term with the $-V^{\pi’}(s)$ outside the summation, and $(b)$ uses the tower property of conditional expectations and the final equality follows from the definition of $d^\pi_s$. $\blacksquare$</p>

<h3 id="details"><span class="me-2">Details</span><a href="#details" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<p>$(a)$:</p>

\[- a_0 +\sum\limits_{k=0}^{\infty} \left(a_k - b_k \right) = \sum\limits_{k=0}^{\infty} \left(a_{k+1} - b_k\right).\]

<p>$(b)$: The tower property of conditional expectations (or law of total probability):
If $\mathcal{H} \subseteq \mathcal{G}$, then</p>

\[\mathbb{E}\left[\mathbb{E}\left[X\mid \mathcal{G} \right] \mid \mathcal{H} \right] = \mathbb{E}\left[X\mid \mathcal{H} \right].\]

<p>Correspondingly,</p>
<ul>
  <li>$\mathcal{G} = \tau \sim {\Pr}^\pi(\tau \mid s_0=s)$,</li>
  <li>$\mathcal{H} = (s_t,a_t)$.</li>
</ul>

<p>$(c)$: Step $(b)$ is necessary. Note that</p>

\[Q^{\pi}(s, a) \ne r(s, a) + \gamma \cdot V^{\pi}(s').\]

<p>But</p>

\[Q^{\pi}(s, a) = r(s, a) + \gamma \cdot \sum\limits_{s'} P(s' \mid s,a) \cdot V^{\pi}(s').\]

<h3 id="other-proofs"><span class="me-2">Other proofs</span><a href="#other-proofs" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<blockquote class="prompt-info">
  <p>Check other proofs <a href="https://people.cs.umass.edu/~akshay/courses/coms6998-11/files/lec7.pdf">here</a> and <a href="https://wensun.github.io/CS4789_data/PDL.pdf">here</a>.</p>
</blockquote>

<p>###</p>

<h2 id="convergence"><span class="me-2">Convergence</span><a href="#convergence" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>

<h3 id="about"><span class="me-2">About</span><a href="#about" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<p>This section is based on the awesome paper:</p>

<blockquote>
  <p class="prompt-info">Agarwal, Alekh, et al. “On the theory of policy gradient methods: Optimality, approximation, and distribution shift.” The Journal of Machine Learning Research 22.1 (2021): 4431-4506.</p>
</blockquote>

<p>And I will provide some omitted details here. 
The writing of the entire note may be somewhat <strong>verbose</strong>, and this is to familiarize myself with the content.</p>

<h3 id="details-of-setting"><span class="me-2">Details of Setting</span><a href="#details-of-setting" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<h4 id="vs-le-frac11-gamma"><span class="me-2">$V(s) \le \frac{1}{1-\gamma}$</span><a href="#vs-le-frac11-gamma" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4>
<p>$V(s)$ reaches its upper bound when $r(s,a)=1,\forall s,a$, which equals $\sum\limits_{t=0}^\infty \gamma^t$.</p>

<p>And it is a geometric progression:</p>
<ul>
  <li>$a_n = a_0 \cdot \gamma^{n-1}$,</li>
  <li>$S_n = a_0 \cdot \frac{1-\gamma^n}{1-\gamma}$,
    <ul>
      <li>$S_n = a_0 + a_1 + \ldots + a_n$,</li>
      <li>$\gamma\cdot S_n = a_1 + \ldots + a_n + a_{n+1}$,</li>
      <li>$(1-\gamma)\cdot S_n = a_0\cdot (1 - \gamma^n)$.</li>
    </ul>
  </li>
  <li>$\lim\limits_{n\to\infty}S_n = \frac{a_0}{1-\gamma} = \frac{1}{1-\gamma}$.</li>
</ul>

<hr />

<h4 id="the-famous-theorem-of-bellman-and-dreyfus-1959"><span class="me-2">The famous theorem of Bellman and Dreyfus (1959)</span><a href="#the-famous-theorem-of-bellman-and-dreyfus-1959" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4>

<blockquote>
  <p>The famous theorem of Bellman and Dreyfus (1959) shows there exists a policy $\pi^\star$ which simultaneously maximizes $V^\pi(s_0)$, for all states $s_0\in S$.</p>
</blockquote>

<p>I have read this referenced paper, and I do not find any theorem. This paper is mainly about trading additional computing time for additional memory capacity.</p>

<p>However this statement is intuitive and is not hard to understand. Assume there is a fixed $s_{-1}$ and can be transited to $s_0$ according to $\rho$, then this problem is equivalent to the one that has a fixed $s_0$.</p>

<hr />

<h4 id="direct-parameterization"><span class="me-2">Direct parameterization</span><a href="#direct-parameterization" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4>

<p>$\theta\in\Delta(A)^{\vert S\vert}$ means for every state $s$ the parameters are  a point in a simplex.</p>

<p>For eample, for state $s_0$, there are actions $a_1, a_2$, the parameters of the current policy $\pi_\theta(\cdot \mid s_0)$ are</p>
<ul>
  <li>$\theta_{s_0,a_1} = 0.2 = \pi_\theta(a_1 \mid s_0)$, and</li>
  <li>$\theta_{s_0,a_2} = 0.8 = \pi_\theta(a_2 \mid s_0)$.</li>
</ul>

<hr />

<h4 id="softmax-parameterization"><span class="me-2">Softmax parameterization</span><a href="#softmax-parameterization" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4>

<p>Sometimes it can be $\pi_\theta(a\mid s) = \frac{\exp(\tau\cdot \theta_{s,a})}{\sum\limits_{a’}\exp(\tau\cdot \theta_{s,a’})}$, which is called energy-based policy, where $\tau$ is the temperature parameter (inverse temperature) and $\theta_{s,a}$ is the energy function.</p>

<blockquote class="prompt-info">
  <p>Haarnoja, Tuomas, et al. “Reinforcement learning with deep energy-based policies.” International conference on machine learning. PMLR, 2017.</p>
</blockquote>

<hr />

<h4 id="vpi_thetas-is-non-concave-lemma-1"><span class="me-2">$V^{\pi_\theta}(s)$ is non-concave (Lemma 1)</span><a href="#vpi_thetas-is-non-concave-lemma-1" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4>
<p>We want to <strong>maximize</strong> $V^{\pi_\theta}(s)$, so if $V^{\pi_\theta}(s)$ is <strong>concave</strong> then we can apply standard tools of convex optimization.  Unfortunately it is not.</p>

<p>As shown in the appendix, there is a MDP where exists policy points $\pi_1, \pi_2$ that $V^{\pi_1}(s)+V^{\pi_2}(s)&gt; 2\cdot V^{\frac{1}{2}(\pi_1+\pi_2)}(s)$. This shows a property of convex, so $V^{\pi_\theta}(s)$ is non-concave.</p>

<hr />

<h4 id="why-is-there-a-coefficient-1-gamma-in-4"><span class="me-2">Why is there a coefficient $(1-\gamma)$ in $(4)$?</span><a href="#why-is-there-a-coefficient-1-gamma-in-4" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4>

\[d_{s_0}^\pi(s) := (1-\gamma) \sum_{t=0}^\infty \gamma^t {\Pr}^\pi(s_t=s|s_0).\]

<p>Recall that <a href="https://lilianweng.github.io/posts/2018-04-08-policy-gradient/">the derivation of the policy gradient theorem</a>:</p>

\[\nabla_{\theta} V^{\pi_\theta}(s_0) = \sum\limits_{s} \sum\limits_{k=0}^{\infty} \gamma^k \cdot \text{Pr}(s_0\to s, k) \sum\limits_{a} \pi(a\mid s) \cdot Q^\pi(s,a)\cdot \nabla_{\theta}\ln \pi(a\mid s).\]

<blockquote class="prompt-info">
  <p>Policy Gradient</p>
  <ul>
    <li>Williams, Ronald J. “Simple statistical gradient-following algorithms for connectionist reinforcement learning.” Machine learning 8 (1992): 229-256.</li>
    <li>Sutton, Richard S., et al. “Policy gradient methods for reinforcement learning with function approximation.” Advances in neural information processing systems 12 (1999).</li>
  </ul>
</blockquote>

<p>Note that $\lim\limits_{k\to\infty} \sum\limits_{k=0}^{\infty} \gamma^k = \frac{1}{1-\gamma} &gt; 1$. The value of discounted state visitation distribution should not larger than $1$. So the coefficient $(1-\gamma)$ is for normalization.</p>

<hr />

<h4 id="why-is-there-a-coefficient-frac11-gamma-in-5"><span class="me-2">Why is there a coefficient $\frac{1}{1-\gamma}$ in $(5)$?</span><a href="#why-is-there-a-coefficient-frac11-gamma-in-5" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4>

\[\begin{aligned}
\nabla_\theta V^{\pi_\theta}(s_0) 
=&amp; \frac{1}{1-\gamma} \, \mathbb{E}_{s \sim d_{s_0}^{\pi_\theta} }\mathbb{E}_{a\sim \pi_\theta(\cdot | s) }
\big[\nabla_\theta \log
\pi_{\theta}(a| s) Q^{\pi_\theta}(s,a)\big] \\
=&amp; \sum\limits_{s} d^\pi_{s_0}(s) \sum\limits_{a} \pi(a\mid s) \cdot Q^\pi(s,a)\cdot \nabla_{\theta}\log \pi(a\mid s).
\end{aligned}\]

<p>It is used to cancel that normalization.</p>

<hr />

<h4 id="advantage"><span class="me-2">Advantage</span><a href="#advantage" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4>

\[\begin{aligned}
A^{\pi}(s,a)
:=&amp; Q^\pi(s,a)-V^\pi(s) \\
=&amp; Q^\pi(s,a) - \sum\limits_{a}\pi(a\mid s) \cdot Q^\pi(s,a).
\end{aligned}\]

<p>Given $s$ and $\pi$, $A^{\pi}(s,a)$ measures how much better the expected future return after selecting action $a$ is compared to the expected future return of sampling action based on the current policy $\pi$ in this state $s$.</p>

<hr />

<h4 id="baseline"><span class="me-2">Baseline</span><a href="#baseline" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4>
<blockquote class="prompt-info">
  <p>This part partially use material from Prof. Wang’s <a href="https://drive.google.com/drive/folders/1u1oyOMsvo4bJ765NE_2HSR5x40uXWwxD">Lecture note 18: Variance reduction</a> and <em>Reinforcement learning: An introduction</em>.</p>
</blockquote>

<p>Policy gradient is unbiased but with high variance. Recall that the form is</p>

\[\nabla_{\theta} V^{\pi_\theta}(s_0) =\frac{1}{1-\gamma} \mathbb{E}_{s\sim d_{s_0}^\pi}\mathbb{E}_{a\sim\pi(\cdot\mid s)}\left[ Q^\pi(s,a)\cdot \nabla_{\theta}\log \pi(a\mid s)\right].\]

<p>To reduce it, a natural solution is to subtract a baseline $b(s)$ from $Q^\pi$ which can be any function, even a random variable, as long as it does not depend on the action $a$, i.e.,</p>

\[\nabla_{\theta} V^{\pi_\theta}(s_0) =\frac{1}{1-\gamma} \mathbb{E}_{s\sim d_{s_0}^\pi}\mathbb{E}_{a\sim\pi(\cdot\mid s)}\left[ \left(Q^\pi(s,a) - b(s)\right)\cdot \nabla_{\theta}\log \pi(a\mid s)\right],\]

\[\nabla_{\theta} V^{\pi_\theta}(s_0) = \sum\limits_{s} d^\pi_{s_0}(s) \sum\limits_{a} \pi(a\mid s) \cdot \left(Q^\pi(s,a) - b(s)\right)\cdot \nabla_{\theta}\log \pi(a\mid s),\]

<p>or</p>

\[\nabla_{\theta} V^{\pi_\theta}(s_0) = \sum\limits_{s} d^\pi_{s_0}(s) \sum\limits_{a} \left(Q^\pi(s,a) - b(s)\right)\cdot \nabla_{\theta} \pi(a\mid s).\]

<p>This is still unbiased:</p>

\[\begin{aligned}
&amp;\sum\limits_a b(s)\cdot \nabla_{\theta} \pi(a\mid s) \\
=&amp; b(s) \cdot\nabla_\theta\sum\limits_a \pi(a\mid s) \\
=&amp; b(s) \cdot\nabla_\theta 1 \\
=&amp; 0. 
\end{aligned}\]

<p>But it has lower variance:</p>
<blockquote>
  <p>Assume that:</p>
  <ul>
    <li>$X = Q^\pi(s,a)\cdot \nabla_{\theta} \pi(a\mid s)$,</li>
    <li>$Y = \nabla_{\theta} \pi(a\mid s)$,</li>
    <li>$\mathbb{E} \left[ X \right] = \mu$,</li>
    <li>$\mathbb{E} \left[ Y \right] = \eta = 0$,</li>
    <li>$X’ = X + c(Y-\eta)$.</li>
  </ul>

  <p>Then:</p>
  <ul>
    <li>$\mathbb{E} \left[ X’ \right] = \mu$,</li>
    <li>
\[\begin{aligned} \mathbb{V} \left[ X' \right] =&amp; \mathbb{V} \left[ X + c(Y-\eta) \right] \\ =&amp; \mathbb{V} \left[ X \right] + c^2\cdot \mathbb{V} \left[ Y-\eta \right] +2c\cdot \text{Cov}(X,Y-\eta) \\ =&amp; \mathbb{V} \left[ Y-\eta \right] \cdot c^2 + 2\cdot \text{Cov}(X,Y-\eta)\cdot c + \mathbb{V} \left[ X \right],\end{aligned}\]
    </li>
    <li>$\min \mathbb{V} \left[ X’ \right] = \left(1 - \text{Corr(X,Y)}\right)\cdot \mathbb{V} \left[ X \right]$.
<!-- $$
\mathbb{E}_x \left[ \mathbb{E}_y \left[ f(x,y) - b(x) \cdot g(x,y) \right] \right]
$$ --></li>
  </ul>
</blockquote>

<blockquote class="prompt-info">
  <p>Common usage: GAE (Generalized Advantage Estimation).</p>

  <p>Schulman, John, et al. “High-dimensional continuous control using generalized advantage estimation.” arXiv preprint arXiv:1506.02438 (2015).</p>
</blockquote>

<hr />

<h4 id="equation-6-does-not-hold-for-the-direct-parameterization"><span class="me-2">Equation (6) does not hold for the direct parameterization</span><a href="#equation-6-does-not-hold-for-the-direct-parameterization" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4>

\[\begin{aligned}
   \sum\limits_a \nabla_\theta \pi(a) =&amp; \left(\sum\limits_a\frac{\partial \pi(a)}{\partial \theta_1},\ldots, \sum\limits_a\frac{\partial \pi(a)}{\partial \theta_m}\right)\\ 
\end{aligned}\]

<p>If every $\frac{\partial \pi(a)}{\partial \theta_1}$ has the same variables, then $\sum\limits_a\frac{\partial \pi(a)}{\partial \theta_1} = \frac{\partial \sum\limits_a\pi(a)}{\partial \theta_1} = 0$. But in the case of the direct parameterization, this assumption does not hold, i.e., $\sum\limits_a\frac{\partial \pi(a)}{\partial \theta_1} = 1$.</p>

<hr />

<h4 id="distribution-mismatch-coefficient-pass"><span class="me-2">Distribution mismatch coefficient (pass)</span><a href="#distribution-mismatch-coefficient-pass" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4>
<p>I think this concept is introduced too soon. Let’s discuss it later.</p>

<hr />

<h3 id="details-on-constrained-tabular-parameterization"><span class="me-2">Details on Constrained Tabular Parameterization</span><a href="#details-on-constrained-tabular-parameterization" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<blockquote>
  <p>This algorithm is <strong>projected gradient ascent</strong> on the <strong>direct policy parametrization</strong> of the MDP.</p>
</blockquote>

<h4 id="equation-7"><span class="me-2">Equation $(7)$</span><a href="#equation-7" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4>
<p>$\mu$ is a distribution of $s_0$.</p>

\[\begin{aligned}
&amp; \nabla_\theta V^{\pi_\theta}(\mu) \\
=&amp;  \frac{\partial V^{\pi_\theta}(\mu)}{\partial } \\
=&amp; \frac{1}{1-\gamma} \, \mathbb{E}_{s \sim d_{s_0}^{\pi_\theta} }\mathbb{E}_{a\sim \pi_\theta(\cdot | s) }
\big[\nabla_\theta \log
\pi_{\theta}(a| s) Q^{\pi_\theta}(s,a)\big] \\
=&amp; \sum\limits_{s} d^\pi_{s_0}(s) \sum\limits_{a} \pi(a\mid s) \cdot Q^\pi(s,a)\cdot \nabla_{\theta}\log \pi(a\mid s).
\end{aligned}\]

<blockquote class="prompt-warning">
  <p>The following part has not been finished yet. One may check my <a href="https://yuelin301.github.io/posts/Schedule/">writing schedule</a>.</p>
</blockquote>

  </div>

  <div class="post-tail-wrapper text-muted">
    <!-- categories -->
    
      <div class="post-meta mb-3">
        <i class="far fa-folder-open fa-fw me-1"></i>
        
          <a href="/categories/artificial-intelligence/">Artificial Intelligence</a>,
          <a href="/categories/reinforcement-learning/">Reinforcement Learning</a>
      </div>
    

    <!-- tags -->
    
      <div class="post-tags">
        <i class="fa fa-tags fa-fw me-1"></i>
        
          <a
            href="/tags/tech/"
            class="post-tag no-text-decoration"
          >tech</a>
        
          <a
            href="/tags/convergence/"
            class="post-tag no-text-decoration"
          >convergence</a>
        
          <a
            href="/tags/policy-gradient/"
            class="post-tag no-text-decoration"
          >policy gradient</a>
        
          <a
            href="/tags/reinforcement-learning/"
            class="post-tag no-text-decoration"
          >reinforcement learning</a>
        
          <a
            href="/tags/tech/"
            class="post-tag no-text-decoration"
          >tech</a>
        
      </div>
    

    <div
      class="
        post-tail-bottom
        d-flex justify-content-between align-items-center mt-5 pb-2
      "
    >
      <div class="license-wrapper">
        
          

          This post is licensed under 
        <a href="https://creativecommons.org/licenses/by/4.0/">
          CC BY 4.0
        </a>
         by the author.
        
      </div>

      <!-- Post sharing snippet -->

<div class="share-wrapper d-flex align-items-center">
  <span class="share-label text-muted me-1">Share</span>
  <span class="share-icons">
    
    
    

    
      
      <a
        href="https://twitter.com/intent/tweet?text=Details%20on%20the%20Analysis%20of%20Policy%20Gradient%20Methods%20-%20Yue%20Lin&url=http%3A%2F%2Flocalhost%3A4000%2Fposts%2FPolicy-Gradient-Details%2F"
        data-bs-toggle="tooltip"
        data-bs-placement="top"
        title="Twitter"
        target="_blank"
        rel="noopener"
        aria-label="Twitter"
      >
        <i class="fa-fw fa-brands fa-square-x-twitter"></i>
      </a>
    
      
      <a
        href="https://www.facebook.com/sharer/sharer.php?title=Details%20on%20the%20Analysis%20of%20Policy%20Gradient%20Methods%20-%20Yue%20Lin&u=http%3A%2F%2Flocalhost%3A4000%2Fposts%2FPolicy-Gradient-Details%2F"
        data-bs-toggle="tooltip"
        data-bs-placement="top"
        title="Facebook"
        target="_blank"
        rel="noopener"
        aria-label="Facebook"
      >
        <i class="fa-fw fab fa-facebook-square"></i>
      </a>
    
      
      <a
        href="https://t.me/share/url?url=http%3A%2F%2Flocalhost%3A4000%2Fposts%2FPolicy-Gradient-Details%2F&text=Details%20on%20the%20Analysis%20of%20Policy%20Gradient%20Methods%20-%20Yue%20Lin"
        data-bs-toggle="tooltip"
        data-bs-placement="top"
        title="Telegram"
        target="_blank"
        rel="noopener"
        aria-label="Telegram"
      >
        <i class="fa-fw fab fa-telegram"></i>
      </a>
    

    <button
      id="copy-link"
      aria-label="Copy link"
      class="btn small"
      data-bs-toggle="tooltip"
      data-bs-placement="top"
      title="Copy link"
      data-title-succeed="Link copied successfully!"
    >
      <i class="fa-fw fas fa-link pe-none"></i>
    </button>
  </span>
</div>

    </div>
    <!-- .post-tail-bottom -->
  </div>
  <!-- div.post-tail-wrapper -->
</article>


            
          </main>

          <!-- panel -->
          <!-- <aside aria-label="Panel" id="panel-wrapper" class="col-xl-3 ps-2 mb-5 text-muted"> -->
          <aside aria-label="Panel" id="panel-wrapper" class="col-12 col-sm-3 col-lg-3 col-xl-3 ps-2 mb-5 text-muted">
            <div class="access">
              <!-- Get the last 5 posts from lastmod list. -->














  <section id="access-lastmod">
    <h2 class="panel-heading">Recently Updated</h2>
    <ul class="content list-unstyled ps-0 pb-1 ms-1 mt-2">
      
        
        
        
        <li class="text-truncate lh-lg">
          <a href="/posts/English-Toolbox/">English Toolbox</a>
        </li>
      
        
        
        
        <li class="text-truncate lh-lg">
          <a href="/posts/Personality/">Personality</a>
        </li>
      
        
        
        
        <li class="text-truncate lh-lg">
          <a href="/posts/MARL-Tasks/">MARL Tasks</a>
        </li>
      
        
        
        
        <li class="text-truncate lh-lg">
          <a href="/posts/Information-Design-10min/">Information Design in 10 Minutes</a>
        </li>
      
    </ul>
  </section>
  <!-- #access-lastmod -->


              <!-- The trending tags list -->















  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        



  <section>
    <h2 class="panel-heading">Trending Tags</h2>
    <div class="d-flex flex-wrap mt-3 mb-1 me-3">
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/tech/">tech</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/multi-agents/">multi agents</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/math/">math</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/reinforcement-learning/">reinforcement learning</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/game-theory/">game theory</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/life/">life</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/efficiency/">efficiency</a>
      
    </div>
  </section>


            </div>

            
              
              



  <section id="toc-wrapper" class="ps-0 pe-4">
    <h2 class="panel-heading ps-3 pt-2 mb-2">Contents</h2>
    <nav id="toc"></nav>
  </section>


            
          </aside>
        </div>

        <div class="row">
          <!-- tail -->
          <div id="tail-wrapper" class="col-12 col-lg-11 col-xl-9 px-md-4">
            
              
              <!-- Recommend the other 3 posts according to the tags and categories of the current post. -->

<!-- The total size of related posts -->


<!-- An random integer that bigger than 0 -->


<!-- Equals to TAG_SCORE / {max_categories_hierarchy} -->














  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  
    
  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  











  <aside id="related-posts" aria-labelledby="related-label">
    <h3 class="mb-4" id="related-label">Further Reading</h3>
    <nav class="row row-cols-1 row-cols-md-2 row-cols-xl-3 g-4 mb-4">
      
        <article class="col">
          <a href="/posts/RL-Toolbox/" class="post-preview card h-100">
            <div class="card-body">
              <!--
  Date format snippet
  See: ${JS_ROOT}/utils/locale-dateime.js
-->




<time
  
  data-ts="1681152000"
  data-df="ll"
  
>
  Apr 10, 2023
</time>

              <h4 class="pt-0 my-2">RL Toolbox</h4>
              <div class="text-muted">
                <p>
                  





                  
  This note will be consistently updated.




PPO Tricks

There are a total of 37 tricks, among which 13 are relatively core.


  
    PPO paper
    The 37 Implementation Details of Proximal Polic...
                </p>
              </div>
            </div>
          </a>
        </article>
      
        <article class="col">
          <a href="/posts/SSCC/" class="post-preview card h-100">
            <div class="card-body">
              <!--
  Date format snippet
  See: ${JS_ROOT}/utils/locale-dateime.js
-->




<time
  
  data-ts="1680523201"
  data-df="ll"
  
>
  Apr  3, 2023
</time>

              <h4 class="pt-0 my-2">Swinging Search and Crawling Control</h4>
              <div class="text-muted">
                <p>
                  





                  
  Please be aware that the video accompanying this article may take some time to load, depending on the speed of your internet connection to GitHub.


A snake-inspired path planning algorithm base...
                </p>
              </div>
            </div>
          </a>
        </article>
      
        <article class="col">
          <a href="/posts/Contraction/" class="post-preview card h-100">
            <div class="card-body">
              <!--
  Date format snippet
  See: ${JS_ROOT}/utils/locale-dateime.js
-->




<time
  
  data-ts="1697740800"
  data-df="ll"
  
>
  Oct 19, 2023
</time>

              <h4 class="pt-0 my-2">Contraction Mapping Theorem</h4>
              <div class="text-muted">
                <p>
                  





                  Metric Space

Definition of metric space


  Definition. 
A metric space is an ordered pair $(M, d)$ where $M$ is a set and $d$ is a metric on $M$, i.e., a function $d: M\times M \to \mathbb{R}$ sa...
                </p>
              </div>
            </div>
          </a>
        </article>
      
    </nav>
  </aside>
  <!-- #related-posts -->


            
              
              <!-- Navigation buttons at the bottom of the post. -->

<nav class="post-navigation d-flex justify-content-between" aria-label="Post Navigation">
  
  

  
    <a
      href="/posts/Seq2Seq/"
      class="btn btn-outline-primary"
      aria-label="Older"
    >
      <p>Sequence-to-Sequence Models</p>
    </a>
  

  
    <a
      href="/posts/Fictitious-Self-Play-Zero-Shot-Coordination/"
      class="btn btn-outline-primary"
      aria-label="Newer"
    >
      <p>Fictitious Self-Play and Zero-Shot Coordination</p>
    </a>
  
</nav>

            
              
              <!--  The comments switcher -->


            

            <!-- The Footer -->

<footer
  aria-label="Site Info"
  class="
    d-flex flex-column justify-content-center text-muted
    flex-lg-row justify-content-lg-between align-items-lg-center pb-lg-3
  "
>
  <p>
    ©
    <time>2023</time>
    <a href="https://github.com/YueLin301">Yue Lin</a>.
    
      <span
        data-bs-toggle="tooltip"
        data-bs-placement="top"
        title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author."
      >Some rights reserved.</span>
    
  </p>

  <p>Adapted from the <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme for <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a>
  </p>
</footer>

          </div>
        </div>

        <!-- The Search results -->

<div id="search-result-wrapper" class="d-flex justify-content-center unloaded">
  <div class="col-11 content">
    <div id="search-hints">
      <!-- The trending tags list -->















  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        



  <section>
    <h2 class="panel-heading">Trending Tags</h2>
    <div class="d-flex flex-wrap mt-3 mb-1 me-3">
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/tech/">tech</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/multi-agents/">multi agents</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/math/">math</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/reinforcement-learning/">reinforcement learning</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/game-theory/">game theory</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/life/">life</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/efficiency/">efficiency</a>
      
    </div>
  </section>


    </div>
    <div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div>
  </div>
</div>

      </div>

      <aside aria-label="Scroll to Top">
        <button id="back-to-top" type="button" class="btn btn-lg btn-box-shadow">
          <i class="fas fa-angle-up"></i>
        </button>
      </aside>
    </div>

    <div id="mask"></div>

    
      <aside
  id="notification"
  class="toast"
  role="alert"
  aria-live="assertive"
  aria-atomic="true"
  data-bs-animation="true"
  data-bs-autohide="false"
>
  <div class="toast-header">
    <button
      type="button"
      class="btn-close ms-auto"
      data-bs-dismiss="toast"
      aria-label="Close"
    ></button>
  </div>
  <div class="toast-body text-center pt-0">
    <p class="px-2 mb-3">A new version of content is available.</p>
    <button type="button" class="btn btn-primary" aria-label="Update">
      Update
    </button>
  </div>
</aside>

    

    <!-- JavaScripts -->

    <!-- JS selector for site. -->

<!-- commons -->



<!-- layout specified -->


  

  
    <!-- image lazy-loading & popup & clipboard -->
    
  















  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  



  <script src="https://cdn.jsdelivr.net/combine/npm/jquery@3.7.1/dist/jquery.min.js,npm/bootstrap@5.3.2/dist/js/bootstrap.bundle.min.js,npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js,npm/loading-attribute-polyfill@2.1.1/dist/loading-attribute-polyfill.umd.min.js,npm/magnific-popup@1.1.0/dist/jquery.magnific-popup.min.js,npm/clipboard@2.0.11/dist/clipboard.min.js,npm/dayjs@1.11.10/dayjs.min.js,npm/dayjs@1.11.10/locale/en.min.js,npm/dayjs@1.11.10/plugin/relativeTime.min.js,npm/dayjs@1.11.10/plugin/localizedFormat.min.js,npm/tocbot@4.21.1/dist/tocbot.min.js"></script>






<script defer src="/assets/js/dist/post.min.js"></script>


  <!-- MathJax -->
  <script>
    /* see: <https://docs.mathjax.org/en/latest/options/input/tex.html#tex-options> */
    MathJax = {
      tex: {
        /* start/end delimiter pairs for in-line math */
        inlineMath: [
          ['$', '$'],
          ['\\(', '\\)']
        ],
        /* start/end delimiter pairs for display math */
        displayMath: [
          ['$$', '$$'],
          ['\\[', '\\]']
        ]
      }
    };
  </script>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-chtml.js"></script>





    

    <!--
  Jekyll Simple Search loader
  See: <https://github.com/christian-fei/Simple-Jekyll-Search>
-->





<script>
  /* Note: dependent library will be loaded in `js-selector.html` */
  SimpleJekyllSearch({
    searchInput: document.getElementById('search-input'),
    resultsContainer: document.getElementById('search-results'),
    json: '/assets/js/data/search.json',
    searchResultTemplate: '  <article class="px-1 px-sm-2 px-lg-4 px-xl-0">    <header>      <h2><a href="{url}">{title}</a></h2>      <div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1">        {categories}        {tags}      </div>    </header>    <p>{snippet}</p>  </article>',
    noResultsText: '<p class="mt-5"></p>',
    templateMiddleware: function(prop, value, template) {
      if (prop === 'categories') {
        if (value === '') {
          return `${value}`;
        } else {
          return `<div class="me-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`;
        }
      }

      if (prop === 'tags') {
        if (value === '') {
          return `${value}`;
        } else {
          return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`;
        }
      }
    }
  });
</script>

  </body>
</html>

