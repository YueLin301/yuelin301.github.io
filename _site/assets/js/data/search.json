[
  
  {
    "title": "Tools of Visual Studio Code",
    "url": "/posts/VSCode/",
    "categories": "Efficiency, Tools",
    "tags": "tech, efficiency, tools, markdown",
    "date": "2023-12-24 18:40:00 +0000",
    





    
    "snippet": "  This note will be consistently updated.ShortcutsCommand + k Command + s: Keyboard Shortcuts  Command + p: Go to file  Command + Shift + o: Go to symbol in editor  Command + t: Go to symbol in wor...",
    "content": "  This note will be consistently updated.ShortcutsCommand + k Command + s: Keyboard Shortcuts  Command + p: Go to file  Command + Shift + o: Go to symbol in editor  Command + t: Go to symbol in workspace  Command + Shift + f: Search in workspace  Command + Shift + p: input command  Command + Shift + n: New window  Command + o: Open  Command + ,: Settings  Control + `: Terminal  F12: Go to definition  Shift + F12: Go to references  Control + -: Go back  Control + Shift + -: Go forward  Control + w: Switch windowRemote ConnectionRemote - SSHè¿™ç©æ„å¯ä»¥ç›´æ¥ç”¨vs codeæ‰“å¼€æœåŠ¡å™¨ä¸Šçš„æ–‡ä»¶å¤¹ã€‚Remote - SSH æ‰©å±•ä½¿å¾—å¼€å‘è€…å¯ä»¥ç›´æ¥é€šè¿‡ SSH åè®®è¿æ¥åˆ°è¿œç¨‹æœåŠ¡å™¨ã€‚è¿™æ„å‘³ç€æ‚¨å¯ä»¥åœ¨æœ¬åœ°çš„ VS Code ä¸­ç¼–è¾‘è¿œç¨‹æœåŠ¡å™¨ä¸Šçš„æ–‡ä»¶ï¼Œè¿è¡Œä»£ç ï¼Œç”šè‡³è°ƒè¯•ç¨‹åºã€‚è¿™å¯¹äºé‚£äº›éœ€è¦å¤„ç†å¤§é‡æ•°æ®ã€è¿è¡Œå¤§å‹åº”ç”¨æˆ–è€…éœ€è¦ç‰¹å®šæœåŠ¡å™¨ç¯å¢ƒçš„å¼€å‘è€…ç‰¹åˆ«æœ‰ç”¨ã€‚æ‚¨åªéœ€åœ¨æœ¬åœ°è®¡ç®—æœºä¸Šå®‰è£… VS Code å’Œå¿…è¦çš„æ‰©å±•ï¼ŒæœåŠ¡å™¨ç«¯åˆ™æ— éœ€å®‰è£… VS Codeã€‚  æ‰“å¼€è¿œç¨‹çª—å£ï¼šç‚¹å‡» VS Code å·¦ä¸‹è§’çš„ç»¿è‰²æŒ‰é’®ï¼ˆè¿œç¨‹çª—å£æŒ‡ç¤ºå™¨ï¼‰ï¼Œæˆ–æŒ‰ Ctrl+Shift+P æ‰“å¼€å‘½ä»¤é¢æ¿ï¼Œç„¶åè¾“å…¥Remote-SSH: Add new ssh Hostã€‚  æ·»åŠ  SSH ä¸»æœºï¼šè¾“å…¥æ‚¨çš„ SSH è¿æ¥å‘½ä»¤ï¼Œæ¯”å¦‚ ssh username@hostnameã€‚å¦‚æœä¹‹å‰å·²æ·»åŠ è¿‡ï¼Œå¯ä»¥ç›´æ¥ä»åˆ—è¡¨ä¸­é€‰æ‹©ã€‚  é€‰æ‹©é…ç½®æ–‡ä»¶ï¼šå¦‚æœæ˜¯é¦–æ¬¡æ·»åŠ ï¼Œç³»ç»Ÿä¼šè¯¢é—®æ‚¨ä¿å­˜é…ç½®çš„ä½ç½®ï¼Œé€šå¸¸ä¿å­˜åœ¨ç”¨æˆ·ç›®å½•çš„ .ssh/config æ–‡ä»¶ä¸­ã€‚å¦‚æœæˆ‘æƒ³ç”¨æˆ‘çš„ç”µè„‘è¿æ¥è¿œç¨‹æœåŠ¡å™¨ï¼Œæˆ‘åº”è¯¥é€‰æ‹©çš„æ˜¯æœ¬æœºçš„ssh configuration file.  è¿æ¥å’Œè¾“å…¥å‡­æ®ï¼šé€‰æ‹©æ‚¨çš„ SSH ä¸»æœºåï¼ŒVS Code ä¼šå°è¯•å»ºç«‹è¿æ¥ã€‚æ ¹æ®é…ç½®ï¼Œæ‚¨å¯èƒ½éœ€è¦è¾“å…¥å¯†ç æˆ–ä½¿ç”¨ SSH å¯†é’¥è®¤è¯ã€‚  æ‰“å¼€é¡¹ç›®ï¼šè¿æ¥æˆåŠŸåï¼Œæ‚¨å¯ä»¥é€šè¿‡ â€œFile -&gt; Open Folderâ€ æ‰“å¼€è¿œç¨‹æœåŠ¡å™¨ä¸Šçš„é¡¹ç›®æ–‡ä»¶å¤¹ã€‚SFTPâ€œSFTPâ€ æ˜¯ Visual Studio Code (VS Code) çš„ä¸€ä¸ªæµè¡Œæ’ä»¶ï¼Œå®ƒå…è®¸ç”¨æˆ·é€šè¿‡ SFTP (Secure File Transfer Protocol) æˆ– FTP (File Transfer Protocol) æ¥ä¸Šä¼ ã€ä¸‹è½½å’ŒåŒæ­¥æ–‡ä»¶ã€‚è¿™ä¸ªæ’ä»¶ç‰¹åˆ«é€‚åˆåœ¨æœ¬åœ°å’Œè¿œç¨‹æœåŠ¡å™¨ä¹‹é—´åŒæ­¥æ–‡ä»¶ï¼Œå¯¹äºéœ€è¦é¢‘ç¹æ›´æ–°æœåŠ¡å™¨ä¸Šæ–‡ä»¶çš„å¼€å‘è€…æ¥è¯´éå¸¸æœ‰ç”¨ã€‚  ä¸‹è½½å’Œä¸Šä¼ ï¼Œæ–‡ä»¶åŒæ­¥  ä¸‹è½½ä¸‹æ¥æ”¹ï¼Œæ”¹å®Œä¸Šä¼ ï¼Œç„¶ååœ¨è¿œç¨‹è¿è¡ŒUsage      In VS Code, open a local directory you wish to sync to the remote server (or create an empty directorythat you wish to first download the contents of a remote server folder in order to edit locally).    Ctrl+Shift+P on Windows/Linux or Cmd+Shift+P on Mac open command palette, run SFTP: config command.    A basic configuration file will appear named sftp.json under the .vscode directory, open and edit the configuration parameters with your remote server information.    For instance:  {    \"name\": \"Profile Name\",    \"host\": \"name_of_remote_host\",    \"protocol\": \"ftp\",    \"port\": 21,    \"secure\": true,    \"username\": \"username\",    \"remotePath\": \"/public_html/project\",    \"password\": \"password\",    \"uploadOnSave\": false}    The password parameter in sftp.json is optional, if left out you will be prompted for a password on sync.Noteï¼š backslashes and other special characters must be escaped with a backslash.      Save and close the sftp.json file.    Ctrl+Shift+P on Windows/Linux or Cmd+Shift+P on Mac open command palette.    Type sftp and youâ€™ll now see a number of other commands. You can also access many of the commands from the projectâ€™s file explorer context menus.    A good one to start with if you want to sync with a remote folder is SFTP: Download Project.  This will download the directory shown in the remotePath setting in sftp.json to your local open directory.    Done - you can now edit locally and after each save it will upload to sync your remote file with the local copy.    Enjoy!    For detailed explanations please go to wiki.ExtensionsDendron Paste ImageCommand + Option + V: put the copied image in a new created file (named the same as the current file) and paste it here.Markdown All in OneRun command â€œCreate Table of Contentsâ€: Create Table of Contents.Outline MapYou can find it in the sidebar.TODO TreeYou can find it in the sidebar.Local HistoryYou can find it in the file explorer.vscode-iconsSkin.MacOS =&gt; Code &gt; Preferences &gt; File Icon Theme &gt; VSCode Icons.Rainbow BracketsPassive skill.Error LensPassive skill.Better AlignChoose texts and Option + APath IntellisensePassive skill.IntelliCodePassive skill.Code Spell CheckerPassive skill.One Dark ProSkin.Power ModeSkin.Passive skill.Show some special effects while you type."
  },
  
  {
    "title": "Interesting Facts",
    "url": "/posts/Interesting-Facts/",
    "categories": "Misc Notes",
    "tags": "life, misc note, writing buffer",
    "date": "2023-12-06 18:40:00 +0000",
    





    
    "snippet": "  This note will be consistently updated.Society  The Rules for Rulers | CGP Grey (YouTube)Animals  Octopus vs Underwater Maze | Mark Rober (YouTube) or ã€Mark Rober å®éªŒã€ä¸­æ–‡é…éŸ³ã€‘ | ç« é±¼åˆ°åº•æœ‰å¤šèªæ˜ï¼Ÿ(BiliBili)ã€‘I...",
    "content": "  This note will be consistently updated.Society  The Rules for Rulers | CGP Grey (YouTube)Animals  Octopus vs Underwater Maze | Mark Rober (YouTube) or ã€Mark Rober å®éªŒã€ä¸­æ–‡é…éŸ³ã€‘ | ç« é±¼åˆ°åº•æœ‰å¤šèªæ˜ï¼Ÿ(BiliBili)ã€‘Information Technology  Why canâ€™t robots check the box that says â€˜Iâ€™m not a robotâ€™? | WTFAQ | ABC TV + iview (YouTube)Psychology  INTJ vs INFJ"
  },
  
  {
    "title": "Information Design in Multi-Agent Reinforcement Learning",
    "url": "/posts/IDMARL/",
    "categories": "Artificial Intelligence, Multi-Agent Reinforcement Learning",
    "tags": "tech, information design, sequential social dilemma, multi agents, reinforcement learning, my paper",
    "date": "2023-11-23 18:40:00 +0000",
    





    
    "snippet": "Key Points  Mixed-motive between two agents. Some other MARL papers use the term â€œmixed-motiveâ€ to describe scenarios involving two groups of agents, where agents within the same group cooperate bu...",
    "content": "Key Points  Mixed-motive between two agents. Some other MARL papers use the term â€œmixed-motiveâ€ to describe scenarios involving two groups of agents, where agents within the same group cooperate but have conflicts with agents from the other group.  Non-stationarity.          Signals should not be viewed as actions.      Signals affects the othersâ€™ â€œenvironmentâ€.        Signaling Gradient is not only suitable for mixed-motive communication, but is also suitable for fully-cooperative communication.  Information design is far more difficult than incentive design.          Signal can be ignored. Incentive is compulsory. In the training beginning, the signaling scheme is almost random, the receiver will easily learn to ignore the messages, and this case is a strong equilibrium.      Signals immediately changes transitions (it affects both the sampling phase and the update phase of RL). Reward does not affect trajectory (it only affects the update phase of RL). So the hyper-gradient method used in LIO is not applicable here (the first-order gradient from the sampling phase is dominant).      The sender cannot take environmental actions. It only can get feedback from the receiverâ€™s actions.        The sender and the receiver are rational (in a sense of RL), self-interested, and risk-neutral.  The receiverâ€™s policy is Markovian, not history-dependent. At every timestep, it takes actions based on the current estimation of the future payoffs. So the obedience constraint can be easily extended in MSGs.  In the learning scenario, we can cancel the commitment assumption and the analysis of the revelation principle.Interesting Experimental Results  The DIAL sender does not concern about itself at all.  Symmetricity. Emergent Languages.  Honesty of the sender can be manipuated by the hyperparameters of the Lagrangian method, as shown in the heatmap in Appendix H.6.  The more the receiver can see, the less the informational adavantage the sender has, the less the sender can manipulate, as shown in the Appendix H.7.  The following part has not been finished yet."
  },
  
  {
    "title": "Policy Distillation",
    "url": "/posts/Policy-Distillation/",
    "categories": "Artificial Intelligence, Reinforcement Learning",
    "tags": "tech, policy distillation",
    "date": "2023-11-15 18:40:00 +0000",
    





    
    "snippet": "Introduction[Paper]: Policy DistillationThe following statements from the paper are key to understand this technique:  Distillation is a method to transfer knowledge from a teacher model $T$ to a s...",
    "content": "Introduction[Paper]: Policy DistillationThe following statements from the paper are key to understand this technique:  Distillation is a method to transfer knowledge from a teacher model $T$ to a student model $S$.  Goals:          It is â€œused to extract the policy of a reinforcement learning agent and train a new network that performs at the expert level while being dramatically smaller and more efficient.â€      It is â€œused to consolidate multiple task-specific policies into a single policy.â€        The following part has not been finished yet.Single-Game Policy DistillationMulti-Task Policy Distillation"
  },
  
  {
    "title": "Rational Emotive Behavior Therapy (REBT)",
    "url": "/posts/Rational-Emotive-Behavior-Therapy/",
    "categories": "Efficiency, Psychology",
    "tags": "life, efficiency, psychology, personality",
    "date": "2023-11-13 18:40:00 +0000",
    





    
    "snippet": "Introduction  Rational Emotive Behavior Therapy is a form of psychotherapy that helps you identify self-defeating thoughts and feelings, challenge the nature of irrational and unproductive feelings...",
    "content": "Introduction  Rational Emotive Behavior Therapy is a form of psychotherapy that helps you identify self-defeating thoughts and feelings, challenge the nature of irrational and unproductive feelings, and replace them with healthier, more productive beliefs.â€” From the website â€œPsychology Todayâ€.Theoretical Assumptions  The REBT framework posits that humans have both innate rational (meaning self-helping, socially helping, and constructive) and irrational (meaning self-defeating, socially defeating, and unhelpful) tendencies and leanings.REBT claims that people to a large degree consciously and unconsciously construct emotional difficulties such as self-blame, self-pity, clinical anger, hurt, guilt, shame, depression and anxiety, and behavior tendencies like procrastination, compulsiveness, avoidance, addiction and withdrawal by the means of their irrational and self-defeating thinking, emoting and behaving.  REBT is then applied as an educational process in which the therapist often active-directively teaches the client how to identify irrational and self-defeating beliefs and philosophies which in nature are rigid, extreme, unrealistic, illogical and absolutist, and then to forcefully and actively question and dispute them and replace them with more rational and self-helping ones.â€” From Wikipedia.A-B-C-D-E-F ModelIllustration from Wikipedia.  The Bs, irrational beliefs that are most important in the A-B-C model are the explicit and implicit philosophical meanings and assumptions about events, personal desires, and preferences. The Bs, beliefs that are most significant are highly evaluative and consist of interrelated and integrated cognitive, emotional and behavioral aspects and dimensions. According to REBT, if a personâ€™s evaluative B, belief about the A, activating event is rigid, absolutistic, fictional and dysfunctional, the C, the emotional and behavioral consequence, is likely to be self-defeating and destructive. Alternatively, if a personâ€™s belief is preferential, flexible, and constructive, the C, the emotional and behavioral consequence is likely to be self-helping and constructive.  Through REBT, by understanding the role of their mediating, evaluative and philosophically based illogical, unrealistic and self-defeating meanings, interpretations and assumptions in disturbance, individuals can learn to identify them, then go to D, disputing and questioning the evidence for them. At E, effective new philosophy, they can recognize and reinforce the notion no evidence exists for any psychopathological must, ought or should and distinguish them from healthy constructs, and subscribe to more constructive and self-helping philosophies. This new reasonable perspective leads to F, new feelings and behaviors appropriate to the A they are addressing in the exercise.â€” From Wikipedia.My UnderstandingWe cannot ignore our feelings. Since the adversity has come true, the best thing we can do now is to adjust what we think to have positive feelings, and move on."
  },
  
  {
    "title": "HyperNetworks",
    "url": "/posts/HyperNetworks/",
    "categories": "Artificial Intelligence, Machine Learning Basics",
    "tags": "tech, HyperNetworks",
    "date": "2023-11-13 18:40:00 +0000",
    





    
    "snippet": "Introduction[Paper]: HyperNetworks  The following part has not been finished yet.Application in QMIXIllustration from the corresponding paper.The following statements from the paper are key to unde...",
    "content": "Introduction[Paper]: HyperNetworks  The following part has not been finished yet.Application in QMIXIllustration from the corresponding paper.The following statements from the paper are key to understand this application:  â€œThe weights of the mixing network are produced by separate hypernetworks. Each hypernetwork takes the state s as input and generates the weights of one layer of the mixing network. Each hypernetwork consists of a single linear layer, followed by an absolute activation function, to ensure that the mixing network weights are non-negative. The output of the hypernetwork is then a vector, which is reshaped into a matrix of appropriate size. The biases are produced in the same manner but are not restricted to being non-negative. The final bias is produced by a 2 layer hypernetwork with a ReLU non-linearity.â€  â€œQMIX relies on a neural network to transform the centralised state into the weights of another neural network, in a manner reminiscent of hypernetworks (Ha et al., 2017). This second neural network is constrained to be monotonic with respect to its inputs by keeping its weights positive.â€  â€œThe state is used by the hypernetworks rather than being passed directly into the mixing network because Qtot is allowed to depend on the extra state information in non-monotonic ways. Thus, it would be overly constraining to pass some function of s through the monotonic network alongside the per-agent values. Instead, the use of hypernetworks makes it possible to condition the weights of the monotonic network on s in an arbitrary way, thus integrating the full state s into the joint action-value estimates as flexibly as possible.â€So in my understanding, the map of $Q^i,\\forall i$ to $Q_{tot}$ should be monotonic and be dependent on the current state. If the current state is inputed directly into the net then it is monotonic, too. And this is not what we want."
  },
  
  {
    "title": "Decision Transformers",
    "url": "/posts/Decision-Transformers/",
    "categories": "Artificial Intelligence, Machine Learning Basics",
    "tags": "tech, transformer, generalist",
    "date": "2023-11-11 18:40:00 +0000",
    





    
    "snippet": "Decision Transformer  Paper: Decision Transformer: Reinforcement Learning via Sequence Modeling - NeurIPS 2021  [Website][Code]Illustration from the corresponding paper.Illustration from the corres...",
    "content": "Decision Transformer  Paper: Decision Transformer: Reinforcement Learning via Sequence Modeling - NeurIPS 2021  [Website][Code]Illustration from the corresponding paper.Illustration from the corresponding paper. Decision Transformer Pseudocode for continuous actions.The following statements from the paper are key to understand this model:  â€œUnlike prior approaches to RL that fit value functions or compute policy gradients, Decision Transformer simply outputs the optimal actions by leveraging a causally masked Transformer. By conditioning an autoregressive model on the desired return (reward), past states, and actions, our Decision Transformer model can generate future actions that achieve the desired return. Despite its simplicity, Decision Transformer matches or exceeds the performance of state-of-the-art model-free offline RL baselines on Atari, OpenAI Gym, and Key-to-Door tasks.â€  â€œStates, actions, and returns are fed into modality- specific linear embeddings and a positional episodic timestep encoding is added. Tokens are fed into a GPT architecture which predicts actions autoregressively using a causal self-attention mask.â€  â€œWe will train transformer models on collected experience using a sequence modeling objective.â€In my understanding, training a decision transformer is a supervised learning. By using offline data, the model is trained to anticipate an action that shares the same pattern in the offline data, given the current history.Trajectory Transformer  Paper: Offline Reinforcement Learning as One Big Sequence Modeling Problem - NeurIPS 2021  [Website][Code][Blog]A Generalist Agent: Gato  Paper: A Generalist Agent - Transactions on Machine Learning Research 2022Illustration from the corresponding paperIllustration from the corresponding paper. Training phase of Gato.Illustration from the corresponding paper. Running Gato as a control policy.The following statements from the paper are key to understand this work:  â€œThe same network with the same weights can play Atari, caption images, chat, stack blocks with a real robot arm and much more, deciding based on its context whether to output text, joint torques, button presses, or other tokens.â€  â€œGato was trained on 604 distinct tasks with varying modalities, observations and action specifications.â€  â€œGato consumes a sequence of interleaved tokenized observations, separator tokens, and previously sampled actions to produce the next action in standard autoregressive manner. The new action is applied to the environment a game console in this illustration, a new set of observations is obtained, and the process repeats.â€In my understanding, Gato is a decision transformer trained on various tasks using offline data. In different tasks, the tokenization is different but the core of the network shares the same one.Multi-Game Decision Transformer  Paper: Multi-Game Decision Transformers - NeurIPS 2022  [Website][Code][Blog]Illustration from the corresponding paper. An overview of the training and evaluation setup.The following statements from the paper are key to understand this work:  â€œWe observe expert-level game-play in the interactive setting after offline learning from trajectories ranging from beginner to expert.â€"
  },
  
  {
    "title": "Music Theory",
    "url": "/posts/Music-Theory/",
    "categories": "Music",
    "tags": "life, music",
    "date": "2023-11-04 18:40:00 +0000",
    





    
    "snippet": "  The following part has not been finished yet.",
    "content": "  The following part has not been finished yet."
  },
  
  {
    "title": "TED | Efficiency Tips",
    "url": "/posts/TED-Efficiency-Tips/",
    "categories": "Efficiency",
    "tags": "life, efficiency, TED",
    "date": "2023-11-04 18:30:00 +0000",
    





    
    "snippet": "  This note will be consistently updated.Communication SkillsHow to speak so that people want to listen | Julian TreasureAvoid these habits  Gossip  Judging  Negativity  Complaining  Excuses  Exagg...",
    "content": "  This note will be consistently updated.Communication SkillsHow to speak so that people want to listen | Julian TreasureAvoid these habits  Gossip  Judging  Negativity  Complaining  Excuses  Exaggeration (Lying)  DogmatismPrinciples  Honesty: Be clear and straight.  Authenticity: Be yourself.  Integrity: Be your word.  Love: Wish them well.The way you say it  Humans prefer voices that are low (coming from chest, rather than from the nose or the throat). Because we associate depth with power and with authority.  We prefer voices rich, warm, smooth.  You can train to get there (coach, posture, breathing, exercises).  Change tones, to communicate meanings. Dont use wrong tonality and make it reflect what you are trying to communicate.  Pace          Silence. Thereâ€™s nothing wrong with a bit of silence. We donâ€™t have to fill it with â€œumâ€s and â€œahâ€s.        Pitch: How high.  Volume: Loud or quiet.Vocal warm-up exercises  Arms up, deep breath in while sayingâ€Ahhhhâ€: Breath out as arms go down  Warm up lips â€œbah, bah, bahâ€¦â€  Lips coming alive â€œbrrrrâ€ like kids  Tongue exaggerated â€œla, la, la, laâ€  â€œRrrrrrrrrâ€ like champagne for the tongue  Siren high to low â€œoooweeeeaaawâ€7 Ways to Make a Conversation With Anyone | Malavika Varadan  The first word flood gates. Just say it. â€œHiâ€ with a big smile.  Skip the small talk. Ask really personal questions. Just ask something unique and personal.          â€œAn interesting name. How did you parents think of it? Is there a story behind it?â€      â€œHow long have you lived in this city? Do you remember the first day you landed here?â€      â€œWhere do you come from? Where do you family live?â€        Find the â€œMee-Tooâ€s.  Pay a unique compliment. The people will forget what you do, and they forget what you say, but they never forget how you make them feel."
  },
  
  {
    "title": "Set",
    "url": "/posts/Set/",
    "categories": "Mathematics",
    "tags": "tech, math",
    "date": "2023-11-02 18:40:00 +0000",
    





    
    "snippet": "  This note will be consistently updated. Related fields: Real Analysis, General Topology, Geometry.Supremum &amp; InfimumThe supremum of a nonempty set $X \\subset \\mathbb{R}$ is the smallest scala...",
    "content": "  This note will be consistently updated. Related fields: Real Analysis, General Topology, Geometry.Supremum &amp; InfimumThe supremum of a nonempty set $X \\subset \\mathbb{R}$ is the smallest scalar $y$ such that\\[y \\geq x \\text { for all } x \\in X.\\]The infimum of a set $X \\subset \\mathbb{R}$ is the largest scalar $y$ such that\\[y \\leq x \\text { for all } x \\in X.\\]If $\\sup X \\in X(\\inf X \\in X)$, then $\\sup X=\\max X(\\inf X=\\min X)$.Example:  $\\sup \\set{1 / n: n \\geq 1}=\\max \\set{1 / n: n \\geq 1}=1,$  $\\inf \\set{1 / n: n \\geq 1}=0 .$  Closed set  Bounded set  Compact set  CompleteFor $\\epsilon&gt;0$ and $x \\in \\mathbb{R}^n$ we define $B_\\epsilon(x)=\\set{y \\in \\mathbb{R}^n:|x-y|&lt;\\epsilon}$ to be open ball with radius $\\epsilon$ and center $x.$ Next, we collect further properties and terminologies for sets:  A set $X \\subset \\mathbb{R}^n$ is called open if for every $x \\in X$ there exists $\\epsilon&gt;0$ such that $B_\\epsilon(x) \\subset X.$  A set $X \\subset \\mathbb{R}^n$ is closed if $\\mathbb{R}^n \\backslash X$ is open. Alternatively, we can define closedness of set as follows: For every sequence $\\left(x^k\\right)$ with $x^k \\in X$ for all $k$ and $x^k \\rightarrow x$, we have $x \\in X.$  A set $X \\subset \\mathbb{R}^n$ is bounded if there exists $B \\in \\mathbb{R}$ with $|x| \\leq B$ for all $x \\in X$.  A bounded and closed set is called compact.  The set ${(ğ‘¥,ğ‘¦)\\mid ğ‘¥^2+ğ‘¦^2&lt;1}$ is bounded but not closed.  The set ${(ğ‘¥,ğ‘¦)\\mid ğ‘¥\\ge0}$ is closed but not bounded.  The following part has not been finished yet."
  },
  
  {
    "title": "Convergence Analysis of Gradient Descent",
    "url": "/posts/Convergence-Gradient-Descent/",
    "categories": "Mathematics",
    "tags": "tech, math, gradient descent, convergence",
    "date": "2023-10-21 18:40:00 +0000",
    





    
    "snippet": "  The following part has not been finished yet.Gradient DescentThe goalWe want to solve this unconstrained minimization problem\\[\\min _x f(x) \\quad \\text { s.t. } \\quad x \\in \\mathbb{R}^n .\\]",
    "content": "  The following part has not been finished yet.Gradient DescentThe goalWe want to solve this unconstrained minimization problem\\[\\min _x f(x) \\quad \\text { s.t. } \\quad x \\in \\mathbb{R}^n .\\]"
  },
  
  {
    "title": "Contraction Mapping Theorem",
    "url": "/posts/Contraction/",
    "categories": "Mathematics",
    "tags": "tech, math, contraction, convergence",
    "date": "2023-10-19 18:40:00 +0000",
    





    
    "snippet": "Metric SpaceDefinition of metric space  Definition. A metric space is an ordered pair $(M, d)$ where $M$ is a set and $d$ is a metric on $M$, i.e., a function $d: M\\times M \\to \\mathbb{R}$ satisfyi...",
    "content": "Metric SpaceDefinition of metric space  Definition. A metric space is an ordered pair $(M, d)$ where $M$ is a set and $d$ is a metric on $M$, i.e., a function $d: M\\times M \\to \\mathbb{R}$ satisfying the following axioms for all points $x, y, z \\in M:$      The distance from a point to itself is zero: $d(x,x) = 0.$    (Positivity) The distance between two distinct points is always positive: If $x\\ne y,$ then $d(x,y)&gt;0.$    (Symmetry) The distance from x to y is always the same as the distance from y to x: $d(x,y) = d(y,x)$    The triangle inequality holds: $d(x,z)\\le d(x,y)+d(y,z).$  Note that $0 = d(x,x)\\le d(x,y)+d(y,x),$ so the second axiom can be weakened to â€œIf $x\\ne y,$ then $d(x,y)\\ne 0$â€ and combined with the first axiom to â€œ$d(x,y) = 0 \\Leftrightarrow x = y.$â€Cauchy sequence  Definition.Let $\\set{x_t}_{t}^\\infty$ be a sequence in a metric space $(M,d),$ then it is a Cauchy sequence, if for every positive real number $\\epsilon&gt;0,$ there is a positive integer $N$ such that for all positive integers $m,n &gt; N,$ the distance $d(x_m, x_n) &lt; \\epsilon.$Symbolically, this is:\\[\\forall \\epsilon &gt; 0 (\\exists N\\in\\mathbb{N} (\\forall m,n\\in \\mathbb{N}(m,n\\ge N \\Rightarrow d(x_m,x_n)&lt; \\epsilon))).\\]Convergent sequence  Definition.Let $\\set{x_t}_{t}^\\infty$ be a sequence in a metric space $(M,d),$ then it is a convergent sequence, if there is a $x\\in M$ such that for every positive real number $\\epsilon&gt;0,$ there is a positive integer $N$ such that for all positive integer $n\\ge N,$ the distance $d(x_n, x) &lt; \\epsilon.$  Definition.A point $x$ of the metric space $(M, d)$ is the limit of the sequence $(x_n)$ if: For each $0&lt;\\epsilon\\in\\mathbb{R},$ there is $N\\in\\mathbb{N}$ such that, for every $N\\le n\\in \\mathbb{N},$ we have $d(x_n, x)&lt;\\epsilon.$Symbolically, this is:\\[\\forall \\epsilon &gt; 0 (\\exists N\\in\\mathbb{N} (\\forall n\\in \\mathbb{N}(n\\ge N \\Rightarrow d(x_n,x)&lt; \\epsilon))).\\]Completeness  Definition.A metric space $(M,d)$ is called complete, if every Cauchy sequence in it converges to an element of $M.$Examples:  $\\mathbb{R}$ with usual distance is complete;  $\\mathbb{Q}$ (the set of rational numbers) with usual distance is not complete. Because a sequence of rational numbers can converge to an irrational number, e.g. $\\pi.$Contraction Mapping  Definition.A contraction mapping on a metric space $(M,d)$ is a function $f:M\\to M$ with the property that there is some real number $0\\le k&lt; 1$ such that for all $x$ and $y$ in $M,$\\[d(f(x),f(y))\\le k\\cdot d(x,y).\\]Fixed Point  Definition.Let $(M,d)$ be a metric space and $f:M\\to M$ be a function. If $x\\in M$ and $x=f(x),$ then $x$ is called a fixed point of $f.$Contraction Mapping â†’ Cauchy SequenceLemma  Lemma.Let $(M,d)$ be a metric space, $f:M\\to M$ be a function, $x_0\\in M$, and $x_{t+1} = f(x_t), t=0,1,\\ldots,$ then $\\set{x_t}_{t}^\\infty$ is a Cauchy sequence.ProofSince $f$ is a contraction mapping, there must exist a $\\beta \\in (0,1)$ such that\\[d(x_2, x_1) = d(f(x_1), f(x_0)) \\leq \\beta d(x_1, x_0)\\]and\\[d(x_3, x_2) \\leq \\beta d(x_2, x_1) \\leq \\beta^2 d(x_1, x_0), \\cdots.\\]Generally, we have\\[d(x_{t+1}, x_t) \\leq \\beta d(x_t, x_{t-1}) \\leq \\cdots \\leq \\beta^t d(x_1, x_0).\\]Let $N$ be a positive integer. Let positive integers $m, n$ satisfy $m &gt; n \\geq N$. Then by the triangle inequality, we have\\[\\begin{aligned}d(x_m, x_n) &amp; \\leq d(x_m, x_{m-1}) + d(x_{m-1}, x_{m-2}) + \\cdots + d(x_{n+2}, x_{n+1}) + d(x_{n+1}, x_n) \\\\&amp; \\leq \\left[\\beta^{m-1} + \\beta^{m-2} + \\cdots + \\beta^{n+1} + \\beta^n\\right] d(x_1, x_0) \\\\&amp; = \\beta^n \\left[\\beta^{m-1-n} + \\beta^{m-2-n} + \\cdots + \\beta^1 + 1\\right] d(x_1, x_0) \\\\&amp; = \\beta^n \\frac{1-\\beta^{m-n}}{1-\\beta} d(x_1, x_0) \\\\&amp; = \\frac{\\beta^n - \\beta^m}{1-\\beta} d(x_1, x_0) \\\\&amp; \\leq \\frac{\\beta^N}{1-\\beta} d(x_1, x_0) .\\end{aligned}\\]Let $\\epsilon &gt; 0$. If $d(x_1, x_0) &gt; 0$ (the case of $d(x_1, x_0) = 0$ will be discussed later), then for a positive integer $N$ satisfying\\[N &gt; \\frac{\\ln \\frac{(1-\\beta) \\epsilon}{d(x_1, x_0)}}{\\ln \\beta},\\]we have for any $m, n \\geq N$, that $d(x_m, x_n) \\leq \\frac{\\beta^N}{1-\\beta} d(x_1, x_0) &lt; \\epsilon$, hence $\\set{x_t}_{t=0}^{\\infty}$ is a Cauchy sequence.If $d(x_1, x_0) = 0$, then $\\set{x_t}_{t=0}^{\\infty}$ is a constant sequence, and therefore also a Cauchy sequence. $\\blacksquare$Contraction Mapping TheoremTheorem  Theorem. Let $(M, d)$ be a complete metric space, and $f: M \\to M$ be a contraction mapping. Then $f$ has a unique fixed point $x^* \\in M$. Furthermore, starting from any point $x_0 \\in M$, the sequence $\\left(x_0, f(x_0), f(f(x_0)), \\cdots\\right)$ converges to $x^*$.ProofUniqueness: First, letâ€™s prove that if a point is a fixed point of $f$, then it is the only fixed point of $f$. Suppose $x_1, x_2 \\in M$ and $x_1=f(x_1)$, $x_2=f(x_2)$. Since $f$ is a contraction mapping, there exists a $\\beta \\in (0, 1)$ such that\\[d(x_1, x_2) = d(f(x_1), f(x_2)) \\leq \\beta d(x_1, x_2).\\]The above inequality holds only when $d(x_1, x_2) = 0$, i.e., $x_1 = x_2$.Convergence: Since $M$ is complete, there exists a point $x^* \\in M$ such that the sequence $\\set{x_t}$ converges to $x^*$.Fixed Point: Let $\\epsilon &gt; 0$. Since $\\set{x_t}$ is a Cauchy sequence and converges to $x^*$, there exists a positive integer $T$ such that for any $t \\geq T$, we have $d(x^*, x_t) &lt; \\frac{\\epsilon}{3}$ and $d(x_{t+1}, x_t) &lt; \\frac{\\epsilon}{3}$. For $t \\geq T$, by the triangle inequality, we have\\[d(x^*, f(x^*)) \\leq d(x^*, x_t) + d(x_t, f(x_t)) + d(f(x_t), f(x^*)).\\]Note that the middle term on the right-hand side is equal to $d(x_t, x_{t+1})$. Hence, the first two terms on the right-hand side are both less than $\\frac{\\epsilon}{3}$ by our earlier discussion. Now, consider the third term. We have\\[d(f(x_t), f(x^*)) \\leq \\beta d(x_t, x^*) &lt; \\frac{\\epsilon}{3}.\\]Thus, $d(x^*, f(x^*)) &lt; \\epsilon$. Since $\\epsilon$ was arbitrary, we have $d(x^*, f(x^*)) = 0$, i.e., $x^* = f(x^*)$. $\\blacksquare$"
  },
  
  {
    "title": "My Understanding and Appreciation of Batman",
    "url": "/posts/Batman/",
    "categories": "Literature",
    "tags": "life, literature, commics, Batman",
    "date": "2023-09-27 18:40:00 +0000",
    





    
    "snippet": "è™è ä¾ çš„æ‚²å‰§å†…æ ¸åœ¨äºï¼Œä»–å¿…é¡»æ¥å—è‡ªå·±æ— æ³•æ”¹å˜çš„å‘½è¿ï¼Œæ¥å—è‡ªå·±å½“æ—¶çš„æ— èƒ½ã€‚ çˆ¶æ¯åœ¨ä»–é¢å‰è¢«æªæ€ï¼Œè‡ªå·±å´ä»€ä¹ˆä¹Ÿåšä¸äº†ã€‚æ— è®ºä»–ä»¥åå˜å¾—å¤šå¼ºï¼Œä¸ç®¡ä»–å¸®åŠ©äº†å¤šå°‘äººé¿å…é‡åˆ°ç›¸åŒçš„äº‹æƒ…ï¼Œå·²å‘ç”Ÿçš„äº‹å†ä¹Ÿæ— æ³•æŒ½å›äº†ã€‚ä¸€ä¸ªæ¯”è¾ƒæ¿€è¿›çš„è§£è¯»æ–¹å¼æ˜¯ï¼Œè™è ä¾ æ˜¯å¸ƒé²æ–¯åœ¨çˆ¶æ¯é‡å®³ä¹‹åæƒ³è±¡å‡ºæ¥çš„ä¸€ä¸ªæ›´å¼ºå¤§çš„è‡ªå·±ï¼Œå“¥è°­é‡Œçš„æ‰€æœ‰åæ´¾éƒ½æ˜¯è‡ªå·±çš„å¿ƒç†éšœç¢ï¼Œå¸ƒé²æ–¯çš„ä½™ç”Ÿéƒ½åœ¨ä¸è‡ªå·±çš„å¿ƒç†éšœç¢ä½œæ–—äº‰ã€‚åœ¨è¯´è™è ä¾ ä¹‹å‰ï¼Œå…ˆè¯´å¦ä¸€ä¸ªåˆ°å¤„æ³„æ„¤çš„ç–¯...",
    "content": "è™è ä¾ çš„æ‚²å‰§å†…æ ¸åœ¨äºï¼Œä»–å¿…é¡»æ¥å—è‡ªå·±æ— æ³•æ”¹å˜çš„å‘½è¿ï¼Œæ¥å—è‡ªå·±å½“æ—¶çš„æ— èƒ½ã€‚ çˆ¶æ¯åœ¨ä»–é¢å‰è¢«æªæ€ï¼Œè‡ªå·±å´ä»€ä¹ˆä¹Ÿåšä¸äº†ã€‚æ— è®ºä»–ä»¥åå˜å¾—å¤šå¼ºï¼Œä¸ç®¡ä»–å¸®åŠ©äº†å¤šå°‘äººé¿å…é‡åˆ°ç›¸åŒçš„äº‹æƒ…ï¼Œå·²å‘ç”Ÿçš„äº‹å†ä¹Ÿæ— æ³•æŒ½å›äº†ã€‚ä¸€ä¸ªæ¯”è¾ƒæ¿€è¿›çš„è§£è¯»æ–¹å¼æ˜¯ï¼Œè™è ä¾ æ˜¯å¸ƒé²æ–¯åœ¨çˆ¶æ¯é‡å®³ä¹‹åæƒ³è±¡å‡ºæ¥çš„ä¸€ä¸ªæ›´å¼ºå¤§çš„è‡ªå·±ï¼Œå“¥è°­é‡Œçš„æ‰€æœ‰åæ´¾éƒ½æ˜¯è‡ªå·±çš„å¿ƒç†éšœç¢ï¼Œå¸ƒé²æ–¯çš„ä½™ç”Ÿéƒ½åœ¨ä¸è‡ªå·±çš„å¿ƒç†éšœç¢ä½œæ–—äº‰ã€‚åœ¨è¯´è™è ä¾ ä¹‹å‰ï¼Œå…ˆè¯´å¦ä¸€ä¸ªåˆ°å¤„æ³„æ„¤çš„ç–¯å­ï¼šå°ä¸‘ã€‚ è¿™ä¸ªè§’è‰²çš„å†…æ ¸æ˜¯ï¼šä¸å…¶å†…è€—æŠ˜ç£¨è‡ªå·±ï¼Œä¸å¦‚å‘ç–¯æŠ˜ç£¨ä»–äººã€‚å¾ˆå¤šäººåº”è¯¥éƒ½æœ‰è¿‡è¿™æ ·çš„ä½“éªŒï¼Œé‡åˆ°äº‹æƒ…ä¼šåæ€æ˜¯ä¸æ˜¯è‡ªå·±ä¸å¯¹ï¼Œä½†æ˜¯æ€ä¹ˆæƒ³éƒ½æ²¡åŠæ³•æƒ³æ¸…æ¥šï¼Œæ€ä¹ˆéƒ½è§‰å¾—ä¸å¯¹åŠ²ã€‚å› ä¸ºå¾ˆå¤šæ—¶å€™è¿™å°±ä¸æ˜¯ä½ çš„é”™ã€‚è‡ªå¤ä»¥æ¥çš„å„ç§ç¤¾ä¼šäº‹ä»¶ä¹Ÿè¡¨æ˜ï¼Œåœ¨è¢«å‘¨å›´äººpuaã€åœ¨è¢«å‘¨å›´äººä¸è¦è„¸åœ°å¾—å¯¸è¿›å°ºåœ°å‰¥å‰Šçš„æ—¶å€™ï¼Œä¼¼ä¹åªæœ‰å‘ç–¯æ‰èƒ½å¾—åˆ°å°Šé‡ã€‚åŒ¹å¤«ä¸€æ€’ï¼Œè¡€æº…äº”æ­¥ï¼Œæˆ‘è¿å‘½éƒ½ä¸è¦äº†ï¼Œä½ æ€•è¿˜æ˜¯ä¸æ€•ï¼Ÿç°åœ¨ä½ æ„¿æ„å¬äº†å—ï¼ŒæŠŠæˆ‘å½“å›äº‹äº†å—ï¼Ÿå°ä¸‘çš„æ„å¿—æ˜¯å¼ºå¤§çš„ï¼Œä»–çš„è¡Œä¸ºæ˜¯ä¸€ç§å¯»æ±‚è§£æ”¾çš„åæŠ—ï¼Œä»ç¤¾ä¼šçš„è§’åº¦æ¥è¯´ï¼Œä»–å¸Œæœ›å¾—åˆ°è¢«å°Šé‡æœºä¼šã€‚ä½†ä»–çš„è¡Œä¸ºæ˜¯æœ‰å®³çš„ï¼Œå¿ƒæ€æ˜¯æ‡¦å¼±çš„ã€‚ä»–ä¸åƒè™è ä¾ ï¼Œä»–ä»æ¥ä¸ä¼šå»æ‰¿è®¤è‡ªå·±çš„æ‚²å‰§çš„èµ·æºï¼Œæ¯”å¦‚ã€Šé»‘æš—éª‘å£«ã€‹ä¸­ï¼Œä»–è‡ªå·±å°±å£è¿°è¿‡éå¸¸å¤šçš„èµ·æºï¼Œä»–çš„ç²¾ç¥æ˜¯æŠ½è±¡çš„ï¼Œä»–ä¸å¸Œæœ›åˆ«äººè§‰å¾—ä»–æ˜¯å› ä¸ºæŸä¸ªå…·ä½“çš„åŸå› æ‰å •è½ï¼Œè€Œæ˜¯å¸Œæœ›è®©äººæ„è¯†åˆ°å°±ç®—ä¸é‡åˆ°è¿™ä»¶äº‹ï¼Œä¹Ÿä¼šé‡åˆ°å¦ä¸€ä»¶äº‹ï¼Œå› ä¸ºè¿™ä¸ªä¸–ç•Œå°±æ˜¯è’è¯çš„ã€‚ä»–åœ¨å¯»æ±‚ç»å†ä¸Šçš„è¶…è¶Šæ€§ï¼Œæˆ–è€…æ˜¯æ—¶é—´ä¸Šçš„è¶…è¶Šæ€§ï¼Œè¿™æ— å¯åšéã€‚è€Œä»–çš„æ‡¦å¼±å´åœ¨äºï¼Œä»–ä¼šå»å‘å‘¨å›´äººå»è¯æ˜è¿™ä¸ªä¸–ç•Œæ˜¯ç–¯ç‹‚çš„ï¼Œè¯•å›¾è®©åˆ«äººè®¤è¯†åˆ°ä¸–ç•Œçš„è’è¯ï¼Œä»è€Œè®©è‡ªå·±å¥½å—äº›ï¼šä½ çœ‹å§ï¼Œè¿™ä¸ªä¸–ç•Œå°±æ˜¯è¿™æ ·çƒ‚ï¼Œå’Œæˆ‘ä¸€èµ·å‘ç–¯å§ã€‚å›´ç»•å°ä¸‘è¿™ä¸ªè§’è‰²çš„è§£è¯´æœ‰å¾ˆå¤šï¼Œç‰¹åˆ«æ˜¯ã€Šè‡´å‘½ç©ç¬‘ã€‹ä¸­å°ä¸‘è¯´è‡ªå·±å’Œè™è ä¾ æ˜¯ç¡¬å¸çš„ä¸¤é¢ï¼Œå¯ä»¥å…±äº«åŒä¸€ä¸ªç²¾ç¥ç—…ç—…æˆ¿ã€‚ä½†æ˜¯è™è ä¾ ä½œä¸ºç–¯å­ï¼Œå´åšäº†æ­£ç¡®çš„äº‹ï¼Œä»–æ­£ç¡®å¼•å¯¼äº†è‡ªå·±çš„ç²¾åŠ›ï¼ˆä¹Ÿè®¸æ–¹æ³•æœ‰å¾…æ”¹è¿›ï¼‰ã€‚å°ä¸‘å½“ç„¶è§‰å¾—è™è ä¾ æœ‰æ„æ€ï¼Œè¿™ä¸ªäººå’Œä»–ä¸€æ ·æ˜¯è®¤è¯†åˆ°ä¸–ç•Œçš„ï¼Œä½†æ˜¯ä»–ä¸ºä»€ä¹ˆè¦å»åšå¥½äº‹ï¼Ÿä»–ä¹Ÿè¦å‘è™è ä¾ è¯æ˜ï¼Œæˆ–è€…æ˜¯æƒ³è®©è‡ªå·±ç›¸ä¿¡ï¼Œè™è ä¾ å’Œè‡ªå·±ä¸€æ ·çƒ‚ã€‚æ¯”å¦‚ã€Šé»‘æš—éª‘å£«ã€‹ä¸­çš„ç‚¸èˆ¹è¡¨æ¼”ï¼Œä»–å°±æ˜¯æƒ³è¯æ˜æœ‰å¾ˆå¤šæ‚²æƒ¨çš„å‘½è¿æ˜¯å®¢è§‚è§„å¾‹ï¼Œä»¥åŠé€¼ç–¯å“ˆç»´ä¸¹ç‰¹ï¼šæ··ä¹±æ˜¯æœ‰å¾ˆå¤§çš„å¼•åŠ›çš„ï¼Œè®©ä¸€ä¸ªäººå‘ç–¯ä½ åªéœ€è¦è½»è½»ä¸€æ¨å°±è¡Œäº†ã€‚å¦å¤–ä¸€ä¸ªä¾‹å­æ˜¯ã€Šè‡´å‘½ç©ç¬‘ã€‹ä¸­çš„æŠ˜ç£¨å’¯å™”å±€é•¿ï¼Œä½†æ˜¯å±€é•¿å¾ˆåšå¼ºã€‚è¿˜æ˜¯ç‚¸èˆ¹çš„ä¾‹å­æ›´å¥½ä¸€ç‚¹ï¼ŒæŠ˜ç£¨å’¯å™”å±€é•¿æ˜¯è‡ªå·±ä¸Šåœºç»™åˆ«äººåŠ éº»çƒ¦ï¼Œæ¡£æ¬¡å¤ªä½äº†ã€‚ã€‚ä¸è¿‡ç”µå½±ä¸ºäº†è‰ºæœ¯æ€§ç›´æ¥è®©ä¸¤æ–¹éƒ½è¡¨ç°å‡ºäº†å–„è‰¯çš„ä¸€é¢ï¼Œè¿™ä¸ªå‘å±•ä¹Ÿå¤ªè‹ç™½äº†ç‚¹ã€‚å…¶å®ä»æˆ‘çš„ä¸“ä¸šçš„è§’åº¦æ¥è¯´ï¼Œå°ä¸‘åœ¨ä¸€å®šç¨‹åº¦ä¸Šæ˜¯å¯¹çš„ï¼Œç¡®å®æœ‰å¾ˆå¤šè¿™æ ·æ— æ³•åæŠ—çš„è’è¯çš„ç¤¾ä¼šç°è±¡ï¼Œè¿™è¢«ç§°ä¸ºæ˜¯ç¤¾ä¼šå›°å¢ƒï¼ˆSocial Dilemmaï¼‰ï¼Œæ„æ€æ˜¯æ¯ä¸ªäººéƒ½åŸºäºè‡ªå·±çš„æœ€ä¼˜å†³ç­–ï¼Œä½†æ˜¯å´å¯¼è‡´äº†ç¾¤ä½“çš„ç¤¾ä¼šæ•ˆç›Šå¾ˆä½ã€‚è¿™æ˜¯éå¸¸æœ‰ç ”ç©¶ä»·å€¼çš„ç»æµå­¦é¢†åŸŸã€‚ç„¶è€Œï¼Œè¿™äº›ç°è±¡è™½ç„¶å­˜åœ¨ï¼Œä½†ä¹Ÿæ˜¯æœ‰è§£çš„ï¼Œå°±æ˜¯æœºåˆ¶è®¾è®¡ï¼ˆMechanism Designï¼‰ï¼Œä½ å¯ä»¥æ”¹å˜æœºåˆ¶æ¥å¼•å¯¼æ¯ä¸ªäººåšè‡ªå·±æœ€ä¼˜çš„å†³ç­–åŒæ—¶å¯¼å‘å¥½çš„ç¤¾ä¼šæ•ˆç›Šã€‚å¦‚æœå°ä¸‘å¤šå­¦ä¸€äº›æ•°å­¦æ¨¡å‹ï¼Œä¹Ÿè®¸å¯ä»¥ä¸æ‰“æ‰“æ€æ€çš„ï¼Œç»™è™è ä¾ é‚®ç®±é‡Œå‘å‡ æœ¬ä¹¦å’Œè®ºæ–‡ï¼Œä»–ä»¬å°±å¯ä»¥ç”¨è®ºæ–‡æ¥battleäº†:)åœ¨2022å¹´å·¦å³ç››è¡Œçš„æ‘†çƒ‚æ–‡åŒ–å’Œå‘ç–¯æ–‡å­¦ï¼Œå°±æ˜¯è¶Šå‘ä¸°å¯Œçš„ç‰©è´¨æ¡ä»¶å’Œè¶Šå‘æš—æµæ¶ŒåŠ¨çš„ç¤¾ä¼šé—®é¢˜è¶Šæ¥è¶Šæ¿€èµ·äººä»¬å¯¹äºç”Ÿå‘½å’Œæƒå¨çš„åæ€çš„ç½‘ç»œç°è±¡ã€‚ã€Šå°ä¸‘2019ã€‹è¿™ä¸ªç”µå½±æƒ³è¡¨è¾¾çš„æ ¸å¿ƒå°±æ˜¯è¿™ä¸ªæ„æ€ï¼Œå¯æƒœç”µå½±é‡Œçš„å°ä¸‘ç”Ÿæ´»æ¡ä»¶è¿˜æ˜¯ä¸å¤Ÿæƒ¨ï¼Œä¹Ÿè¿˜æ˜¯å¤ªçŸ«æƒ…ï¼Œæ¿€ä¸èµ·å¤ªå¤§çš„å…±æƒ…ã€‚ç°å®ä¸­çš„é—®é¢˜æ—©å·²æ›´åŠ è’è¯äº†ã€‚æœ€è®©æˆ‘æƒŠè‰³çš„å…³äºå°ä¸‘çš„æå†™æ˜¯Batman V3ä¸­çš„ç¬‘è¯­è°œè¯­ä¹‹æˆ˜ï¼Œå°ä¸‘å·²ç»å¾ˆä¹…ç¬‘ä¸å‡ºæ¥äº†ã€‚æ€äº†ä¸€ä¸ªäººï¼šä¸€ä¸ªå­å¼¹æ’ä¸Šäº†å¦ä¸€ä¸ªäººçš„è„‘è¢‹ï¼Ÿä¸å¤Ÿå¥½ç¬‘ã€‚Tom Kingæœ¬äººè¡¨ç¤ºè¿‡ï¼Œä»–å†™è™è ä¾ æ•…äº‹å°±åƒæ˜¯åœ¨å†™è‡ªå·±çš„ç»å†ï¼Œè¯»åˆ°è¿™ä¸ªç‰‡æ®µæˆ‘æ‰æ˜¯çœŸçš„ä¿¡æœäº†ï¼Œè¿™æ®µæå†™å·²ç»æ˜¯è’è¯çš„å·…å³°ã€‚å°ä¸‘è§£æ„å¤ªä¹…ï¼Œå·²ç»è§‰å¾—æ— èŠäº†ã€‚å¦‚æœä½ æŠ‘éƒè¿‡ï¼Œè€Œä¸”çœŸçš„åˆ°äº†æƒ³æ­»çš„é‚£ç§åœ°æ­¥ï¼Œä½ åº”è¯¥ç»å†è¿‡å°ä¸‘çš„è¿™ç§é˜¶æ®µï¼šæˆ‘è¿å‘½éƒ½ä¸è¦äº†ï¼Œæˆ‘ä»€ä¹ˆéƒ½ä¸åœ¨ä¹ï¼Œä»»ä½•çš„æƒå¨å’Œè§„åˆ™å¯¹æˆ‘æ¥è¯´éƒ½æ˜¯ç‹—å±ä¸é€šï¼Œæˆ‘ä¹ŸåŒå€¦äº†å‘åˆ«äººè¡¨è¾¾æˆ‘çš„æƒ³æ³•ï¼ŒåŒå€¦äº†åˆ°å¤„å»æ¸¸è¯´ï¼Œæˆ‘éƒ½è¦æ­»äº†ï¼Œä½ ä»¬æ€ä¹ˆæƒ³å…³æˆ‘ä»€ä¹ˆäº‹ï¼Ÿï¼ˆæ›´ä½•å†µæˆ‘æœ€åœ¨ä¹çš„è™è ä¾ éƒ½ä¸ç†æˆ‘ï¼Œä»–è¿˜åœ¨åšå¥½äº‹ï¼Œç”Ÿæ°”ï¼‰å½“ç„¶å°ä¸‘å¹¶ä¸æ˜¯æŠ‘éƒï¼Œæ˜¯ç‹‚å¾—ä¸æ¥å—ä»»ä½•æ³•åˆ™ï¼Œæˆ‘ä¸¾è¿™ä¸ªä¾‹å­åªæ˜¯ä¸ºäº†æ–¹ä¾¿å…±æƒ…è¿™ç§å¿ƒæ€ã€‚è¿™æ—¶å€™è‡ªä½œèªæ˜çš„è°œè¯­äººå‡ºæ¥äº†ï¼šå“ˆï¼Œæˆ‘çœ‹å‡ºæ¥äº†ï¼Œå°ä¸‘ä½ è§‰å¾—æ— èŠäº†ï¼Œé‚£ä¹ˆè®©èªæ˜çš„æˆ‘æ¥å¸®ä½ è§£å†³å§ï¼ä½†æ˜¯ä»–å¹¶æ²¡æœ‰æ„è¯†åˆ°å°ä¸‘çœŸæ­£é¢ä¸´çš„é—®é¢˜ã€‚è°œè¯­äººçš„åšæ³•æ˜¯ï¼šç¼–ç»‡äº†ä¸€ä¸ªå€Ÿå£ï¼Œè¦æ‰“ä»—è¦æ€äººï¼Œè¦æŠŠè™è ä¾ ç‰µæ‰¯è¿›æ¥ä»¥æ­¤è®©å°ä¸‘ç‰µæ‰¯è¿›æ¥ï¼Œç„¶åæŠŠåŸå¸‚æ…å¾—å¤©ç¿»åœ°è¦†ï¼Œç„¶åæ•…æ„è®¾å±€æ€äº†é£ç­äººå…¨å®¶ï¼Œåˆ¶é€ å‡ºé£ç­äººè¿™ä¹ˆä¸€ä¸ªè¢«å‘½è¿æ‰“è´¥çš„å •è½äººï¼Œç„¶ååˆæ•…æ„ç®—æ¼ï¼Œæ•…æ„è®©è¿™ä¹ˆä¸ªä¸èµ·çœ¼çš„loseré£ç­äººç§°ä¸ºè‡ªå·±è¢«å°ä¸‘æ‰“è´¥çš„å…³é”®å› ç´ ã€‚å¥½ï¼åˆæ˜¯ä¸€ä¸ªè™è ä¾ æ‹¯æ•‘ä¸–ç•Œçš„æ•…äº‹ï¼æˆ‘è®¾è®¡äº†å…¨å±€ï¼Œæ‰€æœ‰çš„æ‰€æœ‰ï¼Œåªä¸ºåšä½ å°ä¸‘ä¸€ç¬‘ã€‚è®©æˆ‘çœ‹åˆ°ä½ åœ¨ç¬‘ï¼Œæˆ‘è§£å†³äº†è¿™ä¸ªé—®é¢˜ï¼ï¼ˆä¸è¿‡è°œè¯­äººç¡®å®æŒºç‰›çš„ï¼Œè¿é£ç­äººä¼šå§”å±ˆå½“å§åº•éƒ½ç®—åˆ°äº†ï¼‰é£ç­äººçš„å„¿å­è¢«è°œè¯­äººæ¯’æ€ä¸€ä¸ªå®Œå…¨è¢«å‘½è¿æ”¯é…çš„æ™®é€šäººï¼Œé™¤äº†è‹¦ç¬‘ä½œä¹è¿˜èƒ½åšä»€ä¹ˆï¼Œæƒ¨é£ç­äººçš„çœ¼ç¥å……æ»¡ä»‡æ¨ï¼Œä»–æ¥æŠ•é è°œè¯­äººæ˜¯æ¥å½“å§åº•çš„ï¼›è°œè¯­äººçš„è°œè¯­å…¶å®ä¹Ÿæš—ç¤ºä»–çŸ¥é“é£ç­äººæ˜¯å°ä¸‘çš„å§åº•ä½ çš„å¿è¾±è´Ÿé‡ä¹Ÿæ˜¯æˆ‘è®¡åˆ’çš„ä¸€ç¯ï¼Œæ­£æ˜¯æˆ‘è¦è®²çš„ç¬‘è¯çš„ç¬‘ç‚¹æ‰€åœ¨è°œè¯­äººæš—æ‹å°ä¸‘ï¼Œå°ä¸‘æš—æ‹è™è ä¾ ä½†æ˜¯è°œè¯­äººæ˜æ˜¾æ˜¯ä½ä¼°äº†å°ä¸‘çš„é—®é¢˜ï¼Œå°ä¸‘å·²ç»æ˜¯è§£æ„å¾—æ— èŠäº†ï¼Œè€Œä¸”ä¹Ÿæ ¹æœ¬è¯´æœä¸äº†è™è ä¾ ï¼Œä½ è¿™å†å¡«ä¸€ä¸ªtrivialçš„ä¾‹å­åˆæœ‰ä»€ä¹ˆç”¨ï¼Ÿä½†æ˜¯æ²¡æƒ³åˆ°çš„æ˜¯ï¼Œåè½¬æ¥äº†ï¼Œæœ€åè™è ä¾ å®åœ¨å¿ä¸ä½äº†ï¼šè°œè¯­äººåšäº†è¿™ä¸€åˆ‡ï¼Œæ€äº†é‚£ä¹ˆå¤šäººï¼Œåªæ˜¯ä¸ºäº†è®²ä¸€ä¸ªç¬‘è¯ï¼Ÿäºæ˜¯ä»–ä¸¾åˆ€æ€å‘äº†è°œè¯­äººã€‚å´è¢«å°ä¸‘æŒ¡ä¸‹æ¥äº†ã€‚è¿™ä¸‹å°ä¸‘ç¬‘äº†ï¼Œå› ä¸ºä»–çŸ¥é“è™è ä¾ çš„ååŠè¾ˆå­éƒ½æ— æ³•è‡ªæ°äº†ï¼Œä»–æ‰“ç ´äº†è‡ªå·±çš„å‡†åˆ™balabalaï¼Œå¯çˆ±çš„è™è ä¾ å˜¿å˜¿å˜¿ï¼ˆå°ä¸‘ç—´æ±‰è„¸ï¼‰ã€‚äº‹å®ä¹Ÿè¿˜çœŸæ˜¯å¦‚æ­¤ï¼Œå› ä¸ºæ•´ä¸ªç¬‘è¯è°œè¯­ä¹‹æˆ˜å…¶å®éƒ½æ˜¯è™è ä¾ çš„å£è¿°ï¼Œæ˜¯å¯¹çŒ«å¥³çš„è‡ªç™½ã€‚ä»–ä¸€ç›´æ”¾ä¸ä¸‹å¿ƒæ¥å‘çŒ«å¥³æ±‚å©šï¼Œå°±æ˜¯å› ä¸ºä»–è®¤ä¸ºè‡ªå·±å¹¶ä¸åƒäººä»¬æƒ³è±¡çš„é‚£ä¹ˆå¥½ï¼Œä»–çš„ä¸€ä¸ªå¿ƒç»“å°±æ˜¯è‡ªå·±å¤±æ§æƒ³æ€æ‰è°œè¯­äººï¼Œå› æ­¤ä»–è®¤ä¸ºè‡ªå·±å½»åº•å •è½äº†ï¼Œå˜å¾—å’Œå°ä¸‘ä¸€æ ·äº†ã€‚çœ‹çœ‹çŒ«å¥³æ˜¯æ€ä¹ˆå›åº”çš„ï¼šè¿™ä¸€å¹•æ˜¯ç¬‘è¯è°œè¯­ä¹‹æˆ˜çš„å¼€ç¯‡è¿™ä¸€å¹•æ˜¯ç¬‘è¯è°œè¯­ä¹‹æˆ˜çš„ç»“å°¾åœæ­¢æƒ³è±¡å°±å¥½:)è¯´å›è™è ä¾ ï¼Œè¿™ä¸ªäººç¡®å®æ˜¯ç–¯å­ã€‚Batman V3ä¸­å°±æœ‰éå¸¸å¤šçš„ç‰‡æ®µæ˜¯è‡ªå˜²ï¼Œä¸€ä¸ªæˆå¹´äººç©¿ç€è¯å‰§æœè£…åœ¨æ™šä¸Šæ‰“äººï¼Œä»¿ä½›æå¾—è¶Šç‹ è¶Šæœ‰ç”¨ã€‚ä»–ç¡®å®åœ¨æ³„æ„¤ï¼Œä½†ä»–ä¹Ÿç¡®å®æ˜¯åœ¨ä¸è¦å‘½åœ°åšå¥½äº‹ã€‚è¿™ä¸ªç–¯å­æ˜¯æœ‰éå¸¸ä¸¥é‡çš„è‡ªæˆ‘æ•‘èµå’Œè‡ªæˆ‘æ‘§æ¯çš„å€¾å‘çš„ã€‚ä»–æƒ³æˆ˜æ–—åˆ°æ­»ï¼ŒæŠŠè‡ªå·±çš„æœ€åä¸€ä¸ç²¾åŠ›å…¨éƒ¨è´¡çŒ®å‡ºæ¥ï¼Œè¿™æ ·æ‰èƒ½è®©è‡ªå·±å®‰å¿ƒã€‚è¿™å°±æ˜¯è™è ä¾ çš„è‡ªæ€æ–¹å¼ã€‚æ¼«ç”»ä¸­æœ‰éå¸¸å¤šçš„ç”»é¢æ˜¯è®¨è®ºgood deathï¼Œåœ¨è„‘å­é‡Œèµ°é©¬ç¯ä¼¼åœ°å’Œè‡ªå·±çš„çˆ¶æ¯å¯¹è¯ï¼Œæ¯”å¦‚åœ¨å’Œè´æ©æ‰“æ¶çš„é‚£ä¸ªç‰‡æ®µï¼ˆç¬¬20è¯ï¼‰ã€‚è¯´å®è¯ï¼Œæˆ‘å°±æ˜¯ä»è¿™ä¸ªæ–¹é¢å…¥å‘çš„è™è ä¾ ã€‚å¯¹æ¯”85è¯æœ€åçš„å…³äºè¿™ä¸€ç‚¹çš„æ–°çš„å‡åã€‚ã€‚è¿™é‡Œå¤šçš„ä¸æƒ³å†è¯´äº†ï¼Œç›´æ¥ä¸Šå›¾ã€‚20è¯å…‹è±å„¿æ˜¯ä¸ªè¶…äººç±»å¥³å­©ï¼Œæœ¬æ¥è™è ä¾ è§‰å¾—æ‰¾åˆ°äº†è¿™ä¸ªäººï¼Œè‡ªå·±å°±å¯ä»¥é€€ä¼‘äº†ï¼Œå“¥è°­å°±æœ‰äººç…§é¡¾äº†ã€‚ä½†æ˜¯å…‹è±å„¿è¢«åæ´¾è®¾å±€é€¼ç–¯äº†ã€‚ä»–éœ€è¦æ‰¾å¿ƒçµæµ·ç›—æ¥åŒ»æ²»å…‹è±å„¿ã€‚ä½†æ˜¯å¿ƒçµæµ·ç›—åœ¨è´æ©é‚£ï¼Œè´æ©éœ€è¦å¿ƒçµæµ·ç›—æ¥è®©è‡ªå·±è·å¾—å¿ƒçµçš„å¹³é™å’Œæ•‘èµã€‚æ‰€ä»¥è™è ä¾ å°±ä»è´æ©é‚£é‡ŒæŠŠå¿ƒçµæµ·ç›—æŠ¢è¿‡æ¥äº†ï¼Œç°åœ¨è´æ©æ¥åˆ°å“¥è°­ï¼Œæ‰¾è™è ä¾ æŠ¥ä»‡ã€‚â€œé‚£ä¸ªå¥³å­©éœ€è¦å¸®åŠ©ï¼Œå› æ­¤æˆ‘å¸®åŠ©äº†å¥¹ï¼Œä»…æ­¤è€Œå·²ã€‚â€æ–¹æ¡†æ˜¯è™è ä¾ è„‘ä¸­æµ®ç°çš„æ¯äº²çš„å£°éŸ³æ­¤æ—¶åœ¨å’Œè´æ©æ‰“æ¶ï¼Œæ‰“ä¸è¿‡æœ€å³è¾¹æ˜¯é—ªå›ï¼Œå°æ—¶å€™çˆ¶äº²è¢«æªæ€æœ€å³è¾¹æ˜¯é—ªå›ï¼Œå°æ—¶å€™æ¯äº²è¢«æªæ€å®‰è¯¦é“å¤´åŠŸç§’æ€ã€‚ã€‚è«åå…¶å¦™ã€‚ã€‚ä½†æ˜¯è¿™ä¸æ˜¯é‡ç‚¹85è¯ä¼šé£çš„è¿™ä¸ªå°±æ˜¯ä¸Šæ–‡æåˆ°çš„å…‹è±å„¿ï¼›æåˆ°çš„çˆ¶äº²æ˜¯æŒ‡çš„å¹³è¡Œå®‡å®™çš„ç‹—è¡€å‰§æƒ…ï¼Œä¸æƒ³æäº†è¯´è¯çš„æ˜¯è™è ä¾ ï¼Œåœ¨å’Œå€’éœ‰è›‹é£ç­äººå–é…’ï¼›è™è ä¾ çŸ¥é“é£ç­äººï¼Œé£ç­äººä¸çŸ¥é“æ˜¯è™è ä¾ è¿™æ˜¯é£ç­äººçŒ«å¥³å…¥åœºå®Œç»“æ’’èŠ±85è¯çš„æ”¶åœºå¾ˆè‰ç‡ï¼Œå†…å®¹ä¹Ÿå¾ˆç›´ç™½ã€‚å› ä¸ºé”€é‡ä¸å¥½Tom Kingè¢«èµ¶èµ°äº†ï¼Œè¿™æ˜¯ä»–çš„æœ€åä¸€è¯ã€‚ã€‚æˆ–è®¸å¾ˆå¤šäººä¸åƒè¿™ä¸€å¥—å§ã€‚å¦å¤–Batman V3ä¸­å¾ˆæœ‰æ„æ€çš„ä¸€è¯æ˜¯53è¯ï¼Œè™è ä¾ å› ä¸ºçŒ«å¥³é€ƒå©šï¼Œæƒ…ç»ªå¤±æ§æäº†äººï¼ŒæŠŠæ€¥å†»äººç»™æ‰“æ‡µäº†ï¼Œæš´åŠ›é€¼ä¾›ï¼ŒæŠŠæ€¥å†»äººé€ä¸Šäº†æ³•åº­ã€‚ç„¶åä¸ºäº†å¼¥è¡¥ï¼Œè‡ªå·±åˆä»¥å¸ƒé²æ–¯çš„èº«ä»½å‚åŠ é™ªå®¡å›¢ï¼Œå°±æ˜¯å¤šä¸ªäººå†³å®šæ€¥å†»äººæœ‰æ²¡æœ‰ç½ªçš„é‚£ä¸ªä¼šï¼Œç„¶åä»¥ä¸€å·±ä¹‹åŠ›è¯´æœäº†å…¶ä»–æ‰€æœ‰äººï¼Œæœ€ç»ˆåˆ¤æ€¥å†»äººæ— ç½ªï¼Œä¸Šæ¼”äº†ä¸€åœºã€ŠåäºŒæ€’æ±‰ã€‹çš„ç»å…¸å‰§æƒ…ã€‚å…¶ä¸­å¸ƒé²æ–¯ä¸ºè™è ä¾ å®šç½ªå¾ˆæœ‰æ„æ€ï¼Œè®¨è®ºäº†â€œä¸ºä»€ä¹ˆè™è ä¾ ä¸æ˜¯ç¥â€ï¼Œä¹Ÿæ˜¯ç›´æ¥ä¸Šå›¾ã€‚53è¯ç²¾å½©ä¹‹å¤„åœ¨äºä»–çš„æ¯ä¸€å¥è¯éƒ½æ˜¯å®è¯ã€‚å¸ƒé²æ–¯è‡ªå·±ä¹Ÿä¸¾æ‰‹äº†ï¼Œè¯´æ˜å¦‚æœä¸èµ°è™è ä¾ è¿™æ¡è·¯ï¼Œä»–ä¼šè‡ªæ€æ— æ³•åæŠ—çš„å‘½è¿ä»–çš„æ¯ä¸€å¥è¯éƒ½æ˜¯å®è¯â€œæˆ‘â€æ˜¯æœ‰æé™çš„è¿™é‡Œçš„æ„æ€æ˜¯è¦çŸ¥è¶³å’Œé¡ºä»å‘½è¿ï¼ŒV3ä¸­å¾ˆå¤šå¤„éƒ½é€éœ²å‡ºæ–¯å¤šè‘›ä¸»ä¹‰ã€‚è¿™äº›åœ°æ–¹æ˜¯æˆ‘ä¸èµåŒçš„ï¼Œæœ‰æ—¶å€™è¿˜æ˜¯ç¨å¾®å°ä¸‘ä¸€ç‚¹å¥½ã€‚ä¸è¿‡è¿™é‡Œæ”¾å¼•ç”¨åº”è¯¥æ˜¯åè®½å†è¯»çš„æ—¶å€™æ„è¯†åˆ°ï¼Œè¿™æ˜¯ä¸€ä¸ªå¾ˆä¸é”™çš„éšå–»ï¼Œä¸Šå¸=æ— æ³•åæŠ—çš„å‘½è¿ï¼Œå‘½è¿ä¸­æœ‰å¥½æœ‰åï¼Œä½ åªèƒ½å…¨ç›˜æ¥å—ã€‚è€Œï¼Œè™è ä¾ =äººç±»ç¤¾ä¼šï¼Œç¤¾ä¼šä¸Šæœ‰å¥½æœ‰åï¼Œä½†ä»–ç»ˆç©¶ä¸æ˜¯å‘½è¿ï¼Œäººæ˜¯ä¸€å®šç¨‹åº¦ä¸Šå¯ä»¥æ”¹å˜çš„ï¼Œä½ ä¸èƒ½è®¤ä¸ºäººç±»ç¤¾ä¼šå°±æ˜¯æ— æ³•åæŠ—çš„å‘½è¿ï¼Œä»–è¯´ä»€ä¹ˆå°±æ˜¯ä»€ä¹ˆï¼Œä»–æ‰“ä½ ä½ å°±åªèƒ½æ¥å—å®£åˆ¤ã€‚äººç±»ç¤¾ä¼šä¹Ÿæ˜¯ä¼šçŠ¯é”™çš„ï¼Œè€Œä½œä¸ºæˆ‘ä»¬æ¯ä¸ªäººï¼Œéœ€è¦æ•¢äºé¢å¯¹çœŸç›¸ï¼Œæ•¢äºç»™äººç±»ç¤¾ä¼šï¼ˆè™è ä¾ ï¼‰å®šç½ªã€‚æˆ‘ä»¬ä¸æ˜¯ç¥ä¹Ÿä¸æ˜¯è¶…äººï¼Œæˆ‘ä»¬æ— æ³•åˆ›é€ ã€æ›´æ”¹ã€åˆ é™¤æ³•åˆ™ï¼Œä½†æ˜¯æˆ‘ä»¬å¯ä»¥è®¤è¯†ã€åŒ…è£…æ³•åˆ™ï¼Œè®¾è®¡æ–°çš„æœºåˆ¶ä¸ºæˆ‘ä»¬æ‰€ç”¨ï¼Œè¿™å°±æ˜¯äººç±»çš„æé™ï¼Œä¹Ÿæ˜¯è™è ä¾ çš„è±¡å¾ã€‚æˆ‘ä»¬è¦åŒºåˆ†å‡ºä»€ä¹ˆæ˜¯æˆ‘ä»¬å¯ä»¥æ”¹å˜çš„ï¼Œä¸è¦å¤ªè¿‡äºæ‚²è§‚äº†ã€‚"
  },
  
  {
    "title": "A Note on Stochastic Processes",
    "url": "/posts/Stochastic-Processes/",
    "categories": "Mathematics",
    "tags": "tech, math, stochastic processes",
    "date": "2023-09-03 18:40:00 +0000",
    





    
    "snippet": "  This note partially uses the materials from the notes of MATH2750.Transition Matrix  The transition kernel $\\mathbf{M}$ is a square matrix of size $\\vert S\\vert \\times \\vert S\\vert$.  $\\mathbf{M}...",
    "content": "  This note partially uses the materials from the notes of MATH2750.Transition Matrix  The transition kernel $\\mathbf{M}$ is a square matrix of size $\\vert S\\vert \\times \\vert S\\vert$.  $\\mathbf{M}_{ij}$ means the probability of the current state $i$ transitioning to the next state $j$.  The exponent $k$ of the matrix power $\\mathrm{M}^k$ represents the state distribution after $k$ consecutive state transitions from the current state distribution.  $\\mathbf{M}_{ij}^k$ means the probability of the current state $i$ transitioning to the state $j$ after $k$ timesteps.Class StructureThere may be some â€œirrelevantâ€ states in a Markov chain. We can eliminate them to simplify the model.AccessibleThese statements are equivalent:  State $j$ is accessible from state $i;$  $i$ can transition to $j$ in $n$ steps;  Starting from $i$, thereâ€™s a positive chance that weâ€™ll get to $j$ at some point in the future;  $\\exists n, \\mathrm{s.t. }\\mathbf{M}_{ij}^n &gt; 0;$  $i \\to j.$Communicates withDefinitionThese statements are equivalent:  $i$ communicates with $j$;  $i\\to j$ and $j\\to i$;  $i \\leftrightarrow j$.Properties  Reflexive: $i\\leftrightarrow i, \\forall i.$  Symmetric: if $i \\leftrightarrow j$ then $j \\leftrightarrow i.$  Transitive: if $i \\leftrightarrow j$ and $j \\leftrightarrow k$ then $i \\leftrightarrow k.$Proof of propertiesReflexive$\\mathrm{M}_{ii}^{n=0}=1&gt;0$. In 0 step we stay where we are.SymmetricThe definition of $i\\leftrightarrow j$ is symmetric under swapping $i$ and $j$.Transitive  $i \\leftrightarrow j$ means $\\exists n_1\\ge 1$ s.t. $\\mathrm{M}_{ij}^{n_1}&gt;0.$  $j \\leftrightarrow k$ means $\\exists n_2\\ge 1$ s.t. $\\mathrm{M}_{jk}^{n_2}&gt;0.$\\[\\begin{aligned}&amp; P(X_n = k\\mid X_0 = i) \\\\=&amp; \\sum\\limits_{l,m\\in S} P(X_n = k, X_{n-n_2} = l, X_{n_1} = m\\mid X_0 = i) \\\\ =&amp; \\sum\\limits_{l,m\\in S} P(X_n = k \\mid X_{n-n_2} = l) \\cdot P(X_{n-n_2} = l \\mid X_{n_1} = m) \\cdot P(X_{n_1} = m\\mid X_0 = i) \\\\ \\ge&amp; P(X_n = k \\mid X_{n-n_2} = j) \\cdot P(X_{n-n_2} = j \\mid X_{n_1} = j) \\cdot P(X_{n_1} = j\\mid X_0 = i) \\\\=&amp; \\mathrm{M}_{ij}^{n_1} \\cdot \\mathrm{M}_{jj}^{n-n_1-n_2} \\cdot \\mathrm{M}_{jk}^{n_2} \\\\\\ge&amp; 0\\end{aligned}\\]Communicating classAll states $j$ that communicate with state $i$ are in the same equivalence class (or communicating class) as state $i.$Closed communicating classThese statements are equivalent:  Class $C\\subset S$ is closed if whenever there exist $i\\in C$ and $j\\in S$ with $i\\to j$, then $j\\in C$ also.  No state outside the class is accessible from any state within the class.  Once you enter a closed class, you canâ€™t leave that class.  If a state $j$ is out of a communicating class where state $i$ in (i.e. $i \\nleftrightarrow j$), it still may enter the communicating class (i.e. $j\\to i$ but $i\\nrightarrow j$).Open communicating classIf a communicating class is not closed, then it is open.Irreducible Markov chainDefinitionA Markov chain is irreducible if its entire state space $S$ is on communicating class.How can I tell if a Markov chain is irreducible?The mere presence of zeros in the transition matrix does not mean that the Markov chain is reducible. E.g.:\\[\\begin{bmatrix}0.5 &amp; 0.5 &amp; 0.0 \\\\0.0 &amp; 0.5 &amp; 0.5 \\\\0.5 &amp; 0.0 &amp; 0.5 \\\\\\end{bmatrix}\\]In my understanding, if a whole row (or a whole column) of a transition matrix is all zeros except for the pivot, then the Markov chain with this transition matrix is reducible, and the corresponding row (or column) state can be eliminated.E.g., this is such a transition matrix with which the Markov chain is reducible:\\[\\begin{bmatrix}0.5 &amp; 0.5 &amp; 0.0 \\\\0.4 &amp; 0.5 &amp; 0.1 \\\\0.0 &amp; 0.0 &amp; 1.0 \\\\\\end{bmatrix}\\]Absorbing stateThese statements are equivalent:  State $i$ is an absorbing state.  State $i$ is in a communicating class ${i}$ by itself and that class is closed.  Once you reach state $i$, you canâ€™t leave it.PeriodicityDefinitionPeriodicity describes a pattern of recurrence for a state. For a state $i$ in a Markov chain, its period $d(i)$ is defined as the greatest common divisor of the number of steps it can take to return to state $i$, excluding the possibility of 1-step self-transitions.More formally, consider all $n$ such that $\\mathbf{M}^n_{ii} &gt; 0$ (i.e., the probability of transitioning from state $i$ to state $i$ in $n$ steps is positive). The period $d(i)$ is the greatest common divisor of all such $n$.  If $d(i) = 1$, then state $i$ is said to be aperiodic.  If $d(i) &gt; 1$, then state $i$ is said to be periodic, with a period of $d(i)$.The transition kernel is a square matrix. The exponent $k$ of the matrix power $\\mathrm{M}^k$ represents the state distribution after $k$ consecutive state transitions from the current state distribution.ExamplesAll states in the Markov chain with the following transition matrix are periodic with a period of 3.\\[\\mathbf{M} = \\begin{bmatrix}0 &amp; 1 &amp; 0 \\\\0 &amp; 0 &amp; 1 \\\\1 &amp; 0 &amp; 0 \\\\\\end{bmatrix}\\]All states in the Markov chain with the following transition matrix are aperiodic.\\[\\mathbf{M} = \\begin{bmatrix}0.2 &amp; 0.4 &amp; 0.4 \\\\0.6 &amp; 0.2 &amp; 0.2 \\\\0.1 &amp; 0.8 &amp; 0.1 \\\\\\end{bmatrix}\\]  In my understanding, this constraint is very lenient; as long as the probability of transitioning to oneself is greater than 0, it counts.Hitting TimesHitting timeLet $(X_n)$ be a Markov chain on state space $S$. Let $H_{Sâ€™}$ be a random variable representing the hitting time to hit the set $Sâ€™\\subset S$, given by\\[H_{S'} = \\min \\{ n\\in \\{0, 1,2,\\ldots\\} : X_n \\in S' \\}\\]$H_{Sâ€™}$ means the first timestep that any state in $Sâ€™$ appears.If $Sâ€™$ contains only one state, i.e. $Sâ€™ = {i}$, then the $H_{i}$ means the first timesstep that state $i$ appears.$H_{Sâ€™} = \\infty$ if $X_n\\in Sâ€™$ for all $n,$ meaning that all the states in $Sâ€™$ will never appear.Hitting probabilityThe hitting probability $h_{iSâ€™}$ of the set $Sâ€™$ from the state $i$ is\\[\\begin{aligned}    h_{iS'}     =&amp; P(X_n\\in S'\\text{ for some }n\\ge n\\mid X_0 = i) \\\\    =&amp; P(H_{S'} &lt; \\infty \\mid X_0 = i)\\end{aligned}\\]It means that the probability of $Sâ€™$ will appear in finite timesteps, given the starting state $i.$If $Sâ€™$ contains only one state, i.e. $Sâ€™ = {i}$, then the $h_{ij}$ means the probability of $i$ transitioning to $j$ in finite timesteps.\\[h_{ij}     = P(H_j &lt; \\infty \\mid X_0 = i)\\]Expected hitting timeThe expected hitting time $\\eta_{iSâ€™}$ of the set $Sâ€™$ starting from state $i$ is\\[\\eta_{iS'} = \\mathbb{E}(H_{S'}\\mid X_0 = i)\\]$\\eta_{iSâ€™}$ can only be finite if $h_{iSâ€™} = 1.$Return timeFirst return timeGiven a state $i \\in S$ of a Markov chain, the first return time $T_i$ to state $i$ is defined as:\\[T_i = \\min \\{ n &gt; 0 : X_n = i | X_0 = i \\}\\]This is the number of time steps required to return to state $i$ for the first time, given that we start at $i$. Essentially, $T_i$ represents the first occurrence of the state $i$ after the initial time, assuming the chain started at state $i$.Expected return timeThe expected return time, $m_i$, to state $i$ is the average number of time steps it takes to return to state $i$ after initially starting at $i$. Mathematically, itâ€™s given by the expected value of the first return time $T_i$:\\[\\begin{aligned}m_i =&amp; \\mathbb{E}[T_i | X_0 = i] \\\\=&amp; \\sum\\limits_{n=1}^\\infty n\\cdot \\mathbf{M}^n_{ii}\\end{aligned}\\]If the Markov chain is guaranteed to return to state $i$ and does so in an average of a finite number of steps, then $m_s$ is finite. If the Markov chain can return to state $i$ but takes, on average, an infinite number of steps, then $m_i$ is infinite. If the chain is not guaranteed to return to state $i$, then the expected return time is undefined (though it is often treated as infinite for certain analyses).Recurrence and TransienceRecurrent and transient statesRecurrent stateA state $i \\in S$ is recurrent if and only if the probability of eventually returning to $i$ starting from $i$ is 1, that is:\\[P(T_i &lt; \\infty) = 1\\]The following statements are equivalent:  State $i$ is a recurrent state;  $P(T_i &lt; \\infty) = 1;$  $P(R_i = \\infty \\mid X_0 = i) = 1$          or $P(R_i&lt;\\infty \\mid X_0 = i) = 0$      The number of return of $i$ is infinite.      $R_i$ is the number of return of $i.$        $\\mathbb{E}(R_i \\mid X_0 = i) = \\infty$State $i$ is a recurrent state if and only if $\\sum\\limits_{n\\ge 1}\\mathbf{M}_{ii}^n = \\infty$Positive ecurrent state            If there exists a constant $m_i$ (expected return time) such that $\\mathbb{E}[T_i      X_0 = i] = m_i &lt; \\infty$. This means that the expected time to return to state $i$ is finite.        If the state space of a Markov chain is finite (has a finite number of states), then all its recurrent states are positive recurrent.  If the state space of an irreducible Markov chain is finite (has a finite number of states), then all its states are positive recurrent.Null recurrent stateIf $\\mathbb{E}[T_i | X_0 = i] = \\infty$. This means that the expected time to return to state $i$ is infinite.Transient StateA state $i \\in S$ is transient if and only if the probability of eventually returning to $i$ starting from $i$ is less than 1, that is:\\(P(T_i &lt; \\infty) &lt; 1\\)  Comparison of the two kinds of state:            Â       Recurrent states      Transient states                  Visiting behavior      If we ever visit $s$, then we keep returning to $s$ again and again      We might visit $s$ a few times, but eventually we leave $s$ and never come back              Expected number of visits      Starting from $s$, the expected number of visits to $s$ is infinite      Starting from $s$, the expected number of visits to $s$ is finite              Certainty of number of visits      Starting from $s$, the number of visits to $s$ is certain to be infinite      Starting from $s$, the number of visits to $s$ is certain to be finite              Return probability      The return probability $m_s$ equals $1$      The return probability $m_s$ is strictly less than $1$        Consider a Markov chain with transition matrix $\\mathbf{M}$.      If the state $s$  is recurrent, then $\\sum_{n=1}^{\\infty} \\mathbf{M}_{ss}(n) = \\infty,$ and we return to state $s$ infinitely many times with probability 1.    If the state $s$ is transient, then $\\sum_{n=1}^{\\infty} \\mathbf{M}_{ss}(n) &lt; \\infty,$ and we return to state $s$ infinitely many times with probability 0.  Recurrent and transient classesThese statements are equivalent:  Within a communicating class, either every state is transient or every state is recurrent.  Let $i,j\\in S$ be such that $i \\leftrightarrow j$. If $i$ is recurrent, then $j$ is recurrent also; while if $i$ is transient, then $j$ is transient also.If a Markov chain is irreducible (the entire state space is a single communicating class), we can refer to it as a â€œrecurrent Markov chainâ€ or a â€œtransient Markov chainâ€.  Every non-closed communicating class is transient.          Non-closed communicating class $Sâ€™$ means that there exists $i\\in Sâ€™$ such that $i\\to j$ and $j\\notin Sâ€™$, meaning that you can escape from $Sâ€™.$      Once you are escaped from $Sâ€™$, you will never come back. Beacause $i\\to j$ and $j\\notin Sâ€™ \\Rightarrow j\\nrightarrow i.$        Every finite closed communicating class is positive recurrent.  Infinite closed classes can be positive recurrent, null recurrent, or transient.Stationary DistributionsDefinition  The distribution is defined on the state set, representing the probability of each state occurring at a timestep.  E.g., a state set is ${s_1, s_2}$, then a distribution can be $(0.7, 0.3)$, meaning that $s_1, s_2$ will appear with the probability of $0.7, 0.3$ respectively.  If the current distribution is $\\mathbf{v}$, then the distribution at the next timestep is $\\mathbf{v} \\mathbf{M}$.If a distribution $\\mathbf{v} = \\mathbf{v} \\mathbf{M}$, then it is a stationary distribution.If a distribution $\\mathbf{v}_i = \\sum_j \\mathbf{v}_i \\cdot \\mathbf{M}_{ij}$, then it is a stationary distribution.A stationary distribution is a fixed point.Existence &amp; UniquenessConsider an irreducible Markov chain.  If the irreducible Markov chain is positive recurrent (i.e. the irreducible Markov chain is finite), then a stationary distribution $\\mathrm{v}$ exists, is unique, and is given by $\\mathrm{v}_i = 1/m_i$, where $m_i$ is the expected return time to state $i$.  If the Markov chain is null recurrent or transient, then no stationary distribution exists.Consider a Markov chain with multiple communicating classes.  If none of the classes are positive recurrent, then no stationary distribution exists.  If exactly one of the classes is positive recurrent (and therefore closed), then there exists a uniquestationary distribution, supported only on that closed class.  If more the one of the classes are positive recurrent, then many stationary distributions will exist.          It can be divided; calculate their respective stationary distributions, and then combine them together.      Proof of Existence            Let $X_n$ be the Markov chain, and let $\\pi_j$ be the probability that the chain is in state $j$ at time $n$ given it starts in state $i$ at time 0, i.e., $\\pi_j = P(X_n = j      X_0 = i)$.      Recall the definitions and basic concepts associated with positive recurrence:      For state $i$, the first return time $T_i$ is defined as $T_i = \\min{ n &gt; 0 : X_n = i }$. In other words, itâ€™s the time required for the chain to return to state $i$ for the first time after starting from $i$.                                State $i$ is said to be positively recurrent if $E(T_i          X_0 = i) = m_i &lt; \\infty$. That is, the expected time to return to $i$ starting from $i$ is finite.                    Now, think about the long-time trajectory starting in state $i$. Given the positive recurrence, youâ€™d expect the chain to return to state $i$ multiple times over a long period. Specifically, if you think of $m_i$ steps, youâ€™d expect, on average, that one of these $m_i$ steps is in state $i$.So, when you think over even longer durations, say $n$ steps, youâ€™d expect the chain to be in state $i$ about $n/m_i$ times. In other words, the â€œproportionâ€ of time the chain spends in state $i$ should converge to $\\frac{1}{m_i}$.From the discussion above, we have:\\[\\lim_{n \\to \\infty} \\frac{1}{n} \\sum_{k=1}^{n} P(X_k = i | X_0 = i)\\]This represents the average proportion of the first $n$ steps where the chain is in state $i$ given it started in state $i$. And indeed, this should equate to $\\frac{1}{m_i}$.Now, letâ€™s consider the average probability of being in state $j$ at time $n$ given we start in state $i$ at time 0:\\[\\lim_{n \\to \\infty} \\frac{1}{n} \\sum_{k=1}^{n} P(X_k = j | X_0 = i) = \\pi_j\\]This $\\pi_j$ is our desired stationary distribution for state $j$.Proof of UniquenessSuppose there are two stationary distributions $\\pi$ and $\\piâ€™$ for an irreducible Markov chain. Then, for each state $j$ and transition probability matrix $P$:\\[\\pi_j = \\sum_i \\pi_i P_{ij}\\]\\[\\pi'_j = \\sum_i \\pi'_i P_{ij}\\]Consider a convex combination of $\\pi$ and $\\piâ€™$:\\[\\rho = \\alpha \\pi + (1 - \\alpha) \\pi'\\]For some $0 &lt; \\alpha &lt; 1$.We can then show:\\(\\rho_j = \\sum_i \\rho_i P_{ij}\\)This means $\\rho$ is also a stationary distribution, but this contradicts the fact that the proportion of time the Markov chain spends in state $j$ (due to the ergodic theorem) should converge to the stationary probability of state $j$. Hence, $\\pi$ and $\\piâ€™$ cannot be distinct, so the stationary distribution is unique.  TODO: what is the ergodic theorem?"
  },
  
  {
    "title": "Sequential Social Dilemma",
    "url": "/posts/SSD/",
    "categories": "Artificial Intelligence, Multi-Agent Reinforcement Learning",
    "tags": "tech, game theory, sequential social dilemma, multi agents, reinforcement learning",
    "date": "2023-09-01 18:40:00 +0000",
    





    
    "snippet": "What is Social Dilemma?DefinitionA social dilemma refers to a situation in which individual actions that seem to be rational and in self-interest can lead to collective outcomes that are undesirabl...",
    "content": "What is Social Dilemma?DefinitionA social dilemma refers to a situation in which individual actions that seem to be rational and in self-interest can lead to collective outcomes that are undesirable for everyone.Examples  Public goods games (Prisonerâ€™s Dilemma, Snowdrift)  Common recourses games.  Braessâ€™s paradox.The free-rider problemThe free-rider problem arises when individuals can benefit from a resource or service without paying for it or contributing to its provision. This can lead to under-provision of that good or service, or overuse and depletion if itâ€™s a shared resource.The problem is particularly pertinent in the case of public goods. Public goods are characterized by two properties:      Non-excludability: Once the good is produced, no one can be excluded from using or benefiting from it. This means producers canâ€™t prevent non-payers from accessing the good.        Non-rivalry: One personâ€™s use of the good doesnâ€™t reduce its availability for others.  Given these properties, individuals often lack the incentive to pay for or contribute to the production of the good, since they can benefit from it without doing so. They instead â€œfree-rideâ€ on the contributions of others. If everyone adopts this mindset, the good may not be produced at all, or it may be produced in insufficient quantities.Examples of the Free-rider Problem:      National Defense: Once a country establishes a defense system, all residents are protected, regardless of how much they personally contributed to it. Some might benefit from the defense without paying taxes or contributing to its provision, effectively free-riding on the contributions of others.        Public Broadcast: A public radio station provides content freely to all listeners. Those who listen without donating or supporting the station are free riders.        Vaccination: When a significant portion of a population gets vaccinated against a contagious disease, herd immunity can protect even those who havenâ€™t been vaccinated. Those who choose not to get vaccinated but still benefit from the reduced risk are free riders.        Public Parks and Clean Air: If a community raises funds to maintain a public park or invests in reducing air pollution, those who enjoy the park or the clean air without contributing to the funding or efforts are free riders.  The free-rider problem can lead to inefficiencies and under-provision of public goods. Itâ€™s a central challenge in economics and public policy and often necessitates government intervention or alternative funding mechanisms to ensure adequate provision of public goods or protection of shared resources.Strategic Intentions  Leibo, Joel Z., et al. â€œMulti-agent reinforcement learning in sequential social dilemmas.â€ arXiv preprint arXiv:1702.03037 (2017).  The following part has not been finished yet. One may check my writing schedule."
  },
  
  {
    "title": "Zero-Determinant Strategy",
    "url": "/posts/Zero-Determinant/",
    "categories": "Economics & Game Theory",
    "tags": "tech, game theory, sequential social dilemma, multi agents",
    "date": "2023-08-29 18:40:00 +0000",
    





    
    "snippet": "  This note aims to summarize the essence of this paper:Press, William H., and Freeman J. Dyson. â€œIterated Prisonerâ€™s Dilemma contains strategies that dominate any evolutionary opponent.â€ Proceedin...",
    "content": "  This note aims to summarize the essence of this paper:Press, William H., and Freeman J. Dyson. â€œIterated Prisonerâ€™s Dilemma contains strategies that dominate any evolutionary opponent.â€ Proceedings of the National Academy of Sciences 109.26 (2012): 10409-10413.Interesting Facts  As stated in the title: Iterated Prisonerâ€™s Dilemma contains strategies that dominate any evolutionary opponent. And this kind of strategy is the Zero-Determinant strategy.  The â€œIterated Prisonerâ€™s Dilemma is an Ultimatum gameâ€.  â€œOne player can enforce a unilateral claim to an unfair share of rewardsâ€, by the ZD strategy.  Any evolutionary agent will be exploited by the agent with the ZD strategy. â€œAn evolutionary playerâ€™s best response is to accede to the extortion. Only a player with a theory of mind about his opponent can do better.â€  â€œFor any strategy of the longer-memory player Y, shorter-memory Xâ€™s score is exactly the same as if Y had played a certain shorter-memory strategy.â€Iterated Prisonerâ€™s DilemmaAt each timestep, two agents are playing Prisonerâ€™s Dilemma:            Player1\\Player2      Cooperate (Deny)      Defect (Confess)                  Cooperate (Deny)      $R,R$      $S,T$              Defect (Confess)      $T,S$      $P,P$      where $T &gt; R &gt; P &gt; S$, and the meanings are as follows.  $T$: Temptation  $R$: Reward  $P$: Punishment  $S$: Suckerâ€™s payoffThe two agents repeatedly play this game $T$ times. It might be finite or infinite.Longer-Memory Strategies Offer No Advantage  Player $i$:          Making decisions on short memories $\\tau^i$.      $\\pi^i(a^i\\mid \\tau^i)$        Player $j$          Making decisions on long memories $(\\tau^i,\\Delta\\tau)$.      $\\pi^j(a^j\\mid \\tau^i,\\Delta\\tau)$      Derivation:\\[\\begin{aligned}    &amp;\\sum\\limits_{\\tau^i,\\Delta\\tau}     \\pi^i(a^i\\mid \\tau^i) \\cdot    \\pi^j(a^j\\mid \\tau^i,\\Delta\\tau) \\cdot    \\mathrm{Pr}(\\tau^i,\\Delta\\tau) \\\\    =&amp; \\sum\\limits_{\\tau^i} \\pi^i(a^i\\mid \\tau^i) \\cdot    \\left[        \\sum\\limits_{\\Delta\\tau} \\pi^j(a^j\\mid \\tau^i,\\Delta\\tau)        \\cdot \\mathrm{Pr}(\\Delta\\tau\\mid \\tau^i) \\cdot \\mathrm{Pr}(\\tau^i)    \\right] \\\\    =&amp; \\sum\\limits_{\\tau^i} \\pi^i(a^i\\mid \\tau^i)  \\cdot    \\left[ \\sum\\limits_{\\Delta\\tau} \\mathrm{Pr}(a^j, \\Delta\\tau\\mid \\tau^i) \\right]    \\cdot \\mathrm{Pr}(\\tau^i)\\\\    =&amp; \\sum\\limits_{\\tau^i} \\pi^i(a^i\\mid \\tau^i) \\cdot \\mathrm{Pr}(a^j \\mid \\tau^i) \\cdot \\mathrm{Pr}(\\tau^i)\\end{aligned}\\]$\\mathrm{Pr}(a^j \\mid \\tau^i)$ is the player $j$â€™s marginalized strategy. And\\[\\mathrm{Pr}(a^j \\mid \\tau^i) = \\sum\\limits_{\\Delta\\tau} \\mathrm{Pr}(a^j, \\Delta\\tau\\mid \\tau^i).\\]  After some plays, $j$ can estimate the expectations, and it can switch to an equivalent short-memory strategy.  â€œ$j$â€™s switching between a long- and short-memory strategy is completely undetectable (and irrelevant) to $i$.â€  Is this conclusion only suitable for repeated games?Zero-Determinant StrategySome tedious parts were automatically filled in with the help of ChatGPT 4.Notation of 4 outcomes            Player1\\Player2      Cooperate (c)      Defect (d)                  Cooperate (c)      $\\mathrm{cc}$ (1)      $\\mathrm{cc}$ (2)              Defect (d)      $\\mathrm{dc}$ (3)      $\\mathrm{cc}$ (4)      Notation of strategies  There are two players, $i$ and $j$, with memory-one strategies.  Strategies are based on the outcome of last play.\\[\\begin{aligned}\\mathbf{p} =&amp; \\pi^i(a_t^i=\\mathrm{Cooperate}\\mid a_{t-1}^i,a_{t-1}^j) \\\\=&amp;(p_{\\mathrm{cc}}, p_{\\mathrm{cd}}, p_{\\mathrm{dc}}, p_{\\mathrm{dd}}) \\\\=&amp;(p_1, p_2, p_3, p_4)\\end{aligned}\\]$p_1, p_2, p_3, p_4$ are independent and range from $[0,1].$\\[\\begin{cases}    p_{\\mathrm{cc}} = \\pi^i(a_t^i=\\mathrm{Cooperate}\\mid a_{t-1}^i =\\mathrm{Cooperate},a_{t-1}^j=\\mathrm{Cooperate}) \\\\    p_{\\mathrm{cd}} = \\pi^i(a_t^i=\\mathrm{Cooperate}\\mid a_{t-1}^i=\\mathrm{Cooperate},a_{t-1}^j=\\mathrm{Defect}) \\\\    p_{\\mathrm{dc}} = \\pi^i(a_t^i=\\mathrm{Cooperate}\\mid a_{t-1}^i=\\mathrm{Defect},a_{t-1}^j=\\mathrm{Cooperate}) \\\\    p_{\\mathrm{dd}} = \\pi^i(a_t^i=\\mathrm{Cooperate}\\mid a_{t-1}^i=\\mathrm{Defect},a_{t-1}^j=\\mathrm{Defect})\\end{cases}\\]\\[\\mathbf{q} = \\pi^j(a_t^j=\\mathrm{Cooperate}\\mid a_{t-1}^i,a_{t-1}^j)\\]Markov transition matrix: $\\mathbf{M}(\\mathbf{p}, \\mathbf{q})$  It is a transition kernel of an MDP.  The rows indicates the current states.  The columns indicates the next states.  Each entry indicates the probability of the current row state transitioning to the next column state.            Â       $\\mathrm{cc}$      $\\mathrm{cd}$      $\\mathrm{dc}$      $\\mathrm{dd}$                  $\\mathrm{cc}$      $\\mathrm{Pr}(\\mathrm{cc}\\mid \\mathrm{cc})$      $\\mathrm{Pr}(\\mathrm{cd}\\mid \\mathrm{cc})$      $\\mathrm{Pr}(\\mathrm{dc}\\mid \\mathrm{cc})$      $\\mathrm{Pr}(\\mathrm{dd}\\mid \\mathrm{cc})$              $\\mathrm{cd}$      $\\mathrm{Pr}(\\mathrm{cc}\\mid \\mathrm{cd})$      $\\mathrm{Pr}(\\mathrm{cd}\\mid \\mathrm{cd})$      $\\mathrm{Pr}(\\mathrm{dc}\\mid \\mathrm{cd})$      $\\mathrm{Pr}(\\mathrm{dd}\\mid \\mathrm{cd})$              $\\mathrm{dc}$      $\\mathrm{Pr}(\\mathrm{cc}\\mid \\mathrm{dc})$      $\\mathrm{Pr}(\\mathrm{cd}\\mid \\mathrm{dc})$      $\\mathrm{Pr}(\\mathrm{dc}\\mid \\mathrm{dc})$      $\\mathrm{Pr}(\\mathrm{dd}\\mid \\mathrm{dc})$              $\\mathrm{dd}$      $\\mathrm{Pr}(\\mathrm{cc}\\mid \\mathrm{dd})$      $\\mathrm{Pr}(\\mathrm{cd}\\mid \\mathrm{dd})$      $\\mathrm{Pr}(\\mathrm{dc}\\mid \\mathrm{dd})$      $\\mathrm{Pr}(\\mathrm{dd}\\mid \\mathrm{dd})$                  Â       $\\mathrm{cc}$      $\\mathrm{cd}$      $\\mathrm{dc}$      $\\mathrm{dd}$                  $\\mathrm{cc}$      $p_1\\cdot q_1$      $p_1\\cdot (1-q_1)$      $(1-p_1)\\cdot q_1$      $(1-p_1)\\cdot(1-q_1)$              $\\mathrm{cd}$      $p_2\\cdot q_3$      $p_2\\cdot (1-q_3)$      $(1-p_2)\\cdot q_3$      $(1-p_2)\\cdot(1-q_3)$              $\\mathrm{dc}$      $p_3\\cdot q_2$      $p_3\\cdot (1-q_2)$      $(1-p_3)\\cdot q_2$      $(1-p_3)\\cdot(1-q_2)$              $\\mathrm{dd}$      $p_4\\cdot q_4$      $p_4\\cdot (1-q_4)$      $(1-p_4)\\cdot q_4$      $(1-p_4)\\cdot(1-q_4)$      $\\mathbf{M}$ has a unit eigenvalue  This part requires some knowledge of stochastic processes. Check my other note.  The following part has not been finished yet. One may check my writing schedule.  $\\mathbf{M}$ is irreducible (?)  $\\mathbf{M}$ is finiteThen $\\mathbf{M}$ is positive recurrent.  $\\mathbf{M}$ is a positive recurrent Markov chain.  $\\Rightarrow$ $\\mathbf{M}$ has a stationary distribution $\\mathbf{v} = \\mathbf{v} \\mathbf{M}.$  $\\Rightarrow$ $\\mathbf{M}$ has a unit eigenvalue: $1\\cdot \\mathbf{v} = \\mathbf{v} \\mathbf{M}.$y distribution exists."
  },
  
  {
    "title": "Classic Games",
    "url": "/posts/Classic-Games/",
    "categories": "Economics & Game Theory",
    "tags": "tech, game theory, matrix game",
    "date": "2023-08-13 08:30:00 +0000",
    





    
    "snippet": "  This note will be consistently updated.Prisonerâ€™s Dilemma  Two members of a criminal organization are arrested and imprisoned. Each prisoner is in solitary confinement with no means of communicat...",
    "content": "  This note will be consistently updated.Prisonerâ€™s Dilemma  Two members of a criminal organization are arrested and imprisoned. Each prisoner is in solitary confinement with no means of communicating with the other. The prosecutors lack sufficient evidence to convict the pair on the principal charge, but they have enough to convict both on a lesser charge. Simultaneously, the prosecutors offer each prisoner a bargain. Each prisoner is given the opportunity either to betray the other by testifying that the other committed the crime, or to cooperate with the ot\ther by remaining silent.  Poundstone, William. Prisonerâ€™s dilemma: John von Neumann, game theory, and the puzzle of the bomb. Anchor, 1993.Normal form            Player1\\Player2      Cooperate (Deny)      Defect (Confess)                  Cooperate (Deny)      $b,b$      $d,a$              Defect (Confess)      $a,d$      $c,c$      $a \\gt b \\gt c \\gt d$.If both cooperate, each gets R (for Reward).If both defect, each gets P (for Punishment).If one defects and the other cooperates, the defector gets T (for Temptation) and the cooperator gets S (for Suckerâ€™s payoff).In some studies, $a,b,c$ and $d$ are represented by $T,R,P$ and $S$ respectively.  $T$: Temptation  $R$: Reward  $P$: Punishment  $S$: Suckerâ€™s payoffAttractor of player1: Reversed N-like (or N-like)            $b\\downarrow$      $d\\downarrow$                  $a$      $c(\\nwarrow)$      Attractor of player2:            $b\\rightarrow$      $a$                  $d\\rightarrow$      $c(\\nwarrow)$      The â€œconfessâ€ action is the dominant one.Nash equilibriumCan be found by iterated elimination of strictly dominated strategies (IESDS). For player1: check the 1st num up-and-down. For player2: check 2nd num left-and-right.            Player1\\Player2      Cooperate (Deny)      Defect (Confess)                  Cooperate (Deny)      $b(\\downarrow),b(\\rightarrow)$      $d(\\downarrow),a(\\checkmark)$              Defect (Confess)      $a(\\checkmark),d(\\rightarrow)$      $c(\\checkmark),c(\\checkmark)$      Pareto optimal(check 3 times for each state)            Player1\\Player2      Cooperate (Deny)      Defect (Confess)                  Cooperate (Deny)      $b,b\\ldots (\\checkmark)$      $d,a\\ldots (\\checkmark)$              Defect (Confess)      $a,d\\ldots (\\checkmark)$      $c,c \\ldots(\\nwarrow)$      Stag Hunt  In the simple, matrix-form, two-player Stag Hunt each player makes a choice between a risky action (hunt the stag) and a safe action (forage for mushrooms). Foraging for mushrooms always yields a safe payoff while hunting yields a high payoff if the other player also hunts but a very low payoff if one shows up to hunt alone.      Peysakhovich, Alexander, and Adam Lerer. â€œProsocial Learning Agents Solve Generalized Stag Hunts Better than Selfish Ones.â€ Proceedings of the 17th International Conference on Autonomous Agents and MultiAgent Systems. 2018.    Harsanyi, John C., and Reinhard Selten. â€œA general theory of equilibrium selection in games.â€ MIT Press Books 1 (1988).  Stag Hunt is an Assurance Game.Normal form            Player1\\Player2      Cooperate (Hunt)      Defect (Forage)                  Cooperate (Hunt)      $a,a$      $d,b$              Defect (Forage)      $b,d$      $c,c$      $a \\gt b \\ge c \\gt d$.Attractor of player1: Reversed u-like (or n-like)            $a$      $d\\downarrow$                  $b\\uparrow$      $c(\\leftarrow)$      Attractor of player2:            $a$      $b\\leftarrow$                  $d\\rightarrow$      $c(\\uparrow)$      Nash equilibrium            Player1\\Player2      Cooperate (Hunt)      Defect (Forage)                  Cooperate (Hunt)      $a(\\checkmark),a(\\checkmark)$      $d(\\downarrow),b(\\leftarrow)$              Defect (Forage)      $b(\\uparrow),d(\\rightarrow)$      $c(\\checkmark),c(\\checkmark)$      Pareto optimal            Player1\\Player2      Cooperate (Hunt)      Defect (Forage)                  Cooperate (Hunt)      $a,a\\ldots(\\checkmark)$      $d,b\\ldots(\\leftarrow)$              Defect (Forage)      $b,d\\ldots(\\uparrow)$      $c,c\\ldots(\\nwarrow)$      Snowdrift  The name Snowdrift game refers to the situation of two drivers caught with their cars in a snow drift. If they want to get home, they have to clear a path. The fairest solution would be for both of them to start shoveling (we assume that both have a shovel in their trunk). But suppose that one of them stubbornly refuses to dig. The other driver could do the same, but this would mean sitting through a cold night. It is better to shovel a path clear, even if the shirker can profit from it without lifting a finger.  Sigmund, Karl. The calculus of selfishness. Princeton University Press, 2010.This game has the same payoff pattern as the games Chicken and Hawkâ€“Dove. Alternatively, I could say that these two games share the same potential game?Normal form            Player1\\Player2      Cooperate      Defect                  Cooperate      $b,b$      $c,a$              Defect      $a,c$      $d,d$      $a \\gt b \\gt c \\gt d$.Attractor of player1: Reversed n-like (or u-like)            $b\\downarrow$      $c(\\leftarrow)$                  $a$      $d\\uparrow$      Attractor of player2:            $b\\rightarrow$      $a$                  $c(\\uparrow)$      $d\\leftarrow$      Nash equilibrium            Player1\\Player2      Cooperate      Defect                  Cooperate      $b(\\downarrow),b(\\rightarrow)$      $c(\\checkmark),a(\\checkmark)$              Defect      $a(\\checkmark),c(\\checkmark)$      $d(\\uparrow),d(\\leftarrow)$      Pareto optimal            Player1\\Player2      Cooperate      Defect                  Cooperate      $b,b\\ldots(\\checkmark)$      $c,a\\ldots(\\checkmark)$              Defect      $a,c\\ldots(\\checkmark)$      $d,d\\ldots(\\leftarrow\\nwarrow\\uparrow)$      Battle of the SexesAlso known as Bach or Stravinsky.Normal form            Player1\\Player2      Bach      Stravinsky                  Bach      $2,1$      $0,0$              Stravinsky      $0,0$      $1,2$      Nash equilibrium            Player1\\Player2      Bach      Stravinsky                  Bach      $2(\\checkmark),1(\\checkmark)$      $0(\\downarrow),0(\\leftarrow)$              Stravinsky      $0(\\uparrow),(\\rightarrow)0$      $1(\\checkmark),2(\\checkmark)$      Mixed strategy Nash equilibrium            policy      Â       q      1-q                  Â       Player1\\Player2      Bach      Stravinsky              p      Bach      $2,1$      $0,0$              1-p      Stravinsky      $0,0$      $1,2$      If player1 chooses â€œBachâ€, its expected payoff is\\[\\mathbb{E}\\left(r^1\\mid a^1 = \\text{Bach}\\right) = 2q + 0\\cdot (1-q) = 2q.\\]And if it chooses â€œStravinskyâ€, its expected payoff is $(1-q)$. It should be indifferent about playing heads or tails, otherwise it can improve its expected payoff by increasing the probability of the action that causes higher expected payoff. In this way,\\[\\begin{aligned}    \\quad &amp; \\mathbb{E}\\left(r^1\\mid a^1 = \\text{Bach}\\right) =         \\mathbb{E}\\left(r^1\\mid a^1 = \\text{Stravinsky}\\right) \\\\    \\Rightarrow \\quad &amp; 2q = 1-q \\\\    \\Rightarrow \\quad &amp; q = 1/3\\end{aligned}\\]If player2 chooses â€œBachâ€, its expected payoff is\\[\\mathbb{E}\\left(r^2\\mid a^2 = \\text{Bach}\\right) = p.\\]And if it chooses â€œStravinskyâ€, its expected payoff is $(2-2p)$. It should be indifferent about playing heads or tails, otherwise it can improve its expected payoff by increasing the probability of the action that causes higher expected payoff. In this way,\\[\\begin{aligned}    \\quad &amp; \\mathbb{E}\\left(r^2\\mid a^2 = \\text{Bach}\\right) =         \\mathbb{E}\\left(r^2\\mid a^2 = \\text{Stravinsky}\\right) \\\\    \\Rightarrow \\quad &amp; p = 2-2p \\\\    \\Rightarrow \\quad &amp; p = 2/3\\end{aligned}\\]The outcome is            policy      Â       1/3      2/3                  Â       Player1\\Player2      Bach      Stravinsky              2/3      Bach      $2,1$      $0,0$              1/3      Stravinsky      $0,0$      $1,2$      \\[\\mathbb{E}\\left( r^1 \\right) = \\frac{2}{3}\\cdot \\frac{1}{3} \\cdot 2 + \\frac{1}{3}\\cdot \\frac{2}{3} \\cdot 1 = \\frac{2}{3}\\]\\[\\mathbb{E}\\left( r^2 \\right) = \\frac{2}{3}\\cdot \\frac{1}{3} \\cdot 1 + \\frac{1}{3}\\cdot \\frac{2}{3} \\cdot 2 = \\frac{2}{3}\\]And this outcome is worse than any of the pure strategy equilibria.Introducing a Trusted AuthorityMatching PenniesIt is zero-sum (at each entry). Players are fully competitive.Normal form            Player1\\Player2      Heads      Tails                  Heads      $1,-1$      $-1,1$              Tails      $-1,1$      $1,-1$      Mixed strategy Nash equilibriumNo pure strategy works.            policy      Â       q      1-q                  Â       Player1\\Player2      Heads      Tails              p      Heads      $1,-1$      $-1,1$              1-p      Tails      $-1,1$      $1,-1$      If player1 chooses â€œHeadsâ€, its expected payoff is\\[\\mathbb{E}\\left(r^1\\mid a^1 = \\text{Heads}\\right) = q + (-1)\\cdot (1-q) = 2q-1.\\]And if it chooses â€œTailsâ€, its expected payoff is $(-2q+1)$. It should be indifferent about playing heads or tails, otherwise it can improve its expected payoff by increasing the probability of the action that causes higher expected payoff. In this way,\\[\\begin{aligned}    \\quad &amp; \\mathbb{E}\\left(r^1\\mid a^1 = \\text{Heads}\\right) =         \\mathbb{E}\\left(r^1\\mid a^1 = \\text{Tails}\\right) \\\\    \\Rightarrow \\quad &amp; 2q-1 = -2q+1 \\\\    \\Rightarrow \\quad &amp; q = 0.5\\end{aligned}\\]Rock paper scissorsIt is zero-sum (at each entry). Players are fully competitive.Normal form            Player1\\Player2      Rock      Paper      Scissors                  Rock      $0,0$      $-1,1$      $1,-1$              Paper      $1,-1$      $0,0$      $-1,1$              Scissors      $-1,1$      $1,-1$      $0,0$      Nash equilibriumNo pure strategy works.\\[\\begin{aligned}    \\quad &amp; \\mathbb{E}\\left(r^1\\mid a^1 = \\text{Rock}\\right) =         \\mathbb{E}\\left(r^1\\mid a^1 = \\text{Paper}\\right) =         \\mathbb{E}\\left(r^1\\mid a^1 = \\text{Scissors}\\right) \\\\    \\Rightarrow \\quad &amp; q = \\frac{1}{3}.\\end{aligned}\\]Muddy Children PuzzleCheck my other note: Dynamic Epistemic Logic.Trust      In the first stage, the Donor (or Investor) receives a certain endowment by the experimenter, and can decide whether or not to send a part of that sum to the Recipient (or Trustee), knowing that the amount will be tripled upon arrival: each euro spent by the Investor yields three euros on the Trusteeâ€™s account.    In the second stage, the Trustee can return some of it to the Investorâ€™s account, on a one-to-one basis: it costs one euro to the Trustee to increase the Investorâ€™s account by one euro.    This ends the game. Players know that they will not meet again.  Ultimatum  The experimenter assigns a certain sum, and the Proposer can offer a share of it to the Responder. If the Responder (who knows the sum) accepts, the sum is split accordingly between the two players, and the game is over. If the Responder declines, the experimenter withdraws the money. Again, the game is over: but this time, neither of the two players gets anything.Braessâ€™s ParadoxA computational example about Braessâ€™s Paradox is in Figure 18.2 of this chapter:  Roughgarden, Tim. â€œRouting games.â€ Algorithmic game theory 18 (2007): 459-484.Public Goods v.s. Common RecoursesThese two kinds of games are social dilemmas, i.e., the situations in which individual actions that seem to be rational and in self-interest can lead to collective outcomes that are undesirable for everyone.Public Goods GameSetupIn this game, a group of players is each given a sum of money (or any resource). They are offered the choice to invest any portion of this sum into a common pool. This pool is then multiplied by a factor greater than one (but less than the number of players) and distributed evenly among all players, regardless of their individual contributions.DilemmaThe groupâ€™s best outcome is if everyone contributes the maximum amount because this would result in the highest multiplication and distribution to everyone. However, from an individualâ€™s perspective, the best strategy is to contribute nothing and free-ride on the contributions of others. If everyone thinks this way, no one contributes, and the group ends up worse off.Examples  The Snowdrift Game (or Hawk-Dove Game)  The Prisonerâ€™s Dilemma.Common Resources Game (or the Tragedy of the Commons)SetupPlayers have access to a shared resource (like a common grazing field for sheep). Each player decides how many units of the resource (e.g., how many sheep) to use. The resource can regenerate over time, but if overused, it can get depleted.DilemmaIf all players use the resource sustainably, the resource persists, benefiting everyone continuously. However, each player has an incentive to use as much of the resource as quickly as possible to gain immediate benefits, especially before others use it up. If all players act on this individual incentive, the shared resource gets depleted, and everyone ends up worse off in the long run.Differences  Nature of the Good:          Public Goods: These are non-excludable (one personâ€™s use doesnâ€™t exclude anotherâ€™s use) and non-rivalrous (use by one person doesnâ€™t reduce its availability to others). Examples include clean air, national defense, and public parks.      Common Resources: These are non-excludable but rivalrous. One personâ€™s use directly impacts another personâ€™s ability to use it. Examples include fisheries, forests, and shared pastures.        Primary Challenge:          Public Goods: The challenge is about contributing to the provision of the good. The temptation is to free-ride on othersâ€™ contributions.      Common Resources: The challenge is about overuse and depletion of the resource. The temptation is to over-exploit before others do.        Outcome of Selfish Behavior:          Public Goods: If everyone acts selfishly, the public good is under-provided or not provided at all.      Common Resources: If everyone acts selfishly, the common resource is quickly depleted, rendering it unavailable even for future use.      The Cooperative GameDisambiguationThe canonical  definition of the cooperative game in game theory is different from the current common sense of MARL.Cooperative game theory deals with situations where players can benefit by cooperating, and binding agreements are possible. In these games, players form coalitions, and the outcomes depend on the behavior of these coalitions. The primary goal in cooperative games is often to understand how the gains from cooperation should be fairly distributed among the players.Notably, a key difference between cooperative and non-cooperative game theory is the idea of commitment. In cooperative games, itâ€™s assumed that players can make binding commitments to each other, whereas in non-cooperative games, players choose strategies without the possibility of making binding agreements.Formal definitionKey elements of a cooperative game:  Players: A finite set of players $N$.  Value function: Given any subset $S$ of $N$ (a coalition), the value function $v: 2^N \\to \\mathbb{R}$ assigns a real number $v(S)$ representing the total value or utility that the coalition $S$ can achieve by cooperating. Note that $v(\\emptyset) = 0$, meaning the value of an empty coalition is zero.  Characteristic function form: Cooperative games often take this form, where for every subset $S$ of $N$, a value $v(S)$ is specified. The number $v(S)$ represents the payoff that the members of $S$ can guarantee by forming a coalition and excluding all other players.An example: The â€œAirport Gameâ€Certainly! Letâ€™s delve into a classic example of a cooperative game: The â€œAirport Gameâ€.Scenario:Imagine there are three airlines: A, B, and C. They are considering building a runway at a shared airport. Each airline can benefit from the runway, but they benefit differently based on the size of their planes and the number of flights they operate. They want to decide how much each should contribute to the construction costs.  Airline Aâ€™s planes are large, and it would be willing to pay up to $600,000 for the runway if it had to bear all the costs itself.  Airline B operates smaller planes and would only be willing to pay $300,000.  Airline C operates the smallest planes and would pay just $100,000.The total cost of the runway is $800,000.Coalitional Values:If they cooperate, the total cost can be divided among them. The value each coalition (subset of airlines) can generate (or save) is as follows:  $v(A) = 600,000$  $v(B) = 300,000$  $v(C) = 100,000$  $v(A,B) = 800,000$ (since together they can cover the total cost)  $v(A,C) = 700,000$  $v(B,C) = 400,000$  $v(A,B,C) = 800,000$ (since all three together can cover the total cost)The question now becomes: How should the $800,000 cost be distributed among A, B, and C in a way that reflects their individual benefits and the benefit of their cooperation?Solution Concepts:There are various ways to allocate the costs based on cooperative game solution concepts. One of the most famous methods is the Shapley value, which provides a unique way to fairly allocate the costs based on individual and collective benefits.By computing the Shapley value for this game, one would find a fair division of the costs among A, B, and C.Iterated Prisonerâ€™s Dilemma  Axelrod, Robert, and William D. Hamilton. â€œThe evolution of cooperation.â€ science 211.4489 (1981): 1390-1396.Successful strategy conditions  Stated by Axelrod. The following conditions are adapted from Wikipedia.  Nice/optimistic: The strategy will not defect before its opponent does.  Retaliating: The strategy must sometimes retaliate. Otherwise it will be exploited by the opponent.  Forgiving: Though players will retaliate, they will cooperate again if the opponent does not continue to defect.  Non-envious: The strategy must not strive to score more than the opponent.Strategies  Tit-for-tat.  Win-stay, lose-switch.  Zero-determinant strategy. (Check my other note.)  Disclaimer: The description of games is from Wikipedia and other sources (books or papers). Corresponding links or references have been provided. The content regarding attractors is my personal understanding."
  },
  
  {
    "title": "Information Design in 10 Minutes",
    "url": "/posts/Information-Design-10min/",
    "categories": "Economics & Game Theory",
    "tags": "tech, game theory, information design, Bayesian persuasion, obedience, multi agents, incentive compatibility",
    "date": "2023-08-10 21:30:00 +0000",
    





    
    "snippet": "  This note provides a brief introduction to the basic concepts of information design. More details can be found in my other note on this topic.  â€œSometimes, the truth is not good enough.â€ â€” Batman...",
    "content": "  This note provides a brief introduction to the basic concepts of information design. More details can be found in my other note on this topic.  â€œSometimes, the truth is not good enough.â€ â€” Batman, The Dark Knight (2008).Information design focuses on scenarios of mixed-motive unidirectional communication, where one self-interested sender with informational advantage attempts to persuade a self-interested rational receiver to take actions that the sender prefers.  The â€œinformational advatageâ€ means that the sender has something that the receiver wants to know (i.e. which affects the receiverâ€™s payoff) but cannot know,  â€œself-interestedâ€ refers to the agent being concerned only about its own expected payoff, and  â€œrationalâ€ means that when it believes one actionâ€™s payoff is greater than anotherâ€™s, the agent will choose the action with higher expected payoff.Additionally, information design with a sender and a receiver is known as Bayesian persuasion. And the flow of an one-step Bayesian persuasion process is as follows:  The sender commits a signaling scheme to the receiver. The receiver will use this to calculate its posterior expected payoff. (This is referred to as the commitment assumption.);  The nature generates a state $s$. The sender observes the state $s$ and then samples a message according to the distribution of the committed signaling scheme; and  Receiving the message, the receiver calculates a posterior and chooses an optimal action for itself. Given the current state and the receiverâ€™s chosen action, the sender and the receiver get rewards from the nature.The key to the sender successfully persuading a receiver with whom it has an interest conflict lies in obedience constraints. To introduce it, letâ€™s simplify the problems first.Assuming that the senderâ€™s signal set is equal to the receiverâ€™s action set, the senderâ€™s signals can be interpreted as recommending the receiver to take a specific action. This common assumption is without loss of generality according to the revelation principle, i.e., there is an optimal signaling scheme that does not require more signals than the number of actions available to the receiver.Under this premise, obedience constraints can be formalized as:\\[\\sum\\limits_{s} \\mu_0(s)   \\cdot \\varphi( a\\mid s )  \\cdot \\Big( r^j(s, a) - r^j(s, a') \\Big) \\ge 0,\\]where $s\\in S$ is the state which is only observable by the sender, $\\mu_0$ is a prior distribution which is a common knowledge (both know, both know both know, etc.), $a\\in A$ is the receiverâ€™s action space, $\\varphi$ is the senderâ€™s signaling scheme, and $r^j$ is the receiverâ€™s reward function that depends on the state and the receiverâ€™s chosen action.The obedience constraints ensure that the receiver will definitely follow the senderâ€™s recommendations. A simple derivation is as follows:\\[\\begin{aligned}  &amp; \\sum\\limits_{s} \\mu_0(s)   \\cdot \\varphi( a\\mid s )  \\cdot \\Big( r^j(s, a) - r^j(s, a') \\Big) \\ge 0 \\\\  \\Leftrightarrow &amp;  \\sum\\limits_{s} \\frac{\\mu_0(s) \\cdot \\varphi( a\\mid s )}  { \\sum\\limits_{s'}\\mu_0(s') \\cdot \\varphi( a\\mid s')}  \\cdot \\Big( r^j(s, a) - r^j(s, a') \\Big) \\ge 0 , \\forall a'\\in A.\\\\  \\Leftrightarrow &amp;  \\sum\\limits_{s} \\mu(s\\mid a)  \\cdot \\Big( r^j(s, a) - r^j(s, a') \\Big) \\ge 0 , \\forall a'\\in A.\\\\  \\Leftrightarrow &amp;  \\sum\\limits_{s} \\mu(s\\mid a)  \\cdot r^j(s, a)  \\ge   \\sum\\limits_{s} \\mu(s\\mid a)  \\cdot r^j(s, a'), \\forall a'\\in A.\\end{aligned}\\]where $\\mu$ represents the posterior probability. Therefore, a self-interested and rational receiver will definitely follow the senderâ€™s recommendations, because the posterior expected payoff of the action recommended by the sender is greater than or equal to the posterior expected payoffs of all other actions.This greatly simplifies the problem, allowing the sender to choose the receiverâ€™s action that maximizes its expected payoff, while ensuring that the receiver obeys, and then recommend the receiver to take that action. Thus, the specific representation of the senderâ€™s optimization goal is:\\[\\begin{aligned}\\max\\limits_{\\varphi} \\mathbb{E}_{\\varphi}[\\ r^i(s, a) \\ ],\\;\\;\\textrm{s.t. Obedience Constraints.}\\end{aligned}\\]"
  },
  
  {
    "title": "A Memo on Game Theory",
    "url": "/posts/Game-Theory-Memo/",
    "categories": "Economics & Game Theory",
    "tags": "tech, game theory",
    "date": "2023-08-10 06:30:00 +0000",
    





    
    "snippet": "  This note will be consistently updated.RationalityA rational player is one who chooses his action, to maximize his payoff consistent with his beliefs about what is going on in the game.1      â€œse...",
    "content": "  This note will be consistently updated.RationalityA rational player is one who chooses his action, to maximize his payoff consistent with his beliefs about what is going on in the game.1      â€œself-interestedâ€ refers to the agent being concerned only about its own expected payoff, and    â€œrationalâ€ means that when it believes one actionâ€™s payoff is greater than anotherâ€™s, the agent will choose the action with higher expected payoff.  Bounded rationality is the idea that rationality is limited when individuals make decisions, and under these limitations, rational individuals will select a decision that is satisfactory rather than optimal.2Normal-form gameDefinition 3.31 A normal-form game includes three components as follows:  A finite set of players, $N = \\set{1,2,\\ldots,n}$.  A collection of sets of pure strategies, $\\set{S_1, S_2, \\ldots, s_n}$.  A set of payoff functions, $\\set{v_1,v_2,\\ldots, v_n}$, each assigning a payoff value to each combination of chosen strategies, that is, a set of functions $v_i: S_1\\times S_2, \\times \\dots \\times S_n \\to \\mathbb{R}$ for each $i\\in N$.Each of the players $i\\in N$ must simultaneously choose a possible strategy $s_i\\in S_i$.1  In my understanding, itâ€™s like $(I,\\set{A^i}_{i\\in I}, \\set{R^i}_{i\\in I})$ in the notation of MARL.Game TreeDefinition 3.13 A tree is a set of nodes and directed edges connecting these nodes suchthat  there is an initial node, for which there is no incoming edge;  for every other node, there is exactly one incoming edge;  for any two nodes, there is a unique path that connect these two nodes.And:  The edges are actions.  Each non-terminal node has been defined with who is to perform the action at this moment.  Payoffs for each player are defined at each terminal node.Information SetDefinition 3.43 An information set is a collection of nodes such that  the same player $i$ is to move at each of these nodes;  the same moves are available at each of these nodes.  In my understanding, the nodes in an information set share the same parent node, and they are at the same depth of the tree. The player taking actions at the current depth cannot figure out which node it is at. It cannot see the opponentâ€™s previous move.Definition 3.53 An information partition is an allocation of each non-terminal node of the tree to an information set; the starting node must be â€œaloneâ€.  In game theory, an information set represents the information available to a player at a particular point in the game. Specifically, it denotes every decision point the player could be at, given what they know so far. An information set is used in games of imperfect information, where players do not have complete information about the actions previously taken (for example, in card games where some cards are hidden).  Hereâ€™s a more formal definition:  An information set for a player in a game is a set of decision points such that:      The player must make a decision at each point in the set.    When the play of the game reaches an information set for a player, that player must choose an action without knowing which particular decision point within the set they are at.    This means that if two decision points are in the same information set, the player must have the same set of actions available at both, and cannot distinguish between these decision points based on past play.  In simpler terms, an information set groups together all the situations a player might be in which are indistinguishable from each other from that playerâ€™s perspective. This concept is crucial in designing strategies for games of imperfect information.â€” Generated by ChatGPT 4Extensive-Form Games  Often credit this paper:H. W. Kuhn. Extensive games and the problem of information. Contributions to the Theory of Games, 2:193â€“216, 1953.The extensive-form representation of a game contains all the information about the game explicitly, by defining who moves when, what each player knows when he moves, what moves are available to him, and where each move leads to, etc. This is done by use of a game tree and information setsâ€“as well as more basic information such as players and the payoffs.3Definition 3.33 (Extensive form) A Game consists of  a set of players,  a tree,  an allocation of non-terminal nodes of the tree to the players,  an informational partition of the non-terminal nodes, and  payoffs for each player at each terminal node.In my understanding, the nodes in extensive-form games are not equivalent to the states in MDP. Because the nodes are not Markovian.  Iâ€™m so confused. What should the corresponding quantity of â€œstateâ€ be? This could be a gap between the MARL and Game Theory frameworks.Information: Complete v.s. Perfect  Iâ€™ve looked up these two concepts so many times already, but every time I read about them, I forget shortly afterward. ğŸ˜…Complete InformationA game of complete information requires that the following four components be common knowledge among all the players of the game:1  all the possible actions of all the players,  all the possible outcomes,  how each combination of actions of all players affects which outcome will materialize, and  the preferences of each and every player over outcomes.Preferences describe how the player ranks the set of possible outcomes, from most desired to least desired.1  In my understanding, a game is with complete information if the utility functions (with their domain) are common knowledge. Generally, rationality is a fundamental assumption, so the fourth point in the definition is usually satisfied.Each player has full information about others. Common knowledge includes4  utility functions (including risk aversion),  payoffs,  strategies, and  â€œtypesâ€ of players (â€œprivateâ€ information).  Iâ€™m not sure what strategies being common knowledge looks like. And it reads that Chess is with incomplete information because of this. I think there is a conflict with the definition in the book.But4  players may not know the othersâ€™ actions (e.g. the initial placement of ships in  Battleship), and  the game may has chance element (card games).Perfect InformationDefinition 7.31 A game of complete information in which every information set is a singleton and there are no moves of Nature is called a game of perfect information.Each player is perfectly informed of all the events that have previously occurred.5 There is no hidden information.Examples4 5  Perfect and complete: Chess, Tic-Tac-Toe, Go.  Perfect but incomplete: Bayesian game.  Complte but imperfect: Card games, where each playerâ€™s cards are hidden from other players but objectives are known. The dealing event is not public (imperfect).Social Choice FunctionIt aims to align the interests of all agents.Price of Anarchy v.s. Price of Stability  PoA = Price of Anarchy          Anarchy = æ— æ”¿åºœä¸»ä¹‰ã€æ··ä¹±      å¦‚æœä¸è¦ä¸­å¿ƒåŒ–çš„authorityæ¥ç»„ç»‡ï¼Œé‚£è¦ä»˜å‡ºå¤šå¤§çš„ä»£ä»·ï¼ˆç›¸æ¯”æ€§èƒ½ä¸‹é™å¤šå°‘ï¼‰ï¼ˆæˆ‘çš„ç†è§£ï¼‰        notation          game $G = (N,S,u)$      ç©å®¶æœ‰$N$ä¸ªï¼Œå•ä¸ªæŸä¸ªç©å®¶è®°ä¸º$i$      å•ä¸ªæŸä¸ªç©å®¶çš„ç­–ç•¥é›†åˆä¸º$S_i$ï¼ˆè¿™é‡Œåªå‡å®šåšçš„ç­–ç•¥æ˜¯ç¡®å®šæ€§çš„ï¼Œåªé€‰æ‹©æŸä¸€ä¸ªåŠ¨ä½œï¼‰      å•ä¸ªæŸä¸ªç©å®¶çš„æ”¶ç›Šä¸º$u_i:S\\to \\mathbb{R}$ï¼›å…¶ä¸­$S=S_1\\times \\ldots\\times S_N$        PoAè¡¡é‡â€œç©å®¶çš„è‡ªç§è¡Œä¸ºâ€æ¶åŒ–â€œç³»ç»Ÿè¡¨ç°â€çš„â€œæ•ˆç‡â€          â€œç³»ç»Ÿâ€æŒ‡çš„æ˜¯æ‰€æœ‰ç©å®¶æ„æˆçš„é‚£ä¸ªæŠ½è±¡çš„æ•´ä½“ï¼ˆæˆ‘çš„ç†è§£ï¼‰      â€œç³»ç»Ÿè¡¨ç°â€æ ¹æ®æˆ‘ä»¬çš„ç›®çš„æ¥å®šä¹‰ï¼Œè®°ä¸º$\\text{Welf}:S\\to\\mathbb{R}$ï¼Œæ¯”å¦‚                  social welfareï¼ˆutilitarian objectiveï¼‰ï¼š$\\text{Welf}=\\sum\\limits_{i\\in N}u_i(s)$ï¼›æ‰€æœ‰ç©å®¶æ”¶ç›Šçš„å’Œ          fairnessï¼ˆegalitarian objectiveï¼‰ï¼š$\\text{Welf}=\\min\\limits_{i\\in N}u_i(s)$          ç³»ç»Ÿè¡¨ç°æ˜¯æˆ‘ä»¬æƒ³æ§åˆ¶å¾—åˆ°çš„æœ€å¤§åŒ–çš„ç›®æ ‡          å¦‚æœæ˜¯æƒ³æœ€å°åŒ–æŸä¸ªç›®æ ‡ï¼Œé‚£å°±åº”è¯¥æ˜¯$\\text{Cost}$                      ç¤¾ä¼šå›°å¢ƒï¼šä¸ªä½“åªæ³¨é‡è‡ªå·±æ”¶ç›ŠæœŸæœ›çš„ä¼˜åŒ–ä¸ä¸€å®šèƒ½è®©ç³»ç»Ÿè¡¨ç°ä¼˜åŒ–          å¦‚æœæœ‰ä¸­å¿ƒåŒ–çš„authorityï¼šæŠŠæ‰€æœ‰äººä½œä¸ºä¸€ä¸ªæŠ½è±¡çš„æ•´ä½“æ¥åˆ†æï¼Œå¯ä»¥è®©æŸäº›äººç‰ºç‰²ä¹‹ç±»çš„ï¼Œè¿™æ ·æ¯”è¾ƒå¥½åˆ†æå¾—åˆ°ä¸€ä¸ª$\\text{Welf}$å¾ˆé«˜çš„$s_{\\text{centralized}}$ï¼ˆæˆ–å¤šä¸ªï¼‰      å¦‚æœæ²¡æœ‰ä¸­å¿ƒåŒ–çš„authorityï¼Œæ¯ä¸ªäººåªä¼˜åŒ–è‡ªå·±ï¼Œæœ€ç»ˆçš„ç»“æœä¼šåˆ°è¾¾equilibriumï¼Œä¹Ÿå°±æ˜¯æŸä¸ª$s_{\\text{equilibrium}}$ï¼ˆæ¯”å¦‚å¸¸è§çš„Nash equilibriaï¼‰ã€‚equilibriaé›†åˆè®°ä¸º$\\text{Equil}$ã€‚      $\\text{PoA} = \\frac{\\max\\limits_{s\\in S}\\text{Welf}(s)} {\\min\\limits_{s\\in \\text{Equil}}\\text{Welf}(s)}$ï¼Œåˆ†æ¯æ˜¯fully decentralizedè®¾ç½®ä¸‹çš„æœ€åæƒ…å†µçš„æ”¶ç›Šã€‚ï¼ˆä¹Ÿæœ‰åˆ†å­åˆ†æ¯åè¿‡æ¥å®šä¹‰çš„è¯´æ³•ï¼‰      $\\text{PoA} = \\frac{\\max\\limits_{s\\in \\text{Equil}}\\text{Cost}(s)} {\\min\\limits_{s\\in S}\\text{Cost}(s)}$      å¦ä¸€ä¸ªæ¦‚å¿µæ˜¯PoSï¼Œthe Price of Stabilityï¼Œ      $\\text{PoS} = \\frac{\\max\\limits_{s\\in S}\\text{Welf}(s)} {\\max\\limits_{s\\in \\text{Equil}}\\text{Welf}(s)}$ï¼Œåˆ†å­å’ŒPoAä¸€æ ·ï¼Œåˆ†æ¯æ˜¯fully decentralizedè®¾ç½®ä¸‹çš„æœ€å¥½æƒ…å†µçš„æ”¶ç›Šã€‚      $\\text{PoS} = \\frac{\\min\\limits_{s\\in \\text{Equil}}\\text{Cost}(s)} {\\min\\limits_{s\\in S}\\text{Cost}(s)}$      æ ¹æ®å®šä¹‰ï¼Œ$\\text{PoA}\\ge\\text{PoS}\\ge1$ï¼Œwikié‡Œæ˜¯è¿™ä¹ˆè¯´çš„ï¼Œæ˜¯è´Ÿæ•°æˆ‘ä¸çŸ¥é“å’‹åŠï¼Œæ¯”å¦‚ï¼šåˆ†æ¯&lt;0&lt;åˆ†å­&lt;-åˆ†æ¯ï¼Œè¿™æ ·åŠ ç»å¯¹å€¼ä¹Ÿä¸å¯¹ã€‚        ä¾‹å­ï¼šPrisonerâ€™s Dilemma          å¸Œæœ›æœ€å¤§åŒ–çš„ä¸œè¥¿æ˜¯social walfareï¼Œå³ï¼Œæˆ‘ä»¬æƒ³ä¼˜åŒ–$\\text{Welf}=u_1(s_1,s_2)+u_2(s_1,s_2)$      å¦‚æœæœ‰ä¸­å¿ƒåŒ–çš„authorityï¼Œåˆ™æœ€ä¼˜çš„æƒ…å†µè‚¯å®šæ˜¯$(s_1=\\text{Cooperate},s_2=\\text{Cooperate})$ï¼Œæ­¤æ—¶$\\text{Welf}=2b$ã€‚      å¦‚æœæ²¡æœ‰ä¸­å¿ƒåŒ–çš„authorityï¼Œæ¯ä¸ªäººåªä¼˜åŒ–è‡ªå·±ï¼Œé‚£ä¹ˆæ­¤æ—¶å¯¹äºä¸¤ä¸ªç©å®¶æ¥è¯´ï¼Œ$\\text{Defect}$éƒ½æ˜¯å ä¼˜ç­–ç•¥ï¼ˆåˆ«äººé€‰æ‹©åˆä½œï¼Œé‚£æˆ‘èƒŒå›æ”¶ç›Šé«˜ï¼›åˆ«äººé€‰æ‹©èƒŒå›ï¼Œé‚£æˆ‘èƒŒå›æ”¶ç›Šé«˜ï¼›ä¸è®ºåˆ«äººæ€ä¹ˆé€‰æ‹©ï¼Œæˆ‘é€‰èƒŒå›æ”¶ç›Šé«˜ï¼‰ã€‚ä¼šæ”¶æ•›åˆ°$(s_1=\\text{Defect},s_2=\\text{Defect})$çš„Nash equilibriumã€‚$\\text{Welf}$ç›¸æ¯”æœ‰ä¸­å¿ƒåŒ–çš„æƒ…å†µï¼Œé™ä½äº†ã€‚      $\\text{PoA} = \\frac{2b}{2c}=\\frac{b}{c}$ï¼Œå…¶è·Ÿè¡¨ä¸­çš„æ•°å€¼æœ‰å…³ã€‚      Shapley ValueIn my understanding, the Shapley value reflects how significant an individualâ€™s effective contribution is to the total value of a certain coupling in a group. A real-world application is to determine how to distribute money: those who work more and contribute more effectively to the group should receive more money. It is a fair distribution method.Given a cooperative game with a set $N$ of players and a value function $v: 2^N \\to \\mathbb{R}$, which assigns a value to each coalition of players, the Shapley value of player $i$, denoted as $\\phi_i(v)$, is defined as:\\[\\begin{aligned}    \\phi_i(v)     &amp;= \\mathbb{E}_{s\\sim \\mathcal{P}(N\\setminus \\{i\\})}\\left[v(S \\cup \\{i\\}) - v(S) \\right] \\\\    &amp;= \\sum_{S \\subseteq N \\setminus \\{i\\}} \\frac{\\vert S\\vert! (\\vert N\\vert - \\vert S\\vert - 1)!}{\\vert N\\vert!} \\left[v(S \\cup \\{i\\}) - v(S)\\right]\\end{aligned}\\]Where:  $N$ is the set of all players.  $S$ is a subset of players not including player $i$.  $\\vert S\\vert$ is the number of players in subset $S$.  $\\vert N\\vert$ is the total number of players.  $v(S)$ is the value function, i.e., the value of coalition $S$.  The term $\\left[v(S \\cup {i}) - v(S)\\right]$ represents the marginal contribution of player $i$ to coalition $S$.  $\\mathcal{P}(N \\setminus {i})$ is the power set of $N \\setminus {i}$, which includes all possible subsets of it, ranging from the empty set to $N \\setminus {i}$ itself. And $S \\sim \\mathcal{P}(N \\setminus {i})$ means that the subset $S$ is uniformly randomly chosen from $\\mathcal{P}(N \\setminus {i})$.In words, the Shapley value of player $i$ is the average of its marginal contributions over all possible coalitions.RegretPotential GameBackward InductionForward InductionSolution ConceptNash equilibriumCorrelated equilibriumCoarse correlated equilibriumSubgame perfect equilibriumPerfect Bayesian equilibriumEfficiency  Perhaps the central theme of economic theory, efficiency is concerned with the optimal allocation of scarce resources. Efficiency is achieved when some specific criterion is maximized and no allocation of resources could yield a higher value according to that criterion. Usually, efficiency measures, such as Pareto optimality and Hicks optimality, attempt to maximize either individual or communal payoffs.â€” From this webpage.Hicks Optimal  Named after John Hicks, Hicks optimality is a measure of efficiency. An outcome of a game is Hicks optimal if there is no other outcome that results in greater total payoffs for the players. Thus, a Hicks optimal outcome is always the point at which total payoffs across all players is maximized. A Hicks optimal outcome is always Pareto optimal.â€” From this webpage.Pareto Optimal  Named after Vilfredo Pareto, Pareto optimality is a measure of efficiency. An outcome of a game is Pareto optimal if there is no other outcome that makes every player at least as well off and at least one player strictly better off. That is, a Pareto Optimal outcome cannot be improved upon without hurting at least one player. Often, a Nash Equilibrium is not Pareto Optimal implying that the playersâ€™ payoffs can all be increased.â€” From this webpage.Risk  In game theory, risk refers to the uncertainty about the outcomes of a decision. Itâ€™s the possibility that the actual outcome of a decision or an action might differ from the expected outcome. This uncertainty can arise from a variety of sources, such as incomplete information, unpredictable behavior of other players, or random events.â€” ChatGPT 4.Risk-Averse PlayersThese players prefer a certain outcome over a gamble with a higher expected value. They tend to avoid risks and might settle for a lower but guaranteed benefit.Risk-Seeking PlayersIn contrast, risk-seeking players prefer taking risks even when the expected value of the gamble is less than the certain outcome. They are attracted to the potential of higher gains.Risk-Neutral PlayersThese players are indifferent to risk. They make decisions based solely on expected values, without regard to the variability of outcomes.The canonical RL agents are risk-neutral.Von Neumannâ€“Morgenstern Utility TheoremReferences  Disclaimer: The above content is summarized from Wikipedia and other sources. Corresponding links or references have been provided.            Tadelis, Steven. Game theory: an introduction. Princeton university press, 2013.Â &#8617;Â &#8617;2Â &#8617;3Â &#8617;4Â &#8617;5Â &#8617;6              Wikipedia: Bounded Rationality.Â &#8617;              MIT OpenCourseWare: 14.12 Economic Applications of Game Theory.Â &#8617;Â &#8617;2Â &#8617;3Â &#8617;4Â &#8617;5              Wikipedia: Complete Information.Â &#8617;Â &#8617;2Â &#8617;3              Wikipedia: Perfect Information.Â &#8617;Â &#8617;2      "
  },
  
  {
    "title": "Fictitious Self-Play and Zero-Shot Coordination",
    "url": "/posts/Fictitious-Self-Play-Zero-Shot-Coordination/",
    "categories": "Artificial Intelligence, Multi-Agent Reinforcement Learning",
    "tags": "tech, game theory, fictitious play, self-play, multi agents, reinforcement learning",
    "date": "2023-07-31 18:40:00 +0000",
    





    
    "snippet": "Fictitious Play  Fictitious play is a learning rule.  In it, each player presumes that the opponents are playing stationary (possibly mixed) strategies.  At each round, each player thus best respon...",
    "content": "Fictitious Play  Fictitious play is a learning rule.  In it, each player presumes that the opponents are playing stationary (possibly mixed) strategies.  At each round, each player thus best responds to the empirical frequency of strategy of their opponent.          Belief of the opponentâ€™s strategy. Average of the history actions. Monte-Carlo style.      Best response.      (Wikipedia)ProcessGiven a game with $n$ players, where each player $i$ has a strategy $\\pi^i$.  Players initialize their beliefs about the strategies of the other players. And $\\hat{\\pi}^{i,j}$ means the belief of $i$ regarding the $j$â€™s strategy.  At each round $t$:          Player $i$ observes the action $a_{t-1}^j$ (or pure strategy) of every other player $j \\neq i$.      Player $i$ updates their belief of player $j$â€™s strategy based on the empirical frequency: $\\hat{\\pi}_t^{i,j} = \\frac{1}{t-1} \\sum_{k=1}^{t-1} a_k^j$. The actions are one-hot encoded.      Player $i$ then plays a best response to $\\hat{\\pi}_t^{i,j}$.      Convergence?Fictitious play doesnâ€™t always converge to a Nash equilibrium in all games. Itâ€™s been proven that fictitious play converges to the set of Nash equilibria in certain classes of games, like zero-sum games and potential games. However, there are games where fictitious play does not necessarily converge to a Nash equilibrium.E.g., in the Matching Pennies game, the trajectory of Fictitious Play tends to circle around the mixed-strategy Nash equilibrium rather than directly converging to it.Self-PlaySelf-play involves an agent (or a model) playing against itself or versions of itself. This can be thought of as a kind of bootstrapping method where an agent learns and refines its strategies through continuous iterations of gameplay against its own evolving strategies.Process  Initialization: Start with a randomly initialized agent or a naive version.  Play: Let the agent play games against itself or against past versions of itself.  Update: After each game, or a batch of games, update the agentâ€™s model based on the results, rewards, and feedback from the games.  Iterate: Repeat the Play and Update steps for a desired number of iterations or until the agentâ€™s strategy converges to an optimal or satisfactory level.Significance  Unlimited Opponents: Self-play provides an infinite supply of opponents since the agent is essentially playing against itself. This eliminates the need for external opponent data or human opponents, which can be limited or introduce variability.  Evolving Difficulty: As the agent improves, its past versions also present incrementally challenging opponents, ensuring that the agent is always pushed to improve and adapt.  Consistency: By playing against itself, the agent is exposed to a consistent level of gameplay, which can lead to more stable learning.ApplicationsThe most famous application of self-play is perhaps in the training of AlphaGo and its successors by DeepMind. AlphaGo utilized self-play to achieve superhuman performance in the game of Go, a feat that was previously thought to be decades away. Following this, AlphaZero utilized a generalized version of this self-play approach to achieve superhuman performance not only in Go but also in Chess and Shogi.Zero-Shot CoordinationDefinitionZero-shot coordination is about developing agents capable of coordinating in the testing phase with some other agents (even humans) that they have never seen in the training phase.  No Prior Coordination: Agents do not have the opportunity to coordinate or train to learn how to work together before starting the task. Essentially, agents start â€œfrom scratchâ€ without any prior coordination strategies.  No Communication: Agents cannot communicate with each other during task execution.The self-play is not good enoughSelf-play is a bootstrapping method. And  It assumes sample is representative: Bootstrapping works under an assumption based on the original sample data, i.e., the sample data is randomly drawn and is representative of the population. If the original sample is not a good representative of the population, bootstrapping can yield misleading results.  It may cuases potential overfitting: Given that bootstrapping is based on multiple resamples of the sample data, it may overemphasize certain features or outliers in the sample, leading to overfitting.In the current setting, the self-play agentâ€™s policy may converge to overfit its own policy, thus forming a specialized convention.After training with self-play, agents might develop different conventions if there are multiple maxima in the sense of the joint policy, which can potentially be symmetric to each other. Thus, without a good way to break the symmetries, agents may fail to coordinate.PerspectivesIn my understanding, the zero-shot coordination problem exists beacuse:  There are symmetric descriptions in the tasks. Some quantities are not (and should not be) labeled.  There are multiple maxima in the optimization problem of the task. Different pairs of agent cannot know which maxima their partner is at.  In different runs of the same experimental code, the agents may have different random seeds.  Self-play is a biased learning rule.Example: LeverIt is a matrix game. There are two agents with the same action space, each of size $m$. If the two agents choose the same actions, then they each receive a reward of 1; otherwise, they get 0.            Â       $a_1$      $a_2$      $a_3$                  $a_1$      (1,1)      (0,0)      (0,0)              $a_2$      (0,0)      (1,1)      (0,0)              $a_3$      (0,0)      (0,0)      (1,1)      If the two agents are trained through self-play successfully, then they will both choose the same action, a convention influenced by the initial random seeds used in the training process. Different pairs may converge to different outcomes, thus agents from different pairs may fail to coordinate.By the way, the mixed strategy Nash equilibrium is $\\left(\\frac{1}{3}, \\frac{1}{3}, \\frac{1}{3}\\right).$ The calculation is like the Matching Pennies case.In the paper â€œOther-Play,â€ which proposed this task, there is another version of it:            Â       $a_1$      $a_2$      $a_3$                  $a_1$      (1,1)      (0,0)      (0,0)              $a_2$      (0,0)      (1,1)      (0,0)              $a_3$      (0,0)      (0,0)      (0.9,0.9)      And the authors claimed that the most robust strategy is for everyone to choose the action $a_3$, which which would result in a payoff expectation $0.9.$ Otherwise, some pairs may choose $a_1$ and the others may choose $a_2$, it would lead to a payoff expectation $0.5.$  @fortytwo6364: But 0.9 is actually not the only unique answer to the lever problem, there is a unique 1.0 lever directly opposite to the .9 lever. This is stable to the symmetries implied by the problem statement where both players are shown the same set of levers, as well as being robust to different reflections and rotations being presented to different players (though not arbitrary permutations). So assuming whoever I am paired with is also optimizing we would both earn 1. This strategy doesnâ€™t work if there are an odd number of buttons to begin with.The task introduced in the paper includes an illustration where the actions are circled up. However, the authors claim that these actions are not labeled. A more accurate description would be that the actions are uniformly sampled within a closed space, meaning that they cannot be identified by their positions.  In my understanding, despite agents in zero-shot coordination never having interacted with others before, the designer assumes they are aware of the presence of others participating in the same task and are rational (and with some level of theory of mind). There is no free lunch here.Equivalence mappingThis definition used to describe the symmetry is from the paper â€œother-playâ€ and it is based on the Dec-POMDPs. It resembles the one used for finding Nash equilibria in large games like poker.An equivalence mapping of can find the states and actions that share the same reward, transition probability, and observation. Formally,\\[\\begin{aligned}  \\phi \\in \\Phi \\iff &amp; P(\\phi(s')\\mid\\phi(s),\\phi(a))   = P(s'\\mid s,a) \\\\   &amp;\\land R(\\phi(s'), \\phi(a), \\phi(s)) = R(s', a, s) \\\\  &amp;\\land O(\\phi(o)\\mid\\phi(s), \\phi(a), i) = O(o\\mid s, a, i) \\\\  &amp;\\forall s', s, a, o, i.\\end{aligned}\\]And itâ€™s a shorthand for $\\phi = {\\phi_S, \\phi_A, \\phi_O}.$ It thus can be extended to trajectories and the policies.  \\[\\phi(\\tau_{t}^{i}) = \\{\\phi(o_{0}^{i}), \\phi(a_{0}^{i}), \\phi(r_{0}), \\ldots , \\phi(o_{t}^{i})\\},\\]    \\[\\pi' = \\phi(\\pi) \\iff \\pi'(\\phi(a)\\mid\\phi(\\tau)) = \\pi(a\\mid\\tau), \\forall \\tau, a.\\]  In this way, a Dec-POMDP with two players has the following properties.  \\[J(\\pi_A, \\pi_B) = J(\\phi(\\pi_{A}^1), \\phi(\\pi_{B}^2)), \\forall \\phi \\in \\Phi, \\pi_A, \\pi_B.\\]    \\[\\{ \\phi \\cdot \\phi' : \\phi' \\in \\Phi \\} = \\Phi, \\quad \\forall \\phi \\in \\Phi.\\]  In my understanding, the \\cdot here means function composition. That is, $(\\phi \\cdot \\phiâ€™)(x)$ means $\\phi(\\phiâ€™(x))$.Evaluation: Cross-Play  The algorithm aimed to be tested is written in code, used to train the agents.  The code runs the traning experiment multiple times, each time with a different random seed.  The agents in different runs (with different random seeds) are required to play the coordination task in the test time.Success in cross-play is a necessary condition for the algorithm to achieve a level of zero-shot coordination.Illustration of Cross-Play from the paper â€œA New Formalism, Method and Open Issues for Zero-Shot Coordination.â€œOther-PlayHu, Hengyuan, et al. â€œâ€œother-playâ€ for zero-shot coordination.â€ International Conference on Machine Learning. PMLR, 2020.  Other-play is a learning rule.  The optimization problem explicitly involves considerations related to policy symmetry.Consider a Dec-POMDP with two players, the optimization problem in the sense of the other-play is\\[\\arg\\max\\limits_{\\boldsymbol\\pi} \\mathbb{E}_{\\phi \\sim \\Phi}\\left[J(\\pi^1, \\phi(\\pi^2)) \\right],\\]where the distribution is unifrom, and $\\mathbb{E}_{\\phi \\sim \\Phi}\\left[J(\\pi^1, \\phi(\\pi^2)) \\right]$ is denoted as $J_{OP}(\\boldsymbol\\pi).$  The expected OP return of $\\boldsymbol{\\pi}$ is equal to the expected return of each player independently playing a policy $\\pi_\\Phi^i$ which is the uniform mixture of $\\phi(\\pi^i)$ for all $\\phi\\in\\Phi.$\\[\\begin{aligned}  \\mathbb{E}_{\\phi \\sim \\Phi}\\left[J(\\pi^1, \\phi(\\pi^2)) \\right]  =&amp; \\mathbb{E}_{\\phi_1 \\sim \\Phi, \\phi_2 \\sim \\Phi}\\left[J(\\phi_1(\\pi^1), \\phi_1(\\phi_2(\\pi^2))) \\right] \\\\  =&amp; \\mathbb{E}_{\\phi_1 \\sim \\Phi, \\phi_2 \\sim \\Phi}\\left[J(\\phi_1(\\pi^1), (\\phi_2(\\pi^2))) \\right] \\\\\\end{aligned}\\]  The distribution $\\boldsymbol{\\pi}^*_{OP}$ produced by OP will be the uniform mixture $\\boldsymbol{\\pi}_{\\Phi}$ with the highest return $J(\\boldsymbol{\\pi}_{\\Phi}).$This means the optimal solution $\\boldsymbol{\\pi}^*_{OP} = \\arg\\max\\limits_{\\boldsymbol\\pi} \\mathbb{E}_{\\phi \\sim \\Phi}\\left[J(\\pi^1, \\phi(\\pi^2)) \\right]$ is the uniform mixture of the maxima of $\\boldsymbol{\\pi}^* = \\arg\\max\\limits_{\\boldsymbol\\pi} J(\\pi^1, \\pi^2).$OPâ€™s best response is also OP. Since OP is a learning rule, the equilibrium reached can be seen as a kind of meta-equilibrium.Simplified Action DecoderHu, Hengyuan, and Jakob N. Foerster. â€œSimplified Action Decoder for Deep Multi-Agent Reinforcement Learning.â€ International Conference on Learning Representations. 2019.Motivation  Fundamentally, RL requires agents to explore in order to discover good policies. However, when done naively, this randomness will inherently make their actions less informative to others during training.The more random the signal is, the less informative it will be.In my experience, this can be easily seen by the Recommendation Letter example from information design, or by the differential privacy.A communication protocol is a mapping $f:X\\to Y,$ where $X$ is the observation set, $Y$ is the signal set, and they are two random variables. â€œInformativeâ€ here refers to the degree to which $Y$ is related to $X.$  If they are the same then the agent reveals all of the information.  If $Y$ is irrelevant to $X$ then the agent reveals no information.Thus the necessary randomness for exploration (of the sender) is harmful to the communication, as it might cause the receiver to lose faith in the sender and eventually learn to ignore the signals.In the paper, equations $(3)$ to $(7)$ demonstrate that $\\epsilon$-greedy exploration is harmful.Technique list  CTDE (TODO)  Joint Q-functions (VDN, QMIX) (TODO)  Recurrent DQN  Auxiliary tasks  Theory of mind and Bayesian reasoningBayesian reasoningThe basic setting is the Dec-POMDPs.  The complete trajectory $\\tau = (s_0, \\boldsymbol{a}_0, r_1, \\ldots, r_T, S_T).$  The partially observable trajectory of agent $i$ is $\\tau^i = (o_0^i, \\boldsymbol{a}_0, r_1, \\ldots, r_T, S_T).$          The observation $o_t^i = O(s_t,i)$ is deterministic.      The agents are fully coorperative and they share the same reward at each timestep.      Each agent will calculate its prior guess about the complete trajectory, based on its own trajectory. That is $P(\\tau_t\\mid \\tau_t^i),$ and it is denoted as $B(\\tau_t).$If the agent can observe another agentâ€™s action $a_t^j$, then it can update its belief.\\[\\begin{aligned}  P(\\tau_t \\mid \\tau_t^i, a_t^j)   =&amp; \\frac{P(a_t^j\\mid \\tau_t)\\cdot P(\\tau_t\\mid \\tau_t^i)}  {P(a_t^j \\mid \\tau_t^i)}  = \\frac{P(a_t^j\\mid \\tau_t)\\cdot P(\\tau_t\\mid \\tau_t^i)}  {\\sum\\limits_{\\tau_t'} P(a_t^j \\mid \\tau_t')\\cdot P(\\tau_t'\\mid \\tau_t^i)} \\\\  =&amp; \\frac{\\pi^j\\left(a_t^j\\mid O(\\tau_t,j) \\right)\\cdot B(\\tau_t)}{\\sum\\limits_{\\tau_t'} \\pi^j\\left(a_t^j\\mid O(\\tau_t',j) \\right)\\cdot B(\\tau_t')}\\end{aligned}\\]$\\pi^j\\left(a_t^j\\mid O(\\tau_t,j) \\right)$ represents how agent $i$ perceives the strategy of agent $j$. And this is the theory of mind.  Agent $i$ needs to have access to the policy of agent $j$ during training. Can be justified by CTDE.  If this explicit belief is inputed into the network then it will cause higher order beliefs.  The authors use RNN to learn it implicitly. (How? TODO.)Simplified beliefIf agent $j$â€™s policy is with the $\\epsilon$-greedy exploration, then agent $i$â€™s belief of it is like\\[\\pi^j\\left(a_t^i\\mid O(\\tau_t, j)\\right) = (1-\\epsilon)\\cdot \\mathbf{I}\\left(a^*(\\tau_t) = a_t^j\\right) + \\epsilon / \\vert A \\vert,\\]where $\\mathbf{I}\\left(a^*(\\tau_t) = a_t^j\\right)$ is the indicator function (one-hot encoded), and $a^*(\\tau_t) = \\arg\\max\\limits_{a} Q^j\\left(O(\\tau_t, a), a\\right)$.Then, equations $(3)$ to $(7)$ demonstrate that $\\epsilon$-greedy exploration is harmful, by substitute it into the equation\\[\\begin{aligned}  P(\\tau_t \\mid \\tau_t^i, a_t^j)   =&amp; \\frac{\\pi^j\\left(a_t^j\\mid O(\\tau_t,j) \\right)\\cdot B(\\tau_t)}{\\sum\\limits_{\\tau_t'} \\pi^j\\left(a_t^j\\mid O(\\tau_t',j) \\right)\\cdot B(\\tau_t')}.\\end{aligned}\\]Now, if agent $i$ can get agent $j$â€™s greedy action (in the centralized training phase), then $i$â€™s posterior can be\\[\\begin{aligned}  P(\\tau_t \\mid \\tau_t^i, a^{j*})   =&amp; \\frac{\\mathbf{I}\\left(a^{j*}(\\tau_t) = a^{j*}\\right)\\cdot B(\\tau_t)}  {\\sum\\limits_{\\tau'} \\mathbf{I}\\left(a^{j*}(\\tau') = a^{j*}\\right)\\cdot B(\\tau')}.\\end{aligned}\\]Note that the agent $i$ only takes $j$â€™s greedy action into account. And this stabilizes the training process.Trajectory DiversityAndrei Lupu, Brandon Cui, Hengyuan Hu, Jakob Foerster. â€œTrajectory diversity for zero-shot coordination.â€ International conference on machine learning. PMLR, 2021.Off-Belief LearningHengyuan Hu, Adam Lerer, Brandon Cui, Luis Pineda, Noam Brown, Jakob Foerster. â€œOff-belief learning.â€ International Conference on Machine Learning. PMLR, 2021.Population-Based TrainingZhao, Rui, et al. â€œMaximum entropy population-based training for zero-shot human-ai coordination.â€ Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 37. No. 5. 2023.  The following part has not been finished yet. One may check my writing schedule.To-ReadJakob Foerster  SAD [code]Hu, Hengyuan, and Jakob N. Foerster. â€œSimplified Action Decoder for Deep Multi-Agent Reinforcement Learning.â€ International Conference on Learning Representations. 2019.  Other-Play [code]Hu, Hengyuan, et al. â€œâ€œother-playâ€ for zero-shot coordination.â€ International Conference on Machine Learning. PMLR, 2020.  Trajectory DiversityAndrei Lupu, Brandon Cui, Hengyuan Hu, Jakob Foerster. â€œTrajectory diversity for zero-shot coordination.â€ International conference on machine learning. PMLR, 2021.  Off-Belief LearningHengyuan Hu, Adam Lerer, Brandon Cui, Luis Pineda, Noam Brown, Jakob Foerster. â€œOff-belief learning.â€ International Conference on Machine Learning. PMLR, 2021.Tencent  Maximum Entropy Population-Based TrainingZhao, Rui, et al. â€œMaximum entropy population-based training for zero-shot human-ai coordination.â€ Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 37. No. 5. 2023.Env  A matrix-game [code][paper]          communication through action challenge      BAD        Lever [code][paper]          Other-Play        Corridor [paper]          Trajectory Diversity        Overcooked [code]  Hanabi [code] [paper]"
  },
  
  {
    "title": "Details on the Analysis of Policy Gradient Methods",
    "url": "/posts/Policy-Gradient-Details/",
    "categories": "Artificial Intelligence, Reinforcement Learning",
    "tags": "tech, convergence, policy gradient, reinforcement learning, tech",
    "date": "2023-07-24 18:40:00 +0000",
    





    
    "snippet": "  The only way to make sense out of change is to plunge into it, move with it, and join the dance. â€” Alan Watts.Policy Gradient Theorem  The proofs of the stochastic and deterministic policy gradie...",
    "content": "  The only way to make sense out of change is to plunge into it, move with it, and join the dance. â€” Alan Watts.Policy Gradient Theorem  The proofs of the stochastic and deterministic policy gradient theorem are mainly summarized from this blog and the supplementary of the paper â€œDeterministic Policy Gradient Algorithms,â€ respectively.Stochastic Policy GradientObjectitve\\[\\max\\limits_{\\theta} J(\\theta) = \\mathbb{E}_{s_0\\sim d_0}\\left[ \\textcolor{red}{V^{\\pi_\\theta} (s_0)} \\right].\\]Gradient\\[\\begin{aligned}\\nabla_\\theta V^{\\pi_\\theta}(s_0) =&amp; \\frac{1}{(1-\\gamma)} \\sum\\limits_{s} d^{\\pi_\\theta}(s\\mid s_0) \\sum\\limits_{a} Q^{\\pi_\\theta}(s,a) \\cdot \\nabla_\\theta \\pi_\\theta(a\\mid s) \\\\=&amp; \\frac{1}{(1-\\gamma)} \\sum\\limits_{s} d^{\\pi_\\theta}(s\\mid s_0) \\sum\\limits_{a} \\pi(a\\mid s) \\cdot Q^{\\pi_\\theta}(s,a) \\cdot \\nabla_\\theta \\ln \\pi_\\theta(a\\mid s) \\\\=&amp; \\frac{1}{(1-\\gamma)} \\mathbb{E}_{s \\sim d^{\\pi_\\theta}(\\cdot \\mid s_0)} \\mathbb{E}_{a\\sim \\pi(\\cdot \\mid s)} \\left[Q^{\\pi_\\theta}(s,a) \\cdot \\nabla_\\theta \\ln \\pi_\\theta(a\\mid s)\\right] \\end{aligned}\\]Proof\\[\\begin{aligned}\\nabla_\\theta V^\\pi(s) =&amp; \\nabla_\\theta \\sum\\limits_{a} \\pi(a\\mid s) \\cdot Q^\\pi(s,a) \\\\=&amp; \\sum\\limits_{a} \\left[Q^\\pi(s,a) \\cdot \\nabla_\\theta\\pi(a\\mid s) + \\pi(a\\mid s) \\cdot \\nabla_\\theta Q^\\pi(s,a)\\right] \\\\=&amp; \\sum\\limits_{a} \\left[Q^\\pi(s,a) \\cdot \\nabla_\\theta\\pi(a\\mid s) + \\pi(a\\mid s) \\cdot \\nabla_\\theta \\sum\\limits_{s',r} P(s',r\\mid s,a)\\cdot \\left(r+ \\gamma \\cdot V^\\pi(s')\\right)\\right] \\\\=&amp; \\sum\\limits_{a} \\left[Q^\\pi(s,a) \\cdot \\nabla_\\theta\\pi(a\\mid s) + \\gamma \\cdot  \\pi(a\\mid s) \\cdot \\sum\\limits_{s'} P(s'\\mid s,a)\\cdot \\nabla_\\theta V^\\pi(s')\\right] \\\\=&amp; \\sum\\limits_{a} Q^\\pi(s,a) \\cdot \\nabla_\\theta\\pi(a\\mid s) + \\gamma \\sum\\limits_{s',a} \\pi(a\\mid s) \\cdot  P(s'\\mid s,a)\\cdot \\nabla_\\theta V^\\pi(s') \\\\\\end{aligned}\\]Note that the action $a$ is sampled from the parameterized policy $\\pi_\\theta.$ Thus $\\nabla_\\theta a$ is $0,$ without using the gumbel-softmax technique. In the deterministic policy, $\\nabla_\\theta a$ is not $0,$ and thus the derivation is different.\\[\\begin{aligned}\\nabla_\\theta V^\\pi(s) =&amp; \\sum\\limits_{a} Q^\\pi(s,a) \\cdot \\nabla_\\theta\\pi(a\\mid s) + \\sum\\limits_{s',\\textcolor{red}{a}} \\textcolor{red}{\\gamma \\cdot \\pi(a\\mid s) \\cdot  P(s'\\mid s,a)} \\cdot \\nabla_\\theta V^\\pi(s') \\\\=&amp; \\sum\\limits_{a} Q^\\pi(s,a) \\cdot \\nabla_\\theta\\pi(a\\mid s) + \\sum\\limits_{s'} \\textcolor{red}{\\gamma \\cdot \\mathrm{Pr}(s\\to s', k=1, \\pi_\\theta)} \\cdot \\nabla_\\theta V^\\pi(s') \\\\=&amp; \\sum\\limits_{a} Q^\\pi(s,a) \\cdot \\nabla_\\theta\\pi(a\\mid s) + \\sum\\limits_{s'} \\gamma \\cdot \\mathrm{Pr}(s\\to s', k=1, \\pi_\\theta) \\left[\\sum\\limits_{a} Q^\\pi(s',a) \\cdot \\nabla_\\theta\\pi(a\\mid s') + \\sum\\limits_{s''} \\gamma \\cdot \\mathrm{Pr}(s'\\to s'', k=1, \\pi_\\theta)\\cdot \\nabla_\\theta V^\\pi(s'')\\right] \\\\=&amp; \\sum\\limits_{a} Q^\\pi(s,a) \\cdot \\nabla_\\theta\\pi(a\\mid s) + \\gamma \\sum\\limits_{s'} \\mathrm{Pr}(s\\to s', k=1, \\pi_\\theta) \\sum\\limits_{a} Q^\\pi(s',a) \\cdot \\nabla_\\theta\\pi(a\\mid s') \\\\\t&amp;+ \\textcolor{red}{\\gamma^2 \\sum\\limits_{s'} \\mathrm{Pr}(s\\to s', k=1, \\pi_\\theta) \\sum\\limits_{s''} \\mathrm{Pr}(s'\\to s'', k=1, \\pi_\\theta)}\\cdot \\nabla_\\theta V^\\pi(s'') \\\\=&amp; \\sum\\limits_{a} Q^\\pi(s,a) \\cdot \\nabla_\\theta\\pi(a\\mid s) + \\gamma \\sum\\limits_{s'} \\mathrm{Pr}(s\\to s', k=1) \\sum\\limits_{a} Q^\\pi(s',a) \\cdot \\nabla_\\theta\\pi(a\\mid s') \\\\\t&amp;+ \\textcolor{red}{\\gamma^2 \\sum\\limits_{s'} \\mathrm{Pr}(s\\to s'', k=2, \\pi_\\theta) }\\cdot \\nabla_\\theta V^\\pi(s'') \\\\=&amp;\\ldots \\\\=&amp; \\sum\\limits_{x\\in S} \\textcolor{blue}{\\sum\\limits_{k=0}^\\infty \\gamma^k \\cdot \\mathrm{Pr}(s\\to x, k, \\pi_\\theta)} \\cdot \\sum\\limits_{a} Q^\\pi(x,a) \\cdot \\nabla_\\theta\\pi(a\\mid x)\\end{aligned}\\]The blue part is defined as the discounted state visitation distribution \\(d^{\\pi_\\theta}(s\\mid s_0) = (1-\\gamma )\\cdot \\sum\\limits_{k=0}^\\infty \\gamma^k \\cdot \\mathrm{Pr}(s_0\\to s, k, \\pi_\\theta).\\)\\[\\begin{aligned}\\sum\\limits_{k=0}^\\infty \\gamma^k \\cdot \\mathrm{Pr}(s_0\\to s, k, \\pi_\\theta) \\le \\sum\\limits_{k=0}^\\infty \\gamma^k = \\frac{1}{1-\\gamma}\\end{aligned}\\]The distribution should beshould lie within the range of $[0,1]$ and thus the coefficient $(1-\\gamma)$ is is for normalization.\\[\\begin{aligned}\\nabla_\\theta V^{\\pi_\\theta}(s_0) =&amp; \\textcolor{blue}{\\frac{1}{(1-\\gamma)}} \\sum\\limits_{s} \\textcolor{blue}{d^{\\pi_\\theta}(s\\mid s_0)} \\sum\\limits_{a} Q^{\\pi_\\theta}(s,a) \\cdot \\nabla_\\theta \\pi_\\theta(a\\mid s) \\\\=&amp; \\frac{1}{(1-\\gamma)} \\sum\\limits_{s} d^{\\pi_\\theta}(s\\mid s_0) \\sum\\limits_{a} \\pi(a\\mid s) \\cdot Q^{\\pi_\\theta}(s,a) \\cdot \\nabla_\\theta \\ln \\pi_\\theta(a\\mid s) \\\\=&amp; \\frac{1}{(1-\\gamma)} \\mathbb{E}_{s \\sim d^{\\pi_\\theta}(\\cdot \\mid s_0)} \\mathbb{E}_{a\\sim \\pi(\\cdot \\mid s)} \\left[Q^{\\pi_\\theta}(s,a) \\cdot \\nabla_\\theta \\ln \\pi_\\theta(a\\mid s)\\right] &amp; \\blacksquare\\end{aligned}\\]Deterministic Policy GradientBasics  $a = \\mu_\\theta (s)$  $V^{\\mu_\\theta}(s) = Q^{\\mu_\\theta}(s,a) = Q^{\\mu_\\theta}(s,\\mu_\\theta(s))$  $\\nabla_\\theta a \\ne 0$          $\\nabla_\\theta r(s,a)\\ne 0$      $\\nabla_\\theta P(sâ€™\\mid s,a)\\ne 0$        $P(sâ€™\\mid s,\\mu_\\theta(s)) = \\mathrm{Pr}(s\\to sâ€™, k=1, \\mu_\\theta)$Gradient\\[\\begin{aligned}\\nabla_\\theta V^{\\mu_\\theta}(s) =&amp; \\frac{1}{(1-\\gamma)}\\int_{x\\in S} d^{\\mu_\\theta}(s) \\cdot \\nabla_\\theta \\mu_\\theta(x)\\cdot \\nabla_a Q^{\\mu_\\theta} (x, \\mu_\\theta(x))\\Big\\vert_{a=\\mu_\\theta(x)} \\,\\mathrm{d} x \\\\=&amp;\\frac{1}{(1-\\gamma)} \\mathbb{E}_{s\\sim d^{\\mu_\\theta}} \\bigg[ \\nabla_\\theta \\mu_\\theta(x)\\cdot \\nabla_a Q^{\\mu_\\theta} (x, \\mu_\\theta(x))\\Big\\vert_{a=\\mu_\\theta(x)}\\bigg]\\end{aligned}\\]Proof\\[\\begin{aligned}\\nabla_\\theta V^{\\mu_\\theta}(s) =&amp; \\textcolor{blue}{\\nabla_\\theta Q^{\\mu_\\theta} (s, \\mu_\\theta(s))} \\\\=&amp; \\nabla_\\theta \\left( r(s,\\mu_\\theta(s)) + \\gamma \\int_S P\\left(s'\\mid s,\\mu_\\theta(s)\\right)\\cdot V^{\\mu_\\theta}(s') \\,\\mathrm{d} s' \\right) \\\\=&amp; \\textcolor{red}{\\nabla_\\theta r(s,\\mu_\\theta(s))} + \\gamma \\int_S V^{\\mu_\\theta}(s') \\cdot \\textcolor{red}{\\nabla_\\theta P\\left(s'\\mid s,\\mu_\\theta(s)\\right)} \\,\\mathrm{d} s' + \\gamma \\int_S P\\left(s'\\mid s,\\mu_\\theta(s)\\right)\\cdot \\nabla_\\theta V^{\\mu_\\theta}(s') \\,\\mathrm{d} s' \\\\=&amp; \\nabla_\\theta \\mu_\\theta(s)\\cdot \\textcolor{blue}{\\nabla_a} \\left( r(s,\\mu_\\theta(s)) + \\gamma \\int_S \\textcolor{blue}{V^{\\mu_\\theta}(s')} \\cdot P\\left(s'\\mid s,\\mu_\\theta(s)\\right) \\,\\mathrm{d} s' \\right)\\Bigg\\vert_{a=\\mu_\\theta(s)} + \\gamma \\int_S P\\left(s'\\mid s,\\mu_\\theta(s)\\right)\\cdot \\nabla_\\theta V^{\\mu_\\theta}(s') \\,\\mathrm{d} s' \\\\=&amp; \\textcolor{blue}{\\nabla_\\theta \\mu_\\theta(s)\\cdot \\nabla_a Q^{\\mu_\\theta} (s, \\mu_\\theta(s))\\Big\\vert_{a=\\mu_\\theta(s)}} + \\gamma \\int_S \\textcolor{green}{P\\left(s'\\mid s,\\mu_\\theta(s)\\right)}\\cdot \\nabla_\\theta V^{\\mu_\\theta}(s') \\,\\mathrm{d} s' \\\\=&amp; \\nabla_\\theta \\mu_\\theta(s)\\cdot \\nabla_a Q^{\\mu_\\theta} (s, \\mu_\\theta(s))\\Big\\vert_{a=\\mu_\\theta(s)} + \\gamma \\int_S \\textcolor{green}{\\mathrm{Pr}(s\\to s', k=1, \\mu_\\theta)} \\cdot \\nabla_\\theta V^{\\mu_\\theta}(s') \\,\\mathrm{d} s' \\\\=&amp; \\nabla_\\theta \\mu_\\theta(s)\\cdot \\nabla_a Q^{\\mu_\\theta} (s, \\mu_\\theta(s))\\Big\\vert_{a=\\mu_\\theta(s)} + \\gamma \\int_S \\mathrm{Pr}(s\\to s', k=1, \\mu_\\theta) \\cdot \\left( \\nabla_\\theta \\mu_\\theta(s')\\cdot \\nabla_a Q^{\\mu_\\theta} (s', \\mu_\\theta(s'))\\Big\\vert_{a=\\mu_\\theta(s')} + \\gamma \\int_S \\mathrm{Pr}(s'\\to s'', k=1, \\mu_\\theta) \\cdot \\nabla_\\theta V^{\\mu_\\theta}(s'') \\,\\mathrm{d} s'' \\right) \\,\\mathrm{d} s' \\\\=&amp; \\nabla_\\theta \\mu_\\theta(s)\\cdot \\nabla_a Q^{\\mu_\\theta} (s, \\mu_\\theta(s))\\Big\\vert_{a=\\mu_\\theta(s)} + \\gamma \\int_S \\mathrm{Pr}(s\\to s', k=1, \\mu_\\theta) \\cdot \\nabla_\\theta \\mu_\\theta(s')\\cdot \\nabla_a Q^{\\mu_\\theta} (s', \\mu_\\theta(s'))\\Big\\vert_{a=\\mu_\\theta(s')} \\,\\mathrm{d} s' \\\\\t&amp;+ \\gamma^2 \\int_S \\mathrm{Pr}(s\\to s', k=1, \\mu_\\theta) \\int_S \\mathrm{Pr}(s'\\to s'', k=1, \\mu_\\theta) \\cdot \\nabla_\\theta V^{\\mu_\\theta}(s'') \\,\\mathrm{d} s'' \\,\\mathrm{d} s' \\\\=&amp;\\ldots \\\\=&amp; \\int_{x\\in S} \\sum\\limits_{k=0}^\\infty \\gamma^k \\cdot \\mathrm{Pr}(s\\to x, k, \\mu_\\theta) \\cdot \\nabla_\\theta \\mu_\\theta(x)\\cdot \\nabla_a Q^{\\mu_\\theta} (x, \\mu_\\theta(x))\\Big\\vert_{a=\\mu_\\theta(x)} \\,\\mathrm{d} x \\\\=&amp; \\frac{1}{(1-\\gamma)}\\int_{x\\in S} d^{\\mu_\\theta}(s) \\cdot \\nabla_\\theta \\mu_\\theta(x)\\cdot \\nabla_a Q^{\\mu_\\theta} (x, \\mu_\\theta(x))\\Big\\vert_{a=\\mu_\\theta(x)} \\,\\mathrm{d} x \\\\=&amp;\\frac{1}{(1-\\gamma)} \\mathbb{E}_{s\\sim d^{\\mu_\\theta}} \\bigg[ \\nabla_\\theta \\mu_\\theta(x)\\cdot \\nabla_a Q^{\\mu_\\theta} (x, \\mu_\\theta(x))\\Big\\vert_{a=\\mu_\\theta(x)}\\bigg]\\end{aligned}\\]Note that the Bellman equation here is different from the one in the stochastic case: the reward is not dependent on the next state. $\\blacksquare$  Calculating $\\nabla_a Q(s,a)$ is the result of accounting for both $\\nabla_\\theta r(s,a)$ and $\\nabla_\\theta p(sâ€™\\mid s,a)$.Performance Difference LemmaFor all policies $\\pi, \\pi^\\prime$ and states$s_0$,\\[\\begin{aligned} V^\\pi(s_0) - V^{\\pi^\\prime}(s_0) =&amp; \\mathbb{E}_{\\tau \\sim {\\Pr}^\\pi(\\tau|s_0=s) } \\left[\\sum_{t=0}^\\infty \\gamma^t A^{\\pi'}(s_t,a_t)\\right] \\\\ =&amp; \\frac{1}{1-\\gamma}\\mathbb{E}_{s\\sim d_{s_0}^\\pi }\\mathbb{E}_{a\\sim \\pi(\\cdot|s) } \\left[  A^{\\pi^\\prime}(s,a)\\right]. \\end{aligned}\\]  Kakade, Sham, and John Langford. â€œApproximately optimal approximate reinforcement learning.â€ Proceedings of the Nineteenth International Conference on Machine Learning. 2002.ProofThe proof is provided in the appendix of â€œOn the theory of policy gradient methods: Optimality, approximation, and distribution shiftâ€ and I just transcribed it here with additional details.Let $\\Pr^\\pi(\\tau \\mid s_0 = s)$ denote the probability of observing a trajectory $\\tau$ when starting in state $s$ and following the policy $\\pi$. Using a telescoping argument, we have:\\[\\begin{aligned}&amp;V^\\pi(s) - V^{\\pi'}(s) \\\\=&amp;  \\mathbb{E}_{\\tau \\sim {\\Pr}^\\pi(\\tau|s_0=s) }\\left[\\sum_{t=0}^\\infty \\gamma^t r(s_t,a_t)\\right] - V^{\\pi'}(s) \\\\=&amp; \\mathbb{E}_{\\tau \\sim {\\Pr}^\\pi(\\tau|s_0=s) }\\left[\\sum_{t=0}^\\infty \\gamma^t \\left(r(s_t,a_t)+V^{\\pi'}(s_t)-V^{\\pi'}(s_t) \\right)\\right]-V^{\\pi'}(s)\\\\\\stackrel{(a)}{=}&amp; \\mathbb{E}_{\\tau \\sim {\\Pr}^\\pi(\\tau|s_0=s) }    \\left[\\sum_{t=0}^\\infty \\gamma^t \\left(r(s_t,a_t)+\\gamma V^{\\pi'}(s_{t+1})-V^{\\pi'}(s_t)\\right)\\right]\\\\\\stackrel{(b)}{=}&amp;\\mathbb{E}_{\\tau \\sim {\\Pr}^\\pi(\\tau|s_0=s) }    \\left[\\sum_{t=0}^\\infty \\gamma^t \\left(r(s_t,a_t)+\\gamma \\mathbb{E}[V^{\\pi'}(s_{t+1})|s_t,a_t]-V^{\\pi'}(s_t)\\right)\\right]\\\\\\stackrel{(c)}{=}&amp; \\mathbb{E}_{\\tau \\sim {\\Pr}^\\pi(\\tau|s_0=s) }    \\left[\\sum_{t=0}^\\infty \\gamma^t A^{\\pi'}(s_t,a_t)\\right] \\\\=&amp; \\frac{1}{1-\\gamma}\\mathbb{E}_{s'\\sim d^\\pi_s }\\,\\mathbb{E}_{a\\sim \\pi(\\cdot | s')}    \\left[ A^{\\pi'}(s',a) \\right],\\end{aligned}\\]where $(a)$ rearranges terms in the summation and cancels the $V^{\\piâ€™}(s_0)$ term with the $-V^{\\piâ€™}(s)$ outside the summation, and $(b)$ uses the tower property of conditional expectations and the final equality follows from the definition of $d^\\pi_s$. $\\blacksquare$Details$(a)$:\\[- a_0 +\\sum\\limits_{k=0}^{\\infty} \\left(a_k - b_k \\right) = \\sum\\limits_{k=0}^{\\infty} \\left(a_{k+1} - b_k\\right).\\]$(b)$: The tower property of conditional expectations (or law of total probability):If $\\mathcal{H} \\subseteq \\mathcal{G}$, then\\[\\mathbb{E}\\left[\\mathbb{E}\\left[X\\mid \\mathcal{G} \\right] \\mid \\mathcal{H} \\right] = \\mathbb{E}\\left[X\\mid \\mathcal{H} \\right].\\]Correspondingly,  $\\mathcal{G} = \\tau \\sim {\\Pr}^\\pi(\\tau \\mid s_0=s)$,  $\\mathcal{H} = (s_t,a_t)$.$(c)$: Step $(b)$ is necessary. Note that\\[Q^{\\pi}(s, a) \\ne r(s, a) + \\gamma \\cdot V^{\\pi}(s').\\]But\\[Q^{\\pi}(s, a) = r(s, a) + \\gamma \\cdot \\sum\\limits_{s'} P(s' \\mid s,a) \\cdot V^{\\pi}(s').\\]Other proofs  Check other proofs here and here.###ConvergenceAboutThis section is based on the awesome paper:  Agarwal, Alekh, et al. â€œOn the theory of policy gradient methods: Optimality, approximation, and distribution shift.â€ The Journal of Machine Learning Research 22.1 (2021): 4431-4506.And I will provide some omitted details here. The writing of the entire note may be somewhat verbose, and this is to familiarize myself with the content.Details of Setting$V(s) \\le \\frac{1}{1-\\gamma}$$V(s)$ reaches its upper bound when $r(s,a)=1,\\forall s,a$, which equals $\\sum\\limits_{t=0}^\\infty \\gamma^t$.And it is a geometric progression:  $a_n = a_0 \\cdot \\gamma^{n-1}$,  $S_n = a_0 \\cdot \\frac{1-\\gamma^n}{1-\\gamma}$,          $S_n = a_0 + a_1 + \\ldots + a_n$,      $\\gamma\\cdot S_n = a_1 + \\ldots + a_n + a_{n+1}$,      $(1-\\gamma)\\cdot S_n = a_0\\cdot (1 - \\gamma^n)$.        $\\lim\\limits_{n\\to\\infty}S_n = \\frac{a_0}{1-\\gamma} = \\frac{1}{1-\\gamma}$.The famous theorem of Bellman and Dreyfus (1959)  The famous theorem of Bellman and Dreyfus (1959) shows there exists a policy $\\pi^\\star$ which simultaneously maximizes $V^\\pi(s_0)$, for all states $s_0\\in S$.I have read this referenced paper, and I do not find any theorem. This paper is mainly about trading additional computing time for additional memory capacity.However this statement is intuitive and is not hard to understand. Assume there is a fixed $s_{-1}$ and can be transited to $s_0$ according to $\\rho$, then this problem is equivalent to the one that has a fixed $s_0$.Direct parameterization$\\theta\\in\\Delta(A)^{\\vert S\\vert}$ means for every state $s$ the parameters are  a point in a simplex.For eample, for state $s_0$, there are actions $a_1, a_2$, the parameters of the current policy $\\pi_\\theta(\\cdot \\mid s_0)$ are  $\\theta_{s_0,a_1} = 0.2 = \\pi_\\theta(a_1 \\mid s_0)$, and  $\\theta_{s_0,a_2} = 0.8 = \\pi_\\theta(a_2 \\mid s_0)$.Softmax parameterizationSometimes it can be $\\pi_\\theta(a\\mid s) = \\frac{\\exp(\\tau\\cdot \\theta_{s,a})}{\\sum\\limits_{aâ€™}\\exp(\\tau\\cdot \\theta_{s,aâ€™})}$, which is called energy-based policy, where $\\tau$ is the temperature parameter (inverse temperature) and $\\theta_{s,a}$ is the energy function.  Haarnoja, Tuomas, et al. â€œReinforcement learning with deep energy-based policies.â€ International conference on machine learning. PMLR, 2017.$V^{\\pi_\\theta}(s)$ is non-concave (Lemma 1)We want to maximize $V^{\\pi_\\theta}(s)$, so if $V^{\\pi_\\theta}(s)$ is concave then we can apply standard tools of convex optimization.  Unfortunately it is not.As shown in the appendix, there is a MDP where exists policy points $\\pi_1, \\pi_2$ that $V^{\\pi_1}(s)+V^{\\pi_2}(s)&gt; 2\\cdot V^{\\frac{1}{2}(\\pi_1+\\pi_2)}(s)$. This shows a property of convex, so $V^{\\pi_\\theta}(s)$ is non-concave.Why is there a coefficient $(1-\\gamma)$ in $(4)$?\\[d_{s_0}^\\pi(s) := (1-\\gamma) \\sum_{t=0}^\\infty \\gamma^t {\\Pr}^\\pi(s_t=s|s_0).\\]Recall that the derivation of the policy gradient theorem:\\[\\nabla_{\\theta} V^{\\pi_\\theta}(s_0) = \\sum\\limits_{s} \\sum\\limits_{k=0}^{\\infty} \\gamma^k \\cdot \\text{Pr}(s_0\\to s, k) \\sum\\limits_{a} \\pi(a\\mid s) \\cdot Q^\\pi(s,a)\\cdot \\nabla_{\\theta}\\ln \\pi(a\\mid s).\\]  Policy Gradient      Williams, Ronald J. â€œSimple statistical gradient-following algorithms for connectionist reinforcement learning.â€ Machine learning 8 (1992): 229-256.    Sutton, Richard S., et al. â€œPolicy gradient methods for reinforcement learning with function approximation.â€ Advances in neural information processing systems 12 (1999).  Note that $\\lim\\limits_{k\\to\\infty} \\sum\\limits_{k=0}^{\\infty} \\gamma^k = \\frac{1}{1-\\gamma} &gt; 1$. The value of discounted state visitation distribution should not larger than $1$. So the coefficient $(1-\\gamma)$ is for normalization.Why is there a coefficient $\\frac{1}{1-\\gamma}$ in $(5)$?\\[\\begin{aligned}\\nabla_\\theta V^{\\pi_\\theta}(s_0) =&amp; \\frac{1}{1-\\gamma} \\, \\mathbb{E}_{s \\sim d_{s_0}^{\\pi_\\theta} }\\mathbb{E}_{a\\sim \\pi_\\theta(\\cdot | s) }\\big[\\nabla_\\theta \\log\\pi_{\\theta}(a| s) Q^{\\pi_\\theta}(s,a)\\big] \\\\=&amp; \\sum\\limits_{s} d^\\pi_{s_0}(s) \\sum\\limits_{a} \\pi(a\\mid s) \\cdot Q^\\pi(s,a)\\cdot \\nabla_{\\theta}\\log \\pi(a\\mid s).\\end{aligned}\\]It is used to cancel that normalization.Advantage\\[\\begin{aligned}A^{\\pi}(s,a):=&amp; Q^\\pi(s,a)-V^\\pi(s) \\\\=&amp; Q^\\pi(s,a) - \\sum\\limits_{a}\\pi(a\\mid s) \\cdot Q^\\pi(s,a).\\end{aligned}\\]Given $s$ and $\\pi$, $A^{\\pi}(s,a)$ measures how much better the expected future return after selecting action $a$ is compared to the expected future return of sampling action based on the current policy $\\pi$ in this state $s$.Baseline  This part partially use material from Prof. Wangâ€™s Lecture note 18: Variance reduction and Reinforcement learning: An introduction.Policy gradient is unbiased but with high variance. Recall that the form is\\[\\nabla_{\\theta} V^{\\pi_\\theta}(s_0) =\\frac{1}{1-\\gamma} \\mathbb{E}_{s\\sim d_{s_0}^\\pi}\\mathbb{E}_{a\\sim\\pi(\\cdot\\mid s)}\\left[ Q^\\pi(s,a)\\cdot \\nabla_{\\theta}\\log \\pi(a\\mid s)\\right].\\]To reduce it, a natural solution is to subtract a baseline $b(s)$ from $Q^\\pi$ which can be any function, even a random variable, as long as it does not depend on the action $a$, i.e.,\\[\\nabla_{\\theta} V^{\\pi_\\theta}(s_0) =\\frac{1}{1-\\gamma} \\mathbb{E}_{s\\sim d_{s_0}^\\pi}\\mathbb{E}_{a\\sim\\pi(\\cdot\\mid s)}\\left[ \\left(Q^\\pi(s,a) - b(s)\\right)\\cdot \\nabla_{\\theta}\\log \\pi(a\\mid s)\\right],\\]\\[\\nabla_{\\theta} V^{\\pi_\\theta}(s_0) = \\sum\\limits_{s} d^\\pi_{s_0}(s) \\sum\\limits_{a} \\pi(a\\mid s) \\cdot \\left(Q^\\pi(s,a) - b(s)\\right)\\cdot \\nabla_{\\theta}\\log \\pi(a\\mid s),\\]or\\[\\nabla_{\\theta} V^{\\pi_\\theta}(s_0) = \\sum\\limits_{s} d^\\pi_{s_0}(s) \\sum\\limits_{a} \\left(Q^\\pi(s,a) - b(s)\\right)\\cdot \\nabla_{\\theta} \\pi(a\\mid s).\\]This is still unbiased:\\[\\begin{aligned}&amp;\\sum\\limits_a b(s)\\cdot \\nabla_{\\theta} \\pi(a\\mid s) \\\\=&amp; b(s) \\cdot\\nabla_\\theta\\sum\\limits_a \\pi(a\\mid s) \\\\=&amp; b(s) \\cdot\\nabla_\\theta 1 \\\\=&amp; 0. \\end{aligned}\\]But it has lower variance:  Assume that:      $X = Q^\\pi(s,a)\\cdot \\nabla_{\\theta} \\pi(a\\mid s)$,    $Y = \\nabla_{\\theta} \\pi(a\\mid s)$,    $\\mathbb{E} \\left[ X \\right] = \\mu$,    $\\mathbb{E} \\left[ Y \\right] = \\eta = 0$,    $Xâ€™ = X + c(Y-\\eta)$.    Then:      $\\mathbb{E} \\left[ Xâ€™ \\right] = \\mu$,    \\[\\begin{aligned} \\mathbb{V} \\left[ X' \\right] =&amp; \\mathbb{V} \\left[ X + c(Y-\\eta) \\right] \\\\ =&amp; \\mathbb{V} \\left[ X \\right] + c^2\\cdot \\mathbb{V} \\left[ Y-\\eta \\right] +2c\\cdot \\text{Cov}(X,Y-\\eta) \\\\ =&amp; \\mathbb{V} \\left[ Y-\\eta \\right] \\cdot c^2 + 2\\cdot \\text{Cov}(X,Y-\\eta)\\cdot c + \\mathbb{V} \\left[ X \\right],\\end{aligned}\\]        $\\min \\mathbb{V} \\left[ Xâ€™ \\right] = \\left(1 - \\text{Corr(X,Y)}\\right)\\cdot \\mathbb{V} \\left[ X \\right]$.    Common usage: GAE (Generalized Advantage Estimation).  Schulman, John, et al. â€œHigh-dimensional continuous control using generalized advantage estimation.â€ arXiv preprint arXiv:1506.02438 (2015).Equation (6) does not hold for the direct parameterization\\[\\begin{aligned}   \\sum\\limits_a \\nabla_\\theta \\pi(a) =&amp; \\left(\\sum\\limits_a\\frac{\\partial \\pi(a)}{\\partial \\theta_1},\\ldots, \\sum\\limits_a\\frac{\\partial \\pi(a)}{\\partial \\theta_m}\\right)\\\\ \\end{aligned}\\]If every $\\frac{\\partial \\pi(a)}{\\partial \\theta_1}$ has the same variables, then $\\sum\\limits_a\\frac{\\partial \\pi(a)}{\\partial \\theta_1} = \\frac{\\partial \\sum\\limits_a\\pi(a)}{\\partial \\theta_1} = 0$. But in the case of the direct parameterization, this assumption does not hold, i.e., $\\sum\\limits_a\\frac{\\partial \\pi(a)}{\\partial \\theta_1} = 1$.Distribution mismatch coefficient (pass)I think this concept is introduced too soon. Letâ€™s discuss it later.Details on Constrained Tabular Parameterization  This algorithm is projected gradient ascent on the direct policy parametrization of the MDP.Equation $(7)$$\\mu$ is a distribution of $s_0$.\\[\\begin{aligned}&amp; \\nabla_\\theta V^{\\pi_\\theta}(\\mu) \\\\=&amp;  \\frac{\\partial V^{\\pi_\\theta}(\\mu)}{\\partial } \\\\=&amp; \\frac{1}{1-\\gamma} \\, \\mathbb{E}_{s \\sim d_{s_0}^{\\pi_\\theta} }\\mathbb{E}_{a\\sim \\pi_\\theta(\\cdot | s) }\\big[\\nabla_\\theta \\log\\pi_{\\theta}(a| s) Q^{\\pi_\\theta}(s,a)\\big] \\\\=&amp; \\sum\\limits_{s} d^\\pi_{s_0}(s) \\sum\\limits_{a} \\pi(a\\mid s) \\cdot Q^\\pi(s,a)\\cdot \\nabla_{\\theta}\\log \\pi(a\\mid s).\\end{aligned}\\]  The following part has not been finished yet. One may check my writing schedule."
  },
  
  {
    "title": "Sequence-to-Sequence Models",
    "url": "/posts/Seq2Seq/",
    "categories": "Artificial Intelligence, Machine Learning Basics",
    "tags": "tech, NLP, RNN, LSTM, GRU, Seq2Seq, transformer, BERT, GPT",
    "date": "2023-07-15 18:40:00 +0000",
    





    
    "snippet": "NLP TermsNLP = Natural Language ProcessingEmbedding  In a general sense, â€œembeddingâ€ refers to the process of representing one kind of object or data in another space or format. It involves mapping...",
    "content": "NLP TermsNLP = Natural Language ProcessingEmbedding  In a general sense, â€œembeddingâ€ refers to the process of representing one kind of object or data in another space or format. It involves mapping objects from a higher-dimensional space into a lower-dimensional space while preserving certain relationships or properties.  â€” GPT 3.5For example, the Principal Component Analysis (PCA) algorithm is an embedding technique. PCA is a widely used dimensionality reduction technique that projects high-dimensional data into a lower-dimensional space, while retaining as much of the dataâ€™s variance as possible. In my understanding, the constraint on variance in PCA is intended to allow us to distinguish each point as effectively as possible in the new lower-dimensional space. And that is the â€œpreserved propertyâ€ in this case.In NLP, the embedding layer is used to obtain the feature vector for each token in the input sequence. One-hot is a simple example of the embedding.Items $\\to$ one-hot variables:import torchimport torch.nn.functional as FA = torch.tensor([3, 0, 2])output1 = F.one_hot(A)'''tensor([[0, 0, 0, 1],        [1, 0, 0, 0],        [0, 0, 1, 0]])'''output2 = F.one_hot(A, num_classes=5)'''tensor([[0, 0, 0, 1, 0],        [1, 0, 0, 0, 0],        [0, 0, 1, 0, 0]])'''Items $\\to$ learnable embedded variables:import torchtorch.manual_seed(1)words = [\"hello\", \"world\", \"haha\"]vocab_size = len(words)idx = list(range(vocab_size))dictionary = dict(zip(words, idx))  # {'hello': 0, 'world': 1, 'haha': 2}embedding_dim = 2embedding_layer = torch.nn.Embedding(num_embeddings=vocab_size,     # how many values in a dim (input)                                     embedding_dim=embedding_dim)   # output_dimprint(list(embedding_layer.parameters()))'''[Parameter containing:tensor([[ 0.6614,  0.2669],        [ 0.0617,  0.6213],        [-0.4519, -0.1661]], requires_grad=True)]'''lookup_tensor = torch.tensor([dictionary[\"haha\"]], dtype=torch.long)haha_embed = embedding_layer(lookup_tensor)print(haha_embed)# tensor([[-0.4519, -0.1661]], grad_fn=&lt;EmbeddingBackward0&gt;)# The result is exactly the third row of the embedding layer parameter matrix.torch.nn.Embedding:  Each element in the weight matrix is sampled from $\\mathcal{N}(0,1)$.  â€œAn Embedding layer is essentially just a Linear layer.â€ (From this website.)  The padding_idx parameter: (int, optional) - If specified, the entries at padding_idx do not contribute to the gradient.  PAD_TOKEN, &lt;BOS&gt; and others should be considered in the num_embeddings count.Map back:  Given an embedding result $y$, find the corresponding word $x$.  Calculate the embeddings of all words $(y_1, \\ldots, y_n)$,  $x = \\arg\\min_i \\Vert y - y_i \\Vert$all_embeddings = embedding_layer(torch.arange(vocab_size))distances = torch.norm(all_embeddings - hello_embed, dim=1)min_index = torch.argmin(distances).item()closest_word = [word for word, idx in dictionary.items() if idx == min_index][0]print(f\"Closest word for the given vector: {closest_word}\")# Closest word for the given vector: hahaInteger (index) tensors $\\to$ learnable embedded variables:import torchvocab_size = 10embedding_dim = 3embedding_layer = torch.nn.Embedding(vocab_size, embedding_dim)print(list(embedding_layer.parameters()))'''tensor([[-2.3097,  2.8327,  0.2768],        [-1.8660, -0.5876, -0.5116],        [-0.6474,  0.7756, -0.1920],        [-1.2533, -0.7186,  1.8712],        [-1.5365, -1.0957, -0.9209],        [-0.0757,  2.3399,  0.9409],        [-0.9143,  1.3293,  0.8625],        [ 1.3818, -0.1664, -0.5298],        [ 2.2011, -0.8805,  1.7162],        [-0.9934,  0.3914,  0.9149]], requires_grad=True)]'''# A batch of 2 samples of 4 indices each# Each index is in [0, vocab_size - 1]input = torch.LongTensor([[1, 2, 4, 5],     # Sentence A                          [4, 3, 2, 9]])    # Sentence Boutput = embedding_layer(input)print(output)'''tensor([[[-1.8660, -0.5876, -0.5116],         [-0.6474,  0.7756, -0.1920],         [-1.5365, -1.0957, -0.9209],         [-0.0757,  2.3399,  0.9409]],        [[-1.5365, -1.0957, -0.9209],         [-1.2533, -0.7186,  1.8712],         [-0.6474,  0.7756, -0.1920],         [-0.9934,  0.3914,  0.9149]]], grad_fn=&lt;EmbeddingBackward0&gt;)'''Differentiable embedding  â€œAn Embedding layer is essentially just a Linear layer.â€ From this website.embedding_layer = torch.nn.Sequential(    torch.nn.Linear(input_dim, output_dim, bias=False),)Tokenization  Check this website.sentence = 'We do not see things as they are, we see them as we are.'character_tokenization = list(sentence)# ['W', 'e', ' ', 'd', 'o', ' ', 'n', 'o', 't', ' ', 's', 'e', 'e', ' ', 't', 'h', 'i', 'n', 'g', 's', ' ', 'a', 's', ' ', 't', 'h', 'e', 'y', ' ', 'a', 'r', 'e', ',', ' ', 'w', 'e', ' ', 's', 'e', 'e', ' ', 't', 'h', 'e', 'm', ' ', 'a', 's', ' ', 'w', 'e', ' ', 'a', 'r', 'e', '.']word_tokenization = sentence.split()# ['We', 'do', 'not', 'see', 'things', 'as', 'they', 'are,', 'we', 'see', 'them', 'as', 'we', 'are.']sentence_tokenization = sentence.split(', ')# ['We do not see things as they are', 'we see them as we are.']BoWBOW = Bag of WordsIn my understanding, BoW is a step  to make and clean a word-level tokenization, and  to count and store the occurrences of each word.  For instance, given the vocabulary {apple, banana, cherry}:      Text: \"apple banana apple\"    BoW representation: {2, 1, 0}    â€” ChatGPT 4Word2VecWord2Vec = Word to Vector  Word2Vec is a group of related models that are used to produce word embeddings. Word2Vec takes a large corpus of text as its input and produces a high-dimensional space (typically of several hundred dimensions), with each unique word in the corpus being assigned a corresponding vector in the space.  â€” ChatGPT 4In my understanding:  During training, the process involves presenting a set of words (the context) and predicting the surrounding words.  The model learns from reading the corpus to understand the context in which each word occurs in the language.  If a certain word at a particular position in a context appears frequently in the corpus, then when given that context, the modelâ€™s output probability for that word should also be high.CBOWCBOW = Continuous Bag of WordsThe CBOW model predicts the current word based on its context. The objective function to maximize can be expressed as:\\(J(\\theta) = \\frac{1}{T} \\sum_{t=1}^{T} \\log p(w_t | C_t)\\)Skip-GramThe Skip-Gram model predicts the context given a word. Its objective function can be expressed as:\\(J(\\theta) = \\frac{1}{T} \\sum_{t=1}^{T} \\sum_{-c \\leq j \\leq c, j \\neq 0} \\log p(w_{t+j} | w_t)\\)Here, $T$ is the total number of words in the training corpus, and $c$ is the size of the context.Illustration of CBOW and Skip-Gram from the paper â€œExploiting Similarities among Languages for Machine Translationâ€.RNN ModelsRNNRNN = Recurrent Neural Network\\[\\begin{aligned}    \\begin{cases}      h_t = f_{w_h}(x_t, h_{t-1}) \\\\      o_t = g_{w_o}(h_t) \\\\    \\end{cases}\\end{aligned}\\]Illustration of RNN from the book â€œDive into Deep Learningâ€. Boxes represent variables (not shaded) or parameters (shaded) and circles represent operators.If there are $L$ RNN layers, then\\[\\begin{aligned}    \\begin{cases}      h_t^1 = \\mathrm{Sigmoid}\\left(\\mathrm{Linear}(x_t) + \\mathrm{Linear}(h_{t-1}^1)\\right) \\\\      h_t^2 = \\mathrm{Sigmoid}\\left(\\mathrm{Linear}(h_t^1) + \\mathrm{Linear}(h_{t-1}^2)\\right) \\\\      \\ldots \\\\      o_t = \\mathrm{Linear}(h_t^L)    \\end{cases}\\end{aligned}\\]Illustration of Stacked RNN from the paper â€œRNNCon: Contribution Coverage Testing for Stacked Recurrent Neural Networksâ€.# Adapted from https://pytorch.org/docs/stable/generated/torch.nn.RNN.htmlimport torchrnn = torch.nn.RNN(10, 20, 2)   # input_size, hidden_size, num_layersinput = torch.randn(5, 3, 10)   # sequence_length, batch_size, input_sizeh0 = torch.randn(2, 3, 20)      # num_layers, batch_size, hidden_sizeoutput, hn = rnn(input, h0)     # h0: Defaults to zeros if not provided.print(output.size(), hn.size())# torch.Size([5, 3, 20]) torch.Size([2, 3, 20])# output size: sequence_length, batch_size, hidden_size# If parameter batch_first=True,# then the first parameter should be the batch_size.In NLP:  An input is a sentence. The sequence_length is the number of words in the sentence.  The input_size is the embedding dimension of each word.  The output_size equals the sequence_length.  All the hidden states have the same hidden_size.An example that makes it easy to remember the dimensions of various quantities:# input_size = embedding_dim = 3embedded_words = {\"I\": [0.3, 0.5, 0.1],                  \"am\": [0.4, 0.4, 0.3],                  \"so\": [0.1, 0.9, 0.2],                  \"happy\": [0.5, 0.3, 0.8],                  \"sad\": [0.2, 0.6, 0.1],                  \".\": [0.6, 0.2, 0.6],                  \"&lt;EOS&gt;\": [0.0, 0.0, 0.0],                  \"&lt;PAD&gt;\": [0.9, 0.9, 0.9]}sentences = [\"I am so happy.\",             \"I am sad.\"]# dataset_size = (batch_size, sequence_length, embedding_dim) = (2, 6, 3,)# sequence_length = max(sentence_length); Paddingembedded_sentences = [[[0.3, 0.5, 0.1],                       [0.4, 0.4, 0.3],                       [0.1, 0.9, 0.2],                       [0.5, 0.3, 0.8],                       [0.2, 0.6, 0.1],                       [0.0, 0.0, 0.0]],                      [[0.3, 0.5, 0.1],                       [0.4, 0.4, 0.3],                       [0.5, 0.3, 0.8],                       [0.6, 0.2, 0.6],                       [0.0, 0.0, 0.0],                       [0.9, 0.9, 0.9]]]# hidden_size, num_layers: determined by parameter tuning :)BPTTBPTT = Backpropagation Through Time\\(L = \\frac{1}{T}\\sum\\limits_{t} l(o_t,y_t)\\)\\(\\begin{aligned}    \\frac{\\partial L}{\\partial w_o} =&amp; \\frac{1}{T}\\sum\\limits_{t} \\frac{\\partial l(y_t,o_t)}{\\partial w_o} \\\\    =&amp; \\frac{1}{T}\\sum\\limits_{t}    \\frac{\\partial l(y_t,o_t)}{\\partial o_t}\\cdot     \\frac{\\partial o_t}{\\partial w_o}\\end{aligned}\\)\\(\\begin{aligned}    \\frac{\\partial L}{\\partial w_h} =&amp; \\frac{1}{T}\\sum\\limits_{t} \\frac{\\partial l(y_t,o_t)}{\\partial w_h} \\\\    =&amp; \\frac{1}{T}\\sum\\limits_{t} \\frac{\\partial l(y_t,o_t)}{\\partial o_t} \\cdot     \\frac{\\partial o_t}{\\partial h_t} \\cdot      \\textcolor{red}{\\frac{\\partial h_t}{\\partial w_h}}\\end{aligned}\\)\\[\\begin{aligned}    \\textcolor{red}{\\frac{\\partial h_t}{\\partial w_h}} =&amp;    \\frac{\\partial f(x_t, h_{t-1}, w_h)}{\\partial w_h} + \\frac{\\partial f(x_t, h_{t-1}, w_h)}{\\partial h_{t-1}} \\cdot     \\textcolor{red}{\\frac{\\partial h_{t-1}}{\\partial w_h}}\\end{aligned}\\]\\[\\begin{cases}      z_0 = a_0 = 0 \\\\      z_k = a_k + b_k \\cdot z_{k-1} \\\\\\end{cases}\\]\\[\\begin{aligned}    z_k = a_k + \\sum\\limits_{i=0}^{k-1}        \\left(\\prod\\limits_{j=i+1}^k b_j \\right) \\cdot a_i\\end{aligned}\\]LSTMLSTM = Long Short-Term Memory  To learn long-term dependencies (owing to vanishing and exploding gradients).  In my understanding, there are two kinds of hidden states, $\\mathrm{c}$ and $\\mathrm{h}$. And $\\mathrm{c}$ is renamed as the memory cell internal state.Each layer is like:\\[\\begin{aligned}  \\begin{cases}      i_t =&amp; \\mathrm{Sigmoid}\\left(\\mathrm{Linear}(x_t) + \\mathrm{Linear}(h_{t-1})\\right) \\\\      f_t =&amp; \\mathrm{Sigmoid}\\left(\\mathrm{Linear}(x_t) + \\mathrm{Linear}(h_{t-1})\\right) \\\\      g_t =&amp; \\mathrm{tanh}\\left(\\mathrm{Linear}(x_t) + \\mathrm{Linear}(h_{t-1})\\right) \\\\      o_t =&amp; \\mathrm{Sigmoid}\\left(\\mathrm{Linear}(x_t) + \\mathrm{Linear}(h_{t-1})\\right) \\\\      c_t =&amp; f_t \\odot c_{t-1} + i_t \\odot g_t \\\\      h_t =&amp; o_t \\odot \\mathrm{tanh}(c_t)  \\end{cases}\\end{aligned}\\]  Adapted from the PyTorch document.  $h_t$: the hidden state.  $c_t$: the cell state.  $i_t$: the input gate.  $f_t$: the forget gate.  $g_t$: the cell gate.  $o_t$: the output gate.  All values of the three gates are in the range of $(0, 1)$ because of the sigmoid function.  $g_t$ is the vanilla part of an RNN, and it indicates the information that we currently get.  $i_t$ controls how much we cares about the current information.  $c$ is an addtional hidden state channel, and it also indicates the memory.  $f_t$ controls how much we cares about the memory.  $c_t$ and $h_t$ do not impact the curren output $o_t$.Illustration of LSTM from the book â€œDive into Deep Learningâ€.# Adapted from https://pytorch.org/docs/stable/generated/torch.nn.LSTM.htmlimport torchrnn = torch.nn.LSTM(10, 20, 2)    # input_size, hidden_size, num_layersinput = torch.randn(5, 3, 10)     # sequence_length, batch_size, input_sizeh0 = torch.randn(2, 3, 20)        # num_layers, batch_size, hidden_sizec0 = torch.randn(2, 3, 20)        # num_layers, batch_size, hidden_sizeoutput, (hn, cn) = rnn(input, (h0, c0)) # h0: Defaults to zeros if not provided.print(output.size(), hn.size(), cn.size())# torch.Size([5, 3, 20]) torch.Size([2, 3, 20]) torch.Size([2, 3, 20])# output size: sequence_length, batch_size, hidden_size# If parameter batch_first=True,# then the first parameter should be the batch_size.GRUGRU = Gated Recurrent UnitsEach layer is like:\\[\\begin{aligned}  \\begin{cases}      r_t =&amp; \\mathrm{Sigmoid}\\left(\\mathrm{Linear}(x_t) + \\mathrm{Linear}(h_{t-1})\\right) \\\\      z_t =&amp; \\mathrm{Sigmoid}\\left(\\mathrm{Linear}(x_t) + \\mathrm{Linear}(h_{t-1})\\right) \\\\      n_t =&amp; \\mathrm{tanh}\\left(\\mathrm{Linear}(x_t) + r_t \\odot \\mathrm{Linear}(h_{t-1})\\right) \\\\      h_t =&amp; (1-z_t)\\odot n_t + z \\odot h_{t-1}  \\end{cases}\\end{aligned}\\]  $h_t$: the hidden state. It can be used as the output.  $r_t$: the reset gate, controls how much we cares about the memory. It is a bit like the forget gate $f_t$ in LSTM  $z_t$: the update gate, controls how much we cares about the current information. It is a bit like the input gate $i_t$ in LSTM.  $n_t$: the candidate hidden state, or the new gate.          If the reset gate $r_t$ is close to $1$, then it is like the vanilla RNN.      If the reset gate $r_t$ is close to $0$, then the new gate $n_t$ is the result of an MLP of $x_t$.      Illustration of GRU from the book â€œDive into Deep Learningâ€.# Adapted from https://pytorch.org/docs/stable/generated/torch.nn.GRU.htmlimport torchrnn = torch.nn.GRU(10, 20, 2)   # input_size, hidden_size, num_layersinput = torch.randn(5, 3, 10)   # sequence_length, batch_size, input_sizeh0 = torch.randn(2, 3, 20)      # num_layers, batch_size, hidden_sizeoutput, hn = rnn(input, h0)     # h0: Defaults to zeros if not provided.print(output.size(), hn.size())# torch.Size([5, 3, 20]) torch.Size([2, 3, 20])# output size: sequence_length, batch_size, hidden_size# If parameter batch_first=True,# then the first parameter should be the batch_size.Encoder-DecoderVariable-length inputs  Truncation and Padding          Dive Into Deep Learning 10.5.3        Relation Network          A blog      ICLR 2017        Embedding.  Encoder-decoder.  In general sequence-to-sequence problems like machine translation (Section 10.5), inputs and outputs are of varying lengths that are unaligned. The standard approach to handling this sort of data is to design an encoderâ€“decoder architecture (Fig. 10.6.1) â€¦ â€” Dive into Deep Learning.The structureIllustration of the encoder-decoder architecture from the book â€œDive into Deep Learningâ€.  Encoder: \"Hello, world.\" $\\to$ a hidden state (or context variable) of fixed-shape.  Decoder 1: the state $\\to$ \"ä½ å¥½ï¼Œä¸–ç•Œã€‚\"  Decoder 2: the state $\\to$ \"Hola mundo\"Illustration of the encoder-decoder architecture (teacher forcing) from the book â€œDive into Deep Learningâ€.Illustration of the encoder-decoder architecture (prediction) from the book â€œDive into Deep Learningâ€.The encoder and the decoder are usually RNNs.  &lt;eos&gt; means the end of the sequence.          Inputting &lt;eos&gt; into the encoder indicates the end of this sentence.      In prediction: When the decoder outputs &lt;eos&gt;, it will automatically stop and no longer continue generating output.        &lt;bos&gt; means the beginning of the sequence, used to signal the decoder when to begin generating a new sequence.  The input of the encoder is a variable-length sequence, but its output is of fixed-length, named as the state or the context variable $c$.  $c = q(h_1, \\ldots, h_t)$, where $q$ is a customized function. In the figures, $c = h_t$.  The context variable will be fed into the decoder at evry time step or at the first time step.  Teacher Forcing: The input of the decoder is (&lt;bos&gt;, sequence), and the target is (sequence, &lt;eos&gt;).  Prediction: The input of the decoder at every time step is the output from the previous time step.  When calculating the loss, the padding tokens are masked.Illustration of the encoder-decoder architecture where the RNNs are stacked, from the book â€œDive into Deep Learningâ€.Teacher forcingTeacher Forcing: The input of the decoder is (&lt;bos&gt;, sequence), and the target is (sequence, &lt;eos&gt;).  Without using teacher forcing, the model at each timestep would receive the output from the previous timestep and use this output to predict the next timestep. However, this approach has an inherent problem: early in training, the model is likely to produce incorrect predictions, leading the next timestep prediction to be based on this incorrect output. Such mistakes can accumulate in subsequent timesteps.  To combat this, the Teacher Forcing technique is introduced during training. Specifically, instead of the model receiving its prediction from the previous timestep, it directly receives the actual output from the previous timestep. In this way, even if the model makes an error at a particular timestep, it can continue making predictions based on the actual data, preventing error accumulation.  â€” ChatGPT 4Code# Generated by ChatGPT 4import torchimport torch.nn as nnimport torch.optim as optimclass Encoder(nn.Module):    def __init__(self, input_dim, emb_dim, hidden_dim, n_layers, dropout):        super().__init__()        self.embedding = nn.Embedding(input_dim, emb_dim)        self.rnn = nn.RNN(emb_dim, hidden_dim, n_layers, dropout=dropout)        self.dropout = nn.Dropout(dropout)    def forward(self, src):        embedded = self.dropout(self.embedding(src))        outputs, hidden = self.rnn(embedded)        return hiddenclass Decoder(nn.Module):    def __init__(self, output_dim, emb_dim, hidden_dim, n_layers, dropout):        super().__init__()        self.output_dim = output_dim        self.embedding = nn.Embedding(output_dim, emb_dim)        self.rnn = nn.RNN(emb_dim, hidden_dim, n_layers, dropout=dropout)        self.fc_out = nn.Linear(hidden_dim, output_dim)        self.dropout = nn.Dropout(dropout)    def forward(self, input, hidden):        input = input.unsqueeze(0)        embedded = self.dropout(self.embedding(input))        output, hidden = self.rnn(embedded, hidden)        prediction = self.fc_out(output.squeeze(0))        return prediction, hiddenclass Seq2Seq(nn.Module):    def __init__(self, encoder, decoder, device):        super().__init__()        self.encoder = encoder        self.decoder = decoder        self.device = device    def forward(self, src, trg, teacher_forcing_ratio=0.5):        trg_len = trg.shape[0]        batch_size = trg.shape[1]        trg_vocab_size = self.decoder.output_dim        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)        hidden = self.encoder(src)        input = trg[0, :]        for t in range(1, trg_len):            output, hidden = self.decoder(input, hidden)            outputs[t] = output            teacher_force = torch.rand(1).item() &lt; teacher_forcing_ratio            top1 = output.argmax(1)            input = trg[t] if teacher_force else top1        return outputsINPUT_DIM = 1000OUTPUT_DIM = 1000ENC_EMB_DIM = 256DEC_EMB_DIM = 256HID_DIM = 512N_LAYERS = 2ENC_DROPOUT = 0.5DEC_DROPOUT = 0.5device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT)model = Seq2Seq(enc, dec, device).to(device)optimizer = optim.Adam(model.parameters())criterion = nn.CrossEntropyLoss()def train(model, iterator, optimizer, criterion, clip):    model.train()    epoch_loss = 0    count = 0    for i, (src, trg) in enumerate(iterator):        optimizer.zero_grad()        output = model(src, trg)        output_dim = output.shape[-1]        output = output[1:].view(-1, output_dim)        trg = trg[1:].view(-1)        loss = criterion(output, trg)        loss.backward()        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)        optimizer.step()        epoch_loss += loss.item()        count = i    return epoch_loss / (count + 1)def main():    src = torch.randint(0, INPUT_DIM, (10, 32)).to(device)  # sequence length 10, batch szie 32    trg = torch.randint(0, OUTPUT_DIM, (10, 32)).to(device)    dataset = [(src, trg) for _ in range(100)]  # 100 batch    iterator = iter(dataset)    N_EPOCHS = 10    CLIP = 1    for epoch in range(N_EPOCHS):        train_loss = train(model, iterator, optimizer, criterion, CLIP)        print(f'Epoch: {epoch + 1:02} | Train Loss: {train_loss:.3f}')def evaluate(model, iterator, criterion):    model.eval()    epoch_loss = 0    count = 0    with torch.no_grad():        for i, (src, trg) in enumerate(iterator):            output = model(src, trg, 0)  # 0 means not using teacher forcing            output_dim = output.shape[-1]            output = output[1:].view(-1, output_dim)            trg = trg[1:].view(-1)            loss = criterion(output, trg)            epoch_loss += loss.item()            count = i    return epoch_loss / (count + 1)def main_test():    src_test = torch.randint(0, INPUT_DIM, (10, 32)).to(device)  # sequence_length 10,  batchsize 32    trg_test = torch.randint(0, OUTPUT_DIM, (10, 32)).to(device)    test_dataset = [(src_test, trg_test) for _ in range(50)]  # 50 batch    test_iterator = iter(test_dataset)    test_loss = evaluate(model, test_iterator, criterion)    print(f'Test Loss: {test_loss:.3f}')if __name__ == \"__main__\":    main()    main_test()Transformer  The Transformer, BERT, and GPT architectures do not use RNNs. Instead, they rely on the self-attention mechanism to process sequences.  â€” ChatGPT 4Queries, Keys, and Values  A data set $\\mathcal{D}:={ (k_i, v_i) \\mid i\\in {1, \\ldots, n} }$.          $k$ is the key, $v$ is the value.      It is a dictionary (in python).        We input the query $q$ to search the data set.  The program returns the value most relevant $v_{i^*}$, where $i^* = \\arg\\min_{i} \\Vert q - x_i \\Vert$.keys = range(1, 8, 3)  # [1, 4, 7]values = [\"xixi\", \"haha\", \"wuwu\"]data_set = dict(zip(keys, values))# {1: 'xixi', 4: 'haha', 7: 'wuwu'}def search(query: int):    distances = [abs(query - key_i) for key_i in keys]    idx_optimal = distances.index(min(distances))    key_optimal = keys[idx_optimal]    value_optimal = data_set[key_optimal]    return value_optimalprint(search(query=3))  # hahaAttention  Attention is all you need.\\[\\mathrm{Attention}(q, \\mathcal{D}) := \\sum\\limits_{i=1}^n v_i \\cdot \\alpha(q, k_i)\\]  $\\alpha(q, k_i)$ is usually a function of the distance between $q$ and $k_i$, reflecting their similarity.  $\\boldsymbol\\alpha = (\\alpha(q, k_1), \\ldots, \\alpha(q, k_n))$ should be a convex combination.          $\\alpha(q, k_i) \\ge 0, \\forall i$      $\\sum\\limits_{i=1}^n \\alpha(q, k_i) = 1$        If $\\boldsymbol\\alpha$ is one-hot, then the attention mechanism is just like the traditional database query.Illustration of the attention mechanism from the book â€œDive into Deep Learningâ€.Common similarity kernels\\[\\boldsymbol\\alpha(q, \\boldsymbol{k}) = \\mathrm{softmax}(\\textcolor{blue}{(}f(\\Vert q - k_1 \\Vert), \\ldots, f(\\Vert q - k_n \\Vert)\\textcolor{blue}{)})\\]$f$ is the similarity kernel (or Parzen Windows).  $f(\\Vert q - k \\Vert) = \\exp\\left(-\\frac{1}{2}\\Vert q-k \\Vert^2\\right)$ (Gaussian)  $f(\\Vert q - k \\Vert) = 1 \\mathrm{if} \\Vert q-k \\Vert \\le 1$ (Boxcar)  $f(\\Vert q - k \\Vert) = \\max(0, 1- \\Vert q-k \\Vert )$ (Epanechikov)Attention Scoring FunctionsCalculating the distances between queries and keys costs a lot. So we should find another efficient way to quantify the similarity between them. The function that measures this similarity is named the scoring function.Illustration of the attention mechanism from the book â€œDive into Deep Learningâ€.  The following part has not been finished yet. One may check my writing schedule.BERTBERT = Bidirectional Encoder Representations from TransformersGPTGPT = Generative Pre-trained Transformer"
  },
  
  {
    "title": "MARL Basics",
    "url": "/posts/MARL-Basics/",
    "categories": "Artificial Intelligence, Multi-Agent Reinforcement Learning",
    "tags": "tech, multi agents, reinforcement learning, Markov model",
    "date": "2023-06-29 09:38:00 +0000",
    





    
    "snippet": "  This note has not been finished yet. One may check my writing schedule.Markov Models  MDP          Markov decision process      $(S, A, \\mathcal{P}, R, \\gamma)$      Single-agent, fully observabl...",
    "content": "  This note has not been finished yet. One may check my writing schedule.Markov Models  MDP          Markov decision process      $(S, A, \\mathcal{P}, R, \\gamma)$      Single-agent, fully observable, and dynamic.        POMDP          Partially observable Markov decision process      $(S, A, \\mathcal{P}, R, O, \\mathcal{Q}, \\gamma)$      Single-agent, partially observable, and dynamic.        Dec-POMDP          Decentralized partially observable Markov decision process      $(I, S,\\set{A^i}_{i\\in I}, \\mathcal{P}, R, \\set{O^i}_{i\\in I}, \\mathcal{Q}, \\gamma)$      Multi-agent, fully observable, cooperative, and dynamic.        Stochastic games (or Markov games)          $(I, S,\\set{A^i}_{i\\in I}, \\mathcal{P}, \\set{R^i}_{i\\in I}, \\gamma)$      Multi-agent, fully observable, C/A/M (meaning that it can be cooperative, adversarial, or mixed-motive), and dynamic.      where  $I$ is a set of agents (and $\\vert I\\vert = n$, where $n\\ge 2$),  $S$ is the state space,  $A$ is the action space,  $P: S\\times A \\to \\Delta(S)$ is the state transition function,  $R: S\\times A$  (TODO)  Normal-form game (or repeated games)          $(I,\\set{A^i}_{i\\in I}, \\set{R^i}_{i\\in I})$      Multi-agent, C/A/M, and static.        Extensive-form game          $(I, X, Z, f,\\set{A^i}_{i\\in I}, \\set{R^i}_{i\\in I})$      Multi-agent, C/A/M, and dynamic.        Bayesian gameCheck this note, A Memo on Game Theory.(TODO)Some potentially confusing concepts:  Stochastic game  Markov game  Repeated game  Matrix game  Static game  Dynamic game  Normal-form game  Extensive-form game  Bayesian gameLearning GoalsThe learning goals of MARL are mainly stability and adaptation.StabilityStability is an inherent characteristic of the learning process within the algorithm we are examining. It signifies the achievement of convergence towards stationary policies or other measurable criteria.Here are some common criteria (in repeated games, where $t$ means the number of times the agents have played) listed, with their strength gradually increasing (Bowling 2004):  Average reward $\\sum\\limits_{t\\le T} (r_t\\cdot \\pi_t)/T$;  Empirical distribution of actions $\\sum\\limits_{t\\le T} (\\pi_t)/T$;  Expected reward $r_t\\cdot \\pi_t$;  Policies $\\pi_t$.The analysis of equilibrium should take dynamic environment into account.  I have a question: Do the resulting policies form an equilibrium when convergence occurs? What if the it converges to a suboptimal case or failed due to insufficient exploration?AdaptationThis concept includes several aspects:  The performance of agents should be maintained or improved.  Agents can be adaptive to the update of the othersâ€™ policies since the others are learning too.  Agents can avoid exploitation by the others.  When agents have several optimal actions in coordination cases (which are common), they can achieve coordination to some extent.To check whether the designed algorithm for agents are adaptive or not, there are two criteria:  Rationality: Whether each agent converges to a best response when the other agents remain stationary.          But even in this kind of equilibrium, agents can be exploited by the others. Rationality cannot prevent the learner from â€˜being exploitedâ€™ by the other agents.        No-regret: An algorithm has no-regret $\\Leftrightarrow$ the average regret is less than or equal to $0$ against all othersâ€™ policies.          No-regret can prevent the learner from â€˜being exploitedâ€™ by the other agents.      Regret $\\mathscr{R}$      Scenarios (or Tasks)There are three types of scenarios based on their properties: fully cooperative, fully competitive, and mixed scenarios. These scenarios are classified according to the reward functions assigned to the agents: a scenario is  fully cooperative if all the reward functions are the same function (i.e. $r^i=r^j\\ \\forall i, j\\in I$),          (Without a centralized controller, a coordination problem will arise. Discussed below.)        fully competitive if $r^i = -r^j$ (two-agent scenarios are mostly discussed in this case),          (Games in this case are called zero-sum.)        and mixed otherwise.Some additional points:  If there is no constraint on the reward functions, the game is called general-sum game, and pure cooperative games and zero-sum games are special cases.  The second and the third case are called noncooperative games.  In coordination games, rewards are always positively related.AlgorithmsMARL algorithms are designed for variant tasks. Some of them are built relying on the assumptions of the tasks, making them applicable only to those particular tasks.For fully cooperative tasksIn this case, if a centralized controller is available, the task reduces to a MDP. Otherwise the agents take actions independently, and a coordination problem arises. When there are multiple equilibria, the agents should coordinate to break ties in a same way.The algorithms designed for the fully cooperative scenarios without centralized controller (i.e. the coordination problems) can be classified into three categories based on the dimension of coordination:  Coordination-free methods: teammate-independent  Direct coordination methods:          teammate-independent if it rely on common-knowledge,      and teammate-aware if they use negotiation.        Indirect coordination methods: team-aware(TODO)  Team-Q: It is the Q-learning algorithm, assuming that the optimal joint action are unique (which will rarely be the case).  Distributed-Q: It is deterministic. Every one can see the  OAL  JAL  FMQFor fully cooperative tasks  minimax-QFor mixed-motive tasksStatic  Fictitious play  MetaStrategy  IGA  WoLF-IGA  GIGA  GIGA-WoLF  AWESOME  Hyper-QDynamic  Single-agent RL  Nash-Q  CE-Q  Asymmetric-Q  NSCP  EIRA  WoLF-PHC  PD-WoLF  EXORLReading List  Convergence and No-Regret in Multiagent Learning. Michael Bowling. Advances in Neural Information Processing Systems (NIPS) 2004.          GIGA-WoLF (Generalized Infinitesimal Gradient Ascent, Win or Learn Fast)        Multi-Agent Reinforcement Learning: A survey. L. Busoniu, R. Babuska, B. De Schutter. International Conference on Control, Automation, Robotics and Vision (ICARCV) 2006.          This note is basically built on the framework of this paper.        Playing is Believing: The Role of Beliefs in Multi-Agent Learning. Yu-Han Chang, Leslie Pack Kaelbling. Advances in Neural Information Processing Systems (NIPS) 2001.  Nash Q-Learning for General-Sum Stochastic Games. Junling Hu, Michael P. Wellman. Journal of machine learning research (JMLR) 2003.  Convergence of Q-learning: A Simple Proof. Francisco S. Melo.  Online Convex Programming and Generalized Infinitesimal Gradient Ascent. Martin Zinkevich. International Conference on Machine Learning (ICML) 2003.  Value-Function Reinforcement Learning in Markov Games. Michael L. Littman. Cognitive Systems Research 2001.  Multi-Agent Reinforcement Learning: A Critical Survey. Yoav Shoham, Rob Powers, Trond Grenager. Technical report, Stanford University 2003.  An Overview of Multi-Agent Reinforcement Learning from Game Theoretical Perspective. Yaodong Yang, Jun Wang. arXiv 2020.  A Survey and Critique of Multiagent Deep Reinforcement Learning. Pablo Hernandez-Leal, Bilal Kartal, Matthew E. Taylor. Autonomous Agents and Multi-Agent Systems (AAMAS) 2019.  Multi-Agent Reinforcement Learning: A Selective Overview of Theories and Algorithms. Kaiqing Zhang, Zhuoran Yang, Tamer Basar. Handbook of reinforcement learning and control 2021.  Disclaimer: The above content partially uses materials from the cited papers. Corresponding links or references have been provided."
  },
  
  {
    "title": "Computation Graph Visualization",
    "url": "/posts/Computation-Graph-Visualization/",
    "categories": "Code",
    "tags": "tech, computation graph, visualization, torchviz, torchsummary, graphviz, reparameterization",
    "date": "2023-06-24 18:00:01 +0000",
    





    
    "snippet": "PyTorchvizBasics  Install          brew install graphviz (or here)      pip install torchviz        Documentation: Github  Official examples: Colab  If a node represents a backward function, it is ...",
    "content": "PyTorchvizBasics  Install          brew install graphviz (or here)      pip install torchviz        Documentation: Github  Official examples: Colab  If a node represents a backward function, it is gray. Otherwise, the node represents a tensor and is either blue, orange, or green:      Blue: reachable leaf tensors that requires grad (tensors whose .grad fields will be populated during .backward())    Orange: saved tensors of custom autograd functions as well as those saved by built-in backward nodes    Green: tensor passed in as outputs    Dark green: if any output is a view, we represent its base tensor with a dark green node.  Example 1: Basicsimport torchfrom torchviz import make_dotif __name__ == '__main__':    batch_size, input_size, hidden_size, output_size = 100, 4, 2, 1    critic = torch.nn.Sequential(        torch.nn.Linear(in_features=input_size, out_features=hidden_size, bias=False, dtype=torch.float),        torch.nn.Tanh(),        torch.nn.Linear(in_features=hidden_size, out_features=output_size, bias=True, dtype=torch.float),    )    states = torch.rand(batch_size, input_size, dtype=torch.float)    values = critic(states).squeeze()    graph = make_dot(values, params=dict(critic.named_parameters()), show_attrs=True, show_saved=True)    graph.view()Example 2: Without Netimport torchfrom torchviz import make_dotif __name__ == '__main__':    x = torch.rand(100, 5, requires_grad=True)    f1 = 2 * x + 3    f2 = torch.softmax(f1, dim=-1)    g = 5 * f1    h = f2+g    graph = make_dot(h, show_attrs=True, show_saved=True)    graph.view()Example 3: Detachimport torchfrom torchviz import make_dotif __name__ == '__main__':    x = torch.rand(100, 5, requires_grad=True)    f1 = 2 * x + 3    f2 = torch.softmax(f1, dim=-1)    g = 5 * f1.detach()    h = f2+g    graph = make_dot(h, show_attrs=True, show_saved=True)    graph.view()Example 4: $\\nabla_\\theta a$ âŒimport torchfrom torchviz import make_dotif __name__ == '__main__':    batch_size, input_size, output_size = 10, 4, 2    actor = torch.nn.Sequential(        torch.nn.Linear(in_features=input_size, out_features=output_size, bias=False, dtype=torch.float),        torch.nn.Softmax(dim=-1)    )    states = torch.rand(batch_size, input_size, dtype=torch.float)    pi_list = actor(states)    distributions = torch.distributions.Categorical(pi_list)    actions = distributions.sample().squeeze(dim=0)    graph = make_dot(actions, params=dict(actor.named_parameters()), show_attrs=True, show_saved=True)    graph.view()No gradient.Example 5: $\\nabla_\\theta a$ with Gumbel-Softmax (Reparameterization)import torchfrom torchviz import make_dotif __name__ == '__main__':    batch_size, input_size, output_size = 10, 4, 2    temperature = 1    actor = torch.nn.Sequential(        torch.nn.Linear(in_features=input_size, out_features=output_size, bias=False, dtype=torch.float),        torch.nn.Softmax(dim=-1)    )    states = torch.rand(batch_size, input_size, dtype=torch.float)    pi_list = actor(states)    logits = torch.log(pi_list)    actions = torch.nn.functional.gumbel_softmax(logits, tau=temperature, hard=True)    graph = make_dot(actions, params=dict(actor.named_parameters()), show_attrs=True, show_saved=True)    graph.view()  Applying Gumbel-Softmax may cause NaN during training. Changing the data type of the variable to float64 seems to have avoided this issue.  Jang, Eric, Shixiang Gu, and Ben Poole. â€œCategorical reparameterization with gumbel-softmax.â€ arXiv preprint arXiv:1611.01144 (2016).Model StructureIf the visualization object is an nn.Module, then its structure can be easily inspected using the approaches in this section.Printimport torchif __name__ == '__main__':    batch_size, input_size, hidden_size, output_size = 100, 4, 2, 1    critic = torch.nn.Sequential(        torch.nn.Linear(in_features=input_size, out_features=hidden_size, bias=False, dtype=torch.float),        torch.nn.Tanh(),        torch.nn.Linear(in_features=hidden_size, out_features=output_size, bias=True, dtype=torch.float),    )    print(critic)Sequential(  (0): Linear(in_features=4, out_features=2, bias=False)  (1): Tanh()  (2): Linear(in_features=2, out_features=1, bias=True))Process finished with exit code 0Torchsummarypip install torchsummaryimport torchfrom torchsummary import summaryif __name__ == '__main__':    batch_size, input_size, hidden_size, output_size = 100, 4, 2, 1    critic = torch.nn.Sequential(        torch.nn.Linear(in_features=input_size, out_features=hidden_size, bias=False, dtype=torch.float),        torch.nn.Tanh(),        torch.nn.Linear(in_features=hidden_size, out_features=output_size, bias=True, dtype=torch.float),    )        summary(critic, input_size=(batch_size, 4))----------------------------------------------------------------        Layer (type)               Output Shape         Param #================================================================            Linear-1               [-1, 100, 2]               8              Tanh-2               [-1, 100, 2]               0            Linear-3               [-1, 100, 1]               3================================================================Total params: 11Trainable params: 11Non-trainable params: 0----------------------------------------------------------------Input size (MB): 0.00Forward/backward pass size (MB): 0.00Params size (MB): 0.00Estimated Total Size (MB): 0.01----------------------------------------------------------------Process finished with exit code 0"
  },
  
  {
    "title": "Dynamic Epistemic Logic",
    "url": "/posts/Dynamic-Epistemic-Logic/",
    "categories": "Mathematics",
    "tags": "tech, math, epistemic logic, common knowledge, logic, induction puzzles, multi agents",
    "date": "2023-06-22 18:00:00 +0000",
    





    
    "snippet": "  Three logicians walk into a bar.The bartender asks: â€œDo you all want a drink?â€The first logician says: â€œI donâ€™t know.â€The second logician says: â€œI donâ€™t know.â€The third logician says: â€œYes.â€What ...",
    "content": "  Three logicians walk into a bar.The bartender asks: â€œDo you all want a drink?â€The first logician says: â€œI donâ€™t know.â€The second logician says: â€œI donâ€™t know.â€The third logician says: â€œYes.â€What is Dynamic Epistemic Logic?1  Dynamic epistemic logic (DEL) is a logical framework dealing with knowledge and information change.Typically, DEL focuses on situations involving multiple agents and studies how their knowledge changes when events occur.There are two kinds of events:  Ontic events: These events can change factual properties of the actual world, e.g., a red card is painted in blue.  Epistemic events: These events can change agentâ€™s knowledge without changing factual properties of the world, e.g., a card is revealed publicly (or privately) to be red.  Originally, DEL focused on epistemic events. In computer science, DEL is for example very much related to multi-agent systems, which are systems where multiple intelligent agents interact and exchange information.I discovered that this topic shares some relevance with the theory of mind in certain aspects. Both of them concern the knowledge of agents and involve analyzing how the agents modeling others.However this is perhaps not what I am looking for, for it being a modal logic. So this blog simply aims to provide a brief overview of the topic, focusing on the game-theoretic part of it.  Epistemic logic is a modal logic dealing with the notions of knowledge and belief. As a logic, it is concerned with understanding the process of reasoning about knowledge and belief.Some relevant keywords:  Common knowledge (logic),  Induction puzzles,  Dynamic epistemic logic.Induction Puzzles2A good way to understand this topic is through examples of the problems that it aims to investigate, namely induction puzzles.  Induction puzzles are logic puzzles, which are examples of multi-agent reasoning, where the solution evolves along with the principle of induction.A puzzleâ€™s scenario always involves multiple players with the same reasoning capability, who go through the same reasoning steps.A logical feature:  According to the principle of induction, a solution to the simplest case makes the solution of the next complicated case obvious. Once the simplest case of the induction puzzle is solved, the whole puzzle is solved subsequently.Basic settings:  Players know informaiton about others but does not know their owns.          Typical tell-tale features of these puzzles include any puzzle in which each participant has a given piece of information (usually as common knowledge) about all other participants but not themselves.        Players are smart and are capaple of theory of mind.          Also, usually, some kind of hint is given to suggest that the participants can trust each otherâ€™s intelligence â€” they are capable of theory of mind (that â€œevery participant knows modus ponensâ€ is common knowledge).        â€œDoing nothingâ€ is also an action and indicates something.          Also, the inaction of a participant is a non-verbal communication of that participantâ€™s lack of knowledge, which then becomes common knowledge to all participants who observed the inaction.        The introduction of â€œcommon knowledgeâ€ will be presented in the next section.The Muddy Children PuzzleThis is one of the most classic examples of inductin puzzles. This problem also has other descriptive forms and variations, such as the Blue-Eyed Islanders and cheating wives/husbands puzzles.Description  There is a set of attentive children. They think perfectly logically. The children consider it possible to have a muddy face. None of the children can determine the state of their own face themselves. But, every child knows the state of all other childrenâ€™s faces. A custodian tells the children that at least one of them has a muddy face. The children are each told that they should step forward if they know that their own face is muddy. Hereafter, the custodian starts to count and after every stroke, every muddy child has an opportunity to step forward.Let us extract the core of it.What is the goal?Players (children) want to find out a piece of information (whether their face is clean or not) that they do not know but others do.What are the assumptions?  Before the game starts, there will be an outsider who provides some indirect information.  Players know informaiton about others but does not know their owns.  There are occasions when making decisions openly (the chances for children to step forward), and the actions taken become common knowledge.  Players are smart and are capaple of theory of mind.  If players know their own information, they will reveal it to their own best interests.  â€œDoing nothingâ€ is also an action and indicates something. Specifically, it means that a player does not know its own information. And this is new information to others.Logical solutionAssume that there are $n$ children and $k$ of them are dirty where $k&gt;1$.  Letâ€™s assume that there are just 2 children: Alice and Bob. If only Alice is dirty, she will step forward at the first stroke, because she does not see any other dirty faces. The same is true for Bob. If Alice sees Bob not stepping forward at the first stroke, she must conclude that he certainly sees another muddy child and they will step forward simultaneously at the second stroke.Letâ€™s assume that there are just 3 children: Alice, Bob, and Charly. If there are less than 3 muddy children, the puzzle evolves like in the case with 2 children. If Charly sees that Alice and Bob are muddy and not stepping forward at the second stroke, they all together will step forward at the third stroke.It can be proven that $k$ muddy children will step forward after $k$ strokes.Denote Alice as $i$ and Bob as $j$. There are three cases:  $i$ is dirty and $j$ is not.          $i$ knows $j$ is not dirty. So $i$ can infer that it is dirty. Thus $i$ will step forward at the first stroke.      $j$ knows $i$ is dirty but $j$ cannot infer its situation. But after the first stroke $j$ will know that â€œ$i$ can infer itself is dirtyâ€, and thus $j$ can infer that $j$ is not dirty.        $j$ is dirty and $i$ is not. The analysis is symmetrical to that of the first case.  $i$ and $j$ are both dirty.          $i$ knows $j$ is dirty but $i$ cannot infer its situation. Thus $i$ will do nothing at the first stroke.      $j$ knows $i$ is dirty but $j$ cannot infer its situation. Thus $j$ will do nothing at the first stroke.      After the first stroke, $i$ will know that $j$ cannot infer anything, meaning that it is not the first case (not â€œYou are dirty but I am notâ€). So $i$ can infer that it is dirty. Thus $i$ will step forward at the second stroke.      So do $j$. Thus $j$ will step forward at the second stroke.        There is a straightforward proof by induction that the first $k âˆ’ 1$ times he asks the question, they will all say â€œNo,â€ but then the $k$th time the children with muddy foreheads will all answer â€œYes.â€3So what information does â€œAt least one of you has mud on your foreheadâ€ bring to players? The common knowledge.  Let us denote the fact â€œat least one child has a muddy foreheadâ€ by $p$. Notice that if $k &gt; 1$, i.e., more than one child has a muddy forehead, then every child can see at least one muddy forehead, and the children initially all know $p$. Thus, it would seem that the father does not provide the children with any new information, and so he should not need to tell them that $p$ holds when $k &gt; 1$.But this is false! What the father provides is common knowledge. If exactly $k$ children have muddy foreheads, then it is straightforward to see that $E^{kâˆ’1}p$ holds before the father speaks, but $E^kp$ does not (here $E^k\\varphi$ means $\\varphi$, if $k = 0$, and everyone knows $E^{k-1}\\varphi$, if $k \\ge 1$). The fatherâ€™s statement actually converts the childrenâ€™s state of knowledge from $E^{kâˆ’1}p$ to $Cp$ (here $Cp$ means that there is common knowledge of $p$). With this extra knowledge, they can deduce whether their foreheads are muddy.In the muddy children puzzle, the children do not actually need common knowledge; Ekp suffices for them to figure out whether they have mud on their foreheads.3I found an alternative and more general notation:  $p$: A piece of information.  $K_i p$: Agent $i$ knows $p$.  $B_i p$: Agent $i$ believes $p$.  $E p := \\land_{i} K_i p$: All agents know $p$.  $C p := Ep \\land EEp \\land EEEp \\land \\ldots$: Common knowledge.  Two interaction axioms: $K_i p \\to B_i p$ (i.e. if $i$ knows $p$ then it believes $p$) and $B_i p \\to KB_i p$ (i.e. if $i$ believes $p$ then it knows that it believes $p$).Game-theoretic solution  Muddy children puzzle can also be solved using backward induction from game theory.The reference is from this paper, written in German.  Muddy children puzzle can be represented as an extensive form game of imperfect information. Every player has two actions â€” stay back and step forwards. There is a move by nature at the start of the game, which determines the children with and without muddy faces. Children do not communicate as in non-cooperative games. Every stroke is a simultaneous move by children. It is a sequential game of unlimited length. The game-theoretic solution needs some additional assumptions:      All children are rational and all childrenâ€™s rationality is common knowledge. This means that Alice is rational, Alice knows that Bob is rational and Alice knows that Bob knows that Charly is rational and so on and vice versa.    Stepping forward without having a muddy face results in a big penalty.    Stepping forward with a muddy face results in a reward.    Every stroke results in minor negative penalty aka discount factor for every child until any of them stepped forward. Any multiple of the minor penalty is always a lesser evil than the big penalty.    If only Alice is muddy, the last assumption makes it irrational for her to hesitate. If Alice and Bob are muddy, Alice knows that Bobâ€™s only reason of staying back after the first stroke is the apprehension to receive the big penalty of stepping forward without a muddy face. In the case with $k$ muddy children, receiving $k$ times the minor penalty is still better than the big penalty.This is a non-cooperative extensive-form (sequential) game of imperfect information.  Agents: finite, rational (common knowledge), and not hesitant.  States: All agentsâ€™ face situations.  Actions: Do nothing, or step forward. Everyone moves  simultaneously at every timestep.  Observations: The othersâ€™ face situations.  Reward function:          $R^i(s^i=\\text{clean face}, a^i=\\text{step forward}) = -\\infty$,      $R^i(s^i=\\text{muddy face}, a^i=\\text{step forward}) = r$, where $r$ is a predefined constant reward.      $R^i(\\cdot, a^i=\\text{step forward}) = 0$, I guess?      So this is just formulation. What about the method â€œbackward inductionâ€?  I found a paper about this, cited 181 times. I will read it later:Rational Dynamics and Epistemic Logic in Games.Johan Van Benthem.International Game Theory Review 2007.Hat Puzzles  One type of induction puzzle concerns the wearing of colored hats, where each person in a group can only see the color of those worn by others, and must work out the color of their own.The Kingâ€™s Wise Men Hat Puzzle  The King called the three wisest men in the country to his court to decide who would become his new advisor. He placed a hat on each of their heads, such that each wise man could see all of the other hats, but none of them could see their own. Each hat was either white or blue. The king gave his word to the wise men that at least one of them was wearing a blue hat; in other words, there could be one, two, or three blue hats, but not zero. The king also announced that the contest would be fair to all three men. The wise men were also forbidden to speak to each other. The king declared that whichever man stood up first and correctly announced the colour of his own hat would become his new advisor. The wise men sat for a very long time before one stood up and correctly announced the answer. What did he say, and how did he work it out?Common Knowledge  The notion of common knowledge, where everyone knows, everyone knows that everyone knows, etc., has proven to be fundamental in various disciplines, including Philosophy, Artificial Intelligence, Game Theory, Psychology, and Distributed Systems. This key notion was first studied by the philosopher David Lewis in the context of conventions. Lewis pointed out that in order for something to be a convention, it must in fact be common knowledge among the members of a group. (For example, the convention that green means â€œgoâ€ and red means â€œstopâ€ is presumably common knowledge among the drivers in our society.)3  Common knowledge also arises in discourse understanding. Suppose Ann asks Bob â€œWhat did you think of the movie?â€ referring to a showing of Monkey Business they have just seen. Not only must Ann and Bob both know that â€œthe movieâ€ refers to Monkey Business, but Ann must know that Bob knows (so that she can be sure that Bob will give a reasonable answer to her question), Bob must know that Ann knows that Bob knows (so that Bob knows that Ann will respond appropriately to his answer), and so on. In fact, by a closer analysis of this situation, it can be shown that there must be common knowledge of what movie is meant in order for Bob to answer the question appropriately.3  This note is incomplete, and I have no intention of completing it because I realized it is not the topic I am looking for.References            Wikipedia: Dynamic Epistemic Logic.Â &#8617;              Wikipedia: Induction PuzzlesÂ &#8617;              Ronald Fagin, Joseph Y. Halpern, Yoram Moses, Moshe Y. Vardi. â€œCommon knowledge revisited.â€ Annals of Pure and Applied Logic (1999).Â &#8617;Â &#8617;2Â &#8617;3Â &#8617;4      "
  },
  
  {
    "title": "Theory of Mind and Markov Models",
    "url": "/posts/ToM-MM/",
    "categories": "Artificial Intelligence, Multi-Agent Reinforcement Learning",
    "tags": "tech, theory of mind, Markov model, reinforcement learning, multi agents",
    "date": "2023-06-19 12:00:01 +0000",
    





    
    "snippet": "  We do not see things as they are, we see them as we are. â€” AnaÃ¯s Nin.What is Theory of Mind?  In psychology, theory of mind refers to the capacity to understand other people by ascribing mental s...",
    "content": "  We do not see things as they are, we see them as we are. â€” AnaÃ¯s Nin.What is Theory of Mind?  In psychology, theory of mind refers to the capacity to understand other people by ascribing mental states to them (that is, surmising what is happening in their mind). This includes the knowledge that othersâ€™ beliefs, desires, intentions, emotions, and thoughts may be different from oneâ€™s own.  Possessing a functional theory of mind is considered crucial for success in everyday human social interactions. People use such a theory when analyzing, judging, and inferring othersâ€™ behaviors. The discovery and development of theory of mind primarily came from studies done with animals and infants.  Empathyâ€”the recognition and understanding of the states of mind of others, including their beliefs, desires, and particularly emotionsâ€”is a related concept. Empathy is often characterized as the ability to â€œput oneself into anotherâ€™s shoesâ€. Recent neuro-ethological studies of animal behaviour suggest that even rodents may exhibit empathetic abilities. While empathy is known as emotional perspective-taking, theory of mind is defined as cognitive perspective-taking.1In my understanding, theory of mind refers to the ability of an individual modeling othersâ€™ decision making processes based on othersâ€™ partial observations.Basic Markov ModelsDec-POMDPA decentralized partially observable Markov decision process (Dec-POMDP) is a tuple $(I, S,\\set{A^i}_{i\\in I}, \\mathcal{P}, R, \\set{O^i}_{i\\in I}, \\mathcal{Q}, \\gamma)$, where  $I$ is a set of agents ($\\vert I\\vert=n$ and they are cooperative),  $S$ is a set of global states of the environment (and agents cannot see the sampled state at any time, but they know the state set),  $A^i$ is a set of actions for agent $i$, with $\\boldsymbol{A}=\\times_{i\\in I} A^i$ is the set of joint actions,  $\\mathcal{P}:S\\times \\boldsymbol{A}\\to\\Delta(S)$ is the state transition probability where $\\Delta(S)$ is a set of distributions over $S$,  $R:S\\times \\boldsymbol{A}\\to \\mathbb{R}$ is a reward function (not $\\mathbb{R}^n$ since the agents are cooperative),  $O^i$ is a set of observations for agent $i$, with $\\boldsymbol{O} = \\times_{i\\in I} O^i$ is the set of joint observations,  $\\mathcal{Q}:S\\times \\boldsymbol{A}\\to\\Delta(\\boldsymbol{O})$ is an observation emission function (sometimes $\\mathcal{Q}:S\\to\\Delta(\\boldsymbol{O})$), and  $\\gamma\\in[0,1]$ is the discount factor.One step of the process is: $(s_{t}, \\boldsymbol{o}_{t})\\to \\boldsymbol{a}_{t}\\to (s_{t+1} \\to (\\boldsymbol{o}_{t+1}, r_{t}))$. At each timestep $t$,  each agent takes an action $a_t^i\\in A^i$ based on its belief of the current state, given its observable $o_t^i$ and previous belief (the term â€œbeliefâ€ will be introduced later),  $s_{t+1}\\sim \\mathcal{P}(\\cdot \\mid s_t, \\boldsymbol{a}_t)$,  $\\boldsymbol{o}_{t+1} \\sim \\mathcal{Q}(\\cdot \\mid s_{t+1}, \\boldsymbol{a}_{t})$ and a reward is generated for the whole team based on the reward function $R(s,\\boldsymbol{a})$.These timesteps repeat until some given horizon (called finite horizon) or forever (called infinite horizon). The discount factor $\\gamma$ maintains a finite sum in the infinite-horizon case ($\\gamma \\in [0,1)$). The goal is to maximize expected cumulative reward over a finite or infinite number of steps.1What is different from what I previously thought is that the observations are sampled after agents make decisions at each timestep.  This definition is adapted from that of Wikipedia2. When discussing Dec-POMDP, these papers34 are often referenced.MDP &amp; POMDPMarkov decision processes (MDPs) and partially observable Markov decision processes (POMDPs) are degenerated cases of Dec-POMDPs:  Dec-POMDP: $(I, S,\\set{A^i}_{i\\in I}, \\mathcal{P}, R, \\set{O^i}_{i\\in I}, \\mathcal{Q}, \\gamma)$.  POMDP: $(S, A, \\mathcal{P}, R, O, \\mathcal{Q}, \\gamma)$, a single-agent version of Dec-POMDP.  MDP: $(S, A, \\mathcal{P}, R, \\gamma)$, a fully observable version of POMDP.  One may check this slides5 for understanding the comparison between MDP, POMDP, and Dec-POMDP.Belief  So how does the agent know which state it is in, in POMDPs?Since it is the state that affects payoffs and the state transitions (thus the future payoffs) rather than the observation, the agent needs to estimate the current state $s_t$ by its previous observations before choosing $a_t$.The state is Markovian by assumption, meaning that maintaining a belief over the current states $s_t$ solely requires knowledge of  the previous belief state $b(\\cdot\\mid a_{t-1}, o_{t})$,          the taken action $a_{t-1}$,      and the current observation $o_{t}$.        and the environmentâ€™s model          the sets $S$ and $O$,      the observation emission function $\\mathcal{Q}$,      and the state transition function $\\mathcal{P}$.      The belief $b$ is a posterior distribution over states. The conditional probability of a current state $s_t$ can be recursively calculated by Bayesâ€™ rule as follows:If the agent has access to the environmentâ€™s model , given $a_{t-1}, o_{t}$, and $b(\\cdot\\mid o_{t-1}, a_{t-2})$ (or $b_{t-1}$ in short), then\\[\\begin{aligned}b\\Big(s_{t}\\mid o_{t}, a_{t-1}, b_{t-1} \\Big)     &amp;= \\frac{        P(o_{t}\\mid s_t, a_{t-1}, b_{t-1})\\cdot         P(s_t\\mid a_{t-1}, b_{t-1})    }    {        P(o_{t}\\mid a_{t-1}, b_{t-1})    }\\\\    &amp;= \\frac{        P(o_{t}\\mid s_t, a_{t-1})\\cdot         \\sum\\limits_{s_{t-1}}         P(s_{t}\\mid s_{t-1}, a_{t-1})         \\cdot b_{t-1}(s_{t-1})    }    {        \\sum\\limits_{s_t}        P(o_{t}\\mid s_t, a_{t-1})\\cdot         \\sum\\limits_{s_{t-1}}         P(s_{t}\\mid s_{t-1}, a_{t-1})         \\cdot b_{t-1}(s_{t-1})    }\\\\    &amp;= \\frac{        \\mathcal{Q}(o_{t}\\mid s_t, a_{t-1})         \\sum\\limits_{s_{t-1}}         \\mathcal{P}(s_{t}\\mid s_{t-1}, a_{t-1})         \\cdot b_{t-1}(s_{t-1})    }    {        \\sum\\limits_{s_{t}}        \\mathcal{Q}(o_{t}\\mid s_t, a_{t-1})         \\sum\\limits_{s_{t-1}}         \\mathcal{P}(s_{t}\\mid s_{t-1}, a_{t-1})        \\cdot b_{t-1}(s_{t-1})    }.\\end{aligned}\\]Note that\\[P(o_{t}\\mid a_{t-1}, b_{t-1}) = \\sum\\limits_{s_{t}}        \\mathcal{Q}(o_{t}\\mid s_t, a_{t-1})         \\sum\\limits_{s_{t-1}}         \\mathcal{P}(s_{t}\\mid s_{t-1}, a_{t-1})        \\cdot b_{t-1}(s_{t-1}),\\]and we will catch it later.  This definition is adapted from that of Wikipedia6. And I found its original definition is a bit of confusing, for agent observing $o$ after reaching $sâ€™$. I suppose that $o$ is ought to be $o_{t+1}$ or $oâ€™$.Theory of Mind in Dec-POMDPsFuchs et al. proposed nested beliefs for deep RL in Hanabi.7 And in this section, I will focus on the settings of their paper rather than their method, since the method is specifically designed to tackle the Hanabi problem.Consider a two-player game. Each agent makes decisions based on its belief of the otherâ€™s policy. So the two agentsâ€™ polices are recursively dependent:  $i$ makes decisions based on $j$â€™s policy.  And $j$ acts the same way.  If $i$ become aware of the second step, then $i$ will speculate how $j$ is guessing it and make decisions based on that.  And so forth.Formally, a belief at depth $k$ is given by $b_k^{ij\\ldots}=\\Delta(S_k)$, such that $\\vert ij\\ldots \\vert = k+1$, and the subscript of $S$ indicates the depth. The beliefs at even depth are self-reflective, and the ones at odd depth model the otherâ€™s modeling. E.g.,  $b_0^{i}$ is $i$â€™s prior knowledge of states.  $b_1^{ij}$ is $i$â€™s belief about $j$â€™s prior knowledge that $i$ models. It is still a distribution over states.  $b_2^{iji}$ is $i$â€™s belief about $b_1^{ji}$. It is still a distribution over states.My questions  I am not sure why the agentâ€™s high-level beliefs are still distributions over states rather than distributions over the former level beliefs.  After we have the tool of belief, what can we do? How should agents make decisions based on their beliefs?An exampleSimplified Action Decoder (SAD). See my other note.  Hu, Hengyuan, and Jakob N. Foerster. â€œSimplified Action Decoder for Deep Multi-Agent Reinforcement Learning.â€ International Conference on Learning Representations. 2019.Belief MDPGiven a POMDP $(S, A, \\mathcal{P}, R, O, \\mathcal{Q}, \\gamma)$, the corresponding belief MDP is $(B, A, \\tau, R, \\gamma)$, where  $B$ is the belief states set, and each element in it is a distribution over the states of the POMDP,  $A$ is the same as the one of the POMDP,  $\\tau: B\\times A\\to\\Delta(B)$ is the belief state transition function,  $\\mathcal{R}: B\\times A \\to \\mathbb{R}$ is the reward function on belief states,  $\\gamma$ is the same as the one of the POMDP.More specifically,\\[\\tau(b_{t+1} \\mid b_t, a_t) = \\sum\\limits_{o_{t+1}} P(b_{t+1}\\mid b_t, a_t, o_{t+1}) \\cdot P(o_{t+1}\\mid b_t, a_t),\\]where $P(o_{t+1}\\mid b_t, a_t)$ is defined in the previous section and\\[P(b_{t+1}\\mid b_t, a_t, o_{t+1}) = \\begin{cases}  1 &amp; \\text{if the belief update with } b_t, a_t, o_{t+1} \\text{ returns } b_{t+1}, \\\\  0 &amp; \\text{otherwise}.\\end{cases}\\]And $\\mathcal{R}(b, a) = \\sum\\limits_{s} b(s) \\cdot R(s,a)$.Compared to the original POMDP, the corresponding belief MDP is not partially observable anymore, and the agent makes decisions at each timestep based on the current belief state. Its policy is denoted as $\\pi(b)$, and its goal is $\\max\\limits_{\\pi} V_{\\pi}(b_0)$, where $V_{\\pi}(b_0) = \\mathbb{E}_\\pi\\left[    \\sum\\limits_{t=0}^{\\infty}\\gamma^t\\cdot     \\mathcal{R}(b_t,a_t)\\mid b_0\\right].$6References            Wikipedia: Theory of Mind.Â &#8617;Â &#8617;2              Wikipedia: Dec-POMDP.Â &#8617;              Daniel S Bernstein, Robert Givan, Neil Immerman, Shlomo Zilberstein. â€œThe complexity of decentralized control of markov decision processes.â€ Mathematics of operations research (2002).Â &#8617;              Frans A Oliehoek, Christopher Amato. â€œA concise introduction to decentralized POMDPs.â€ Springer (2016).Â &#8617;              Alina Vereshchakaâ€™s slides about MDPs.Â &#8617;              Wikipedia: POMDP.Â &#8617;Â &#8617;2              Andrew Fuchs, Michael Walton, Theresa Chadwick, Doug Lange. â€œTheory of mind for deep reinforcement learning in hanabi.â€ NeurIPS Workshop (2019).Â &#8617;      "
  },
  
  {
    "title": "Theoretical Computer Science (TCS)",
    "url": "/posts/TCS-Basics/",
    "categories": "Mathematics",
    "tags": "tech, math, theoretical computer science, computational complexity theory, reduction, complexity classes",
    "date": "2023-06-17 12:00:01 +0000",
    





    
    "snippet": "  This note will be consistently updated.What is TCS?(Wikipedia) Theoretical computer science (TCS) is a subset of general computer science and mathematics that focuses on mathematical aspects of c...",
    "content": "  This note will be consistently updated.What is TCS?(Wikipedia) Theoretical computer science (TCS) is a subset of general computer science and mathematics that focuses on mathematical aspects of computer science such as the theory of computation, lambda calculus, and type theory.Topics that I might come across:  Algorithms  Computational complexity theory  Computational learning theory  Information-based complexity  Information theory  Machine learning  currently I mainly focus on the computational complexity theory.What is Computational Complexity Theory?(Wikipedia)  Computational complexity theory is a branch of the theory of computation that focuses on classifying comiputational problems according to their inherent diffculty, and relating those classes to each other.  A computational problem is understood to be a task that is in principle amenable to being solved by a computer, which is equivalent to stating that the problem may be solved by mechanical application of mathematical steps, such as an algorithm.  A problem is regarded as inherently difficult if its solution requires significant resources, whatever the algorithm used. The theory formalizes this intuition, by introducing mathematical models of computation to study these problems and quantifying the amount of resources needed to solve them, such as time and storage. Other complexity measures are also used, such as the amount of communication (used in communication complexity), the number of gates in a circuit (used in circuit complexity) and the number of processors (used in parallel computing).  One of the roles of computational complexity theory is to determine the practical limits on what computers can and cannot do.Computational ProblemsProblem instancesRepresenting problem instancesComplexity classesMisc Important ConceptsReduction(Wikipedia)In computability theory and computational complexity theory, a reduction is an algorithm for transforming one problem into another problem. A sufficiently efficient reduction from one problem to another may be used to show that the second problem is at least as difficult as the first.IntractabilityBig $O$ notation  This part partially uses material from this website and Wikipedia.  Big-oh is about finding an asymptotic upper bound.$f(x) = O\\left(g(x)\\right)$, iff (if and only if) $\\exists 0&lt;k, 0&lt;x_0$, s.t.$f(x)\\le k\\cdot g(x), \\forall x_0\\le x$.After $x_0$, there is a $k\\cdot g(x)$ which is the upper bound of $f(x)$. (So in my understanding, the complexity means the upper bound.)It is read â€œ$f(x)$ is big O of $g(x)$â€.Comparison:  $f(x) = O\\left(g(x)\\right)$, iff $\\exists 0&lt;k, 0&lt;x_0$, s.t.$f(x)\\le k\\cdot g(x), \\forall x_0\\le x$.          The upper bound of $f(x)$ after $x_0$.      $\\lim\\limits_{x\\to\\infty} \\frac{f(x)}{g(x)} &lt; \\infty$.        $f(x) = \\Omega\\left(g(x)\\right)$, iff $\\exists 0&lt;k, 0&lt;x_0$, s.t.$f(x)\\ge k\\cdot g(x), \\forall x_0\\le x$.          The lower bound of $f(x)$ after $x_0$.      $\\lim\\limits_{x\\to\\infty} \\frac{f(x)}{g(x)} &gt; 0$.        $f(x) = \\Theta\\left(g(x)\\right)$, iff $f(x) = O\\left(g(x)\\right)$ and $f(x) = \\Omega\\left(g(x)\\right)$.          The exact bound of $f(x)$.      $\\lim\\limits_{x\\to\\infty} \\frac{f(x)}{g(x)} \\in \\mathbb{R}_{&gt;0}$.        $f(x) = o\\left(g(x)\\right)$, iff $f(x) = O\\left(g(x)\\right)$ and $f(x)$ is not $\\Theta\\left(g(x)\\right)$.          The upper bound of $f(x)$ excluding the exact bound.      $\\lim\\limits_{x\\to\\infty} \\frac{f(x)}{g(x)} = 0$.      Big O can also be used to describe the error term in an  approximation to a mathematical function.\\[\\begin{aligned}e^x =&amp; 1+x+\\frac{x^2}{2!}+\\frac{x^3}{3!}+\\frac{x^4}{4!}+\\dots &amp; \\forall x \\\\=&amp; 1+x+\\frac{x^2}{2!}+O(x^3) &amp; x\\to 0\\\\=&amp; 1+x+O(x^2) &amp; x\\to 0  \\\\\\end{aligned}\\]Compared with $x^2$, $x^3$ is closer to $0$, when $x\\to 0$.  Disclaimer: The above content is summarized from Wikipedia and other sources. Corresponding links or references have been provided."
  },
  
  {
    "title": "Principal Component Analysis",
    "url": "/posts/PCA/",
    "categories": "Mathematics",
    "tags": "tech, math, PCA",
    "date": "2023-06-16 06:40:00 +0000",
    





    
    "snippet": "  å¾ˆä¹…ä»¥å‰çš„ç¬”è®°ä»‹ç»PCA is a widely used dimensionality reduction technique that projects high-dimensional data into a lower-dimensional space, while retaining as much of the dataâ€™s variance as possible.My ...",
    "content": "  å¾ˆä¹…ä»¥å‰çš„ç¬”è®°ä»‹ç»PCA is a widely used dimensionality reduction technique that projects high-dimensional data into a lower-dimensional space, while retaining as much of the dataâ€™s variance as possible.My understanding is that the constraint on variance in PCA is intended to allow us to distinguish each point as effectively as possible in the new lower-dimensional space.ç®—æ³•æœ‰ä¸ª$m$ç»´çš„éšæœºå˜é‡$X$ï¼Œæœ‰$n$ä¸ªæ•°æ®ï¼Œé‚£ä¹ˆ$X$æ˜¯ä¸ª$m\\times n$çš„çŸ©é˜µã€‚ç°åœ¨è¦æŠŠç»´åº¦ä»$m$é™åˆ°$k$ï¼Œè®©è¿™äº›æ ·æœ¬ç‚¹åœ¨é™ä½åçš„ç»´åº¦ä¸Šå’ŒåŸæ¥çš„ç‚¹ä¸€ä¸€å¯¹åº”ï¼Œä¸”ç‚¹ä¹‹é—´å°½å¯èƒ½è¿œç¦»  æŠŠæ•°æ®ä¸­å¿ƒåŒ–ï¼šæŠŠè¿™$n$ä¸ªæ•°æ®ç®—ä¸ªå‡å€¼ï¼Œç„¶åæ¯ä¸ªæ•°æ®éƒ½å‡å»å‡å€¼ï¼ˆè¿™ä¸€æ­¥çš„ç›®çš„æ˜¯å¿«é€Ÿåœ°ç®—åæ–¹å·®çŸ©é˜µï¼‰  ç®—åæ–¹å·®çŸ©é˜µï¼šä¸­å¿ƒåŒ–åçš„æ•°æ®çŸ©é˜µï¼Œè‡ªå·±ä¹˜è‡ªå·±ï¼Œå°±æ˜¯åæ–¹å·®çŸ©é˜µï¼Œ$\\frac{1}{n-1}XX^\\intercal$æˆ–è€…$\\frac{1}{n}XX^\\intercal$          è¿™ä¸€æ­¥æ˜¯ä¸ºäº†æ±‚åæ–¹å·®çŸ©é˜µçš„ç‰¹å¾å‘é‡ï¼Œé™¤ä»¥$n$æˆ–è€…$n-1$ä¸å½±å“æ±‚ç‰¹å¾å‘é‡      æŒ‰ç†è¯´æ˜¯é™¤ä»¥$n-1$ï¼Œå› ä¸ºæ˜¯æ±‚çš„æ ·æœ¬æ–¹å·®ï¼Œé™¤ä»¥è¿™ä¸ªæ˜¯æ— åä¼°è®¡ï¼ˆæ ¹æ®Besselâ€™s correctionï¼Œæ±‚æ–¹å·®æ—¶ç”¨åˆ°äº†å‡å€¼ï¼Œæ‰€ä»¥å¾—å°‘ä¸€ä¸ªè‡ªç”±åº¦ï¼‰      è€Œä¸”å‡å€¼æ˜¯0äº†ï¼Œä¹Ÿæ²¡æœ‰ç”¨åˆ°å‡å€¼ï¼Œæ‰€ä»¥å¯ä»¥é™¤ä»¥$n$        æ±‚åæ–¹å·®çŸ©é˜µçš„ç‰¹å¾å€¼å’Œç‰¹å¾å‘é‡  æŠŠç‰¹å¾å€¼æœ€å¤§çš„é‚£$k$ä¸ªç‰¹å¾å‘é‡æœ€ä¸ºæ–°çš„ç©ºé—´çš„åŸºåæ ‡ï¼Œç„¶åæŠŠåŸç©ºé—´æŠ•å½±ä¸Šå»ä¾‹å­  Generated by ChatGPT 3.5PCAï¼ˆä¸»æˆåˆ†åˆ†æï¼‰çš„è®¡ç®—æ­¥éª¤å¦‚ä¸‹ï¼Œæˆ‘å°†ä¸ºä½ æä¾›ä¸€ä¸ªç®€åŒ–çš„ä¾‹å­å’Œè¯¦ç»†çš„è®¡ç®—è¿‡ç¨‹ï¼šå‡è®¾æˆ‘ä»¬æœ‰ä»¥ä¸‹äºŒç»´æ•°æ®é›†ï¼š            æ•°æ®ç‚¹      x      y                  A      1      2              B      2      3              C      3      4              D      4      5              E      5      6      æ­¥éª¤ 1ï¼šæ•°æ®ä¸­å¿ƒåŒ–è®¡ç®—æ¯ä¸ªç»´åº¦çš„å‡å€¼å¹¶ä»æ¯ä¸ªæ•°æ®ç‚¹ä¸­å‡å»å‡å€¼ã€‚\\[\\mathrm{Mean}(x) = \\frac{1 + 2 + 3 + 4 + 5}{5} = 3\\]\\[\\mathrm{Mean}(y) = \\frac{2 + 3 + 4 + 5 + 6}{5} = 4\\]            æ•°æ®ç‚¹      x - Mean(x)      y - Mean(y)                  A      -2      -2              B      -1      -1              C      0      0              D      1      1              E      2      2      æ­¥éª¤ 2ï¼šè®¡ç®—åæ–¹å·®çŸ©é˜µè®¡ç®—æ•°æ®çš„åæ–¹å·®çŸ©é˜µã€‚åæ–¹å·®çŸ©é˜µçš„å…ƒç´ $(i, j)$è¡¨ç¤ºç¬¬ $i$ ç»´å’Œç¬¬ $j$ ç»´ä¹‹é—´çš„åæ–¹å·®ã€‚\\[\\mathrm{Covariance}(x, x) = \\frac{\\sum{(x - \\mathrm{Mean}(x))^2}}{n-1} = 2.5\\]\\[\\mathrm{Covariance}(y, y) = \\frac{\\sum{(y - \\mathrm{Mean}(y))^2}}{n-1} = 2.5\\]\\[\\mathrm{Covariance}(x, y) = \\frac{\\sum{(x - \\mathrm{Mean}(x))(y - \\mathrm{Mean}(y))}}{n-1} = 2.5\\]åæ–¹å·®çŸ©é˜µï¼š\\[\\begin{bmatrix}\\mathrm{Cov}(x, x) &amp; \\mathrm{Cov}(x, y) \\\\\\mathrm{Cov}(y, x) &amp; \\mathrm{Cov}(y, y) \\\\\\end{bmatrix}=\\begin{bmatrix}2.5 &amp; 2.5 \\\\2.5 &amp; 2.5 \\\\\\end{bmatrix}\\]æ­¥éª¤ 3ï¼šè®¡ç®—ç‰¹å¾å‘é‡å’Œç‰¹å¾å€¼å¯¹åæ–¹å·®çŸ©é˜µè¿›è¡Œç‰¹å¾åˆ†è§£ï¼Œå¾—åˆ°ç‰¹å¾å‘é‡å’Œç‰¹å¾å€¼ã€‚ç‰¹å¾æ–¹ç¨‹ $\\mathrm{det}(\\mathrm{Covariance} - \\lambda I) = 0$ç‰¹å¾å€¼ï¼š$\\lambda_1 = 5$, $\\lambda_2 = 0$å¯¹äº $\\lambda_1 = 5$ï¼Œè§£æ–¹ç¨‹ $\\mathrm{Covariance} - \\lambda_1 I = 0$ï¼Œå¾—åˆ°ç‰¹å¾å‘é‡ $v_1 = \\begin{bmatrix} 1 \\ 1 \\end{bmatrix}$ã€‚å¯¹äº $\\lambda_2 = 0$ï¼Œè§£æ–¹ç¨‹ $\\mathrm{Covariance} - \\lambda_2 I = 0$ï¼Œå¾—åˆ°ç‰¹å¾å‘é‡ $v_2 = \\begin{bmatrix} -1 \\ 1 \\end{bmatrix}$ã€‚æ­¥éª¤ 4ï¼šé€‰æ‹©ä¸»æˆåˆ†é€‰æ‹©ç‰¹å¾å€¼æœ€å¤§çš„ç‰¹å¾å‘é‡ä½œä¸ºä¸»æˆåˆ†ã€‚åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œé€‰æ‹© $v_1 = \\begin{bmatrix} 1 \\ 1 \\end{bmatrix}$ ä½œä¸ºä¸»æˆåˆ†ï¼Œå› ä¸ºå®ƒå¯¹åº”çš„ç‰¹å¾å€¼ $\\lambda_1 = 5$ æœ€å¤§ã€‚æ­¥éª¤ 5ï¼šæŠ•å½±å°†æ•°æ®ç‚¹æŠ•å½±åˆ°ä¸»æˆåˆ†ä¸Šã€‚æŠ•å½± A: $[1, 2]\\cdot [1, 1]^T=3$æŠ•å½± B: $[2, 3]\\cdot [1, 1]^T = 5$æŠ•å½± C: $[3, 4] \\cdot [1, 1]^T = 7$æŠ•å½± D: $[4, 5] \\cdot [1, 1]^T = 9$æŠ•å½± E: $[5, 6] \\cdot [1, 1]^T = 11$è¿™æ ·ï¼Œæˆ‘ä»¬å¾—åˆ°äº†åœ¨ä¸»æˆåˆ†æ–¹å‘ä¸Šçš„æŠ•å½±å€¼ï¼Œå®ƒä»¬å¯ä»¥ç”¨äºè¡¨ç¤ºé™ç»´åçš„æ•°æ®ã€‚åœ¨å®é™…åº”ç”¨ä¸­ï¼Œé€šå¸¸ä¼šé€‰æ‹©å¤šä¸ªä¸»æˆåˆ†æ¥ä¿ç•™æ›´å¤šçš„æ•°æ®æ–¹å·®ã€‚æˆ‘çš„ç†è§£ä¹‹æ‰€ä»¥æ˜¯æ±‚åæ–¹å·®çŸ©é˜µå¹¶è®©å…¶å¯¹è§’åŒ–ï¼ŒæŠŠä½¿å…¶å¯¹è§’åŒ–çš„é‚£ä¸ªæ­£äº¤çŸ©é˜µæ‹¿æ¥å¯¹åŸæ¥çš„ç»´åº¦è¿›è¡Œå˜åŒ–ï¼Œæ˜¯å› ä¸ºå½¢å¼åˆšåˆšå¥½ä¸€è‡´é¦–å…ˆç›®çš„æ˜¯ä¸¤ä¸ª  ä¸€ä¸ªæ˜¯æ‰¾åˆ°æ–°çš„ä¸€ç»„çº¿æ€§æ— å…³çš„ç»´åº¦ï¼ˆä»æ—§çš„ç»´åº¦ä½œå˜æ¢å¾—æ¥ï¼‰          è¦æ‰¾åˆ°ä¸ªæ­£äº¤å•ä½çŸ©é˜µ$P$ï¼Œå¯¹åŸæ¥çš„ç©ºé—´ä½œå˜åŒ–ï¼Œä¹Ÿå°±æ˜¯$P$ä¹˜åœ¨åŸæ¥ç©ºé—´çš„å·¦è¾¹      ä¸æ”¹å˜åŸç©ºé—´çš„å¤§å°ï¼ˆæ¨¡é•¿å’Œå†…ç§¯ï¼‰ï¼Œåªæ—‹è½¬ï¼Œæ‰€ä»¥è¦æ‰¾æ­£äº¤å•ä½çŸ©é˜µ        ä¸€ä¸ªæ˜¯è¿™ä¸ªæ–°çš„ç»´åº¦è¦èƒ½æè¿°æœ€å¤§çš„æ•°æ®ä¹‹é—´çš„æ–¹å·®ï¼Œä¿æŒç‚¹ä¹‹é—´çš„è·ç¦»å¤Ÿå¤§ï¼Œå¥½åŒºåˆ†æ•°æ®ä¸­å¿ƒåŒ–ä¹‹åï¼Œ$\\frac{1}{n} XX^\\intercal$å°±æ˜¯åæ–¹å·®çŸ©é˜µï¼Œå› ä¸ºå‡å€¼æ˜¯0\\[\\text{Cov}(X, Y) = \\frac{\\sum_{i=1}^{n} (x_i \\cdot y_i)}{n-1} = \\frac{\\sum_{i=1}^{n} (x_i \\cdot y_i)}{n}\\]æ±‚åæ–¹å·®çŸ©é˜µçš„ç›®çš„æ˜¯ï¼Œå¯¹è§’çº¿ä¸Šæ˜¯è‡ªå·±çš„æ–¹å·®ï¼Œå…¶ä»–æ˜¯ä¸¤ä¸ªçš„ç›¸å…³æ€§ï¼Œé‚£ä¹ˆè¦å˜æ¢çš„ç»´åº¦ä¸Šå°½å¯èƒ½æ— å…³é‚£æ˜¯æœ€å¥½çš„æ‰€ä»¥è¦è®©åæ–¹å·®å¯¹è§’åŒ–ï¼Œ$PX(PX)^\\intercal$=æ–°çš„å¯¹è§’çŸ©é˜µï¼Œå¯ä»¥çœ‹æˆæ˜¯På¯¹Xå˜æ¢åçš„æ–°å‘é‡çš„åæ–¹å·®çŸ©é˜µï¼Œé‚£ä¹ˆPå°±æ˜¯åˆšå¥½æˆ‘è¦çš„è¿™ä¸ªæ­£äº¤å•ä½çŸ©é˜µäº†ï¼Œå› ä¸ºå®ƒè®©Xå˜æ¢åçš„åæ–¹å·®çŸ©é˜µæ˜¯å¯¹è§’çš„åŒæ—¶å¯¹è§’åŒ–å®Œååˆæ˜¯ä»å¤§åˆ°å°æ’çš„ï¼Œå¤§çš„è¡¨ç¤ºç‰¹å¾å‘é‡è´¡çŒ®å¤§çš„ï¼Œæ›´å¤šå¾—æè¿°äº†è¿™ä¸ªçŸ©é˜µï¼Œé‚£ä¹ˆç•™$P$å‰é¢$k$è¡Œå°±å¯ä»¥é™åˆ°$k$ç»´äº†  å‚è€ƒï¼šä¸»æˆåˆ†åˆ†æPCAç®—æ³•ï¼šä¸ºä»€ä¹ˆå»å‡å€¼ä»¥åçš„é«˜ç»´çŸ©é˜µä¹˜ä»¥å…¶åæ–¹å·®çŸ©é˜µçš„ç‰¹å¾å‘é‡çŸ©é˜µå°±æ˜¯â€œæŠ•å½±â€ï¼Ÿ - éƒç†Šå‡çš„å›ç­” - çŸ¥ä¹ "
  },
  
  {
    "title": "Information Design",
    "url": "/posts/Information-Design/",
    "categories": "Economics & Game Theory",
    "tags": "tech, game theory, information design, Bayesian persuasion, Bayes plausible, concavification, obedience, Bayes correlated equilibrium, Bayes nash equilibrium, multi agents, incentive compatibility",
    "date": "2023-06-01 12:00:01 +0000",
    





    
    "snippet": "What is Information Design?Communication does not just happen in fully cooperative scenarios. In some cases, the sender can persuade the receiver by â€œstrategically deceivingâ€, to increase its own e...",
    "content": "What is Information Design?Communication does not just happen in fully cooperative scenarios. In some cases, the sender can persuade the receiver by â€œstrategically deceivingâ€, to increase its own expected payoff. Actually, â€œone quarter of gdp is persuasion.â€ (McCloskey &amp; Klamer 1995).Nontrivially, â€œsuccessful partially deceivingâ€ is a better equilibrium than â€œsaying nothingâ€ and â€œrevealing all informationâ€.Information design is the study of this persuasion. And Bayesian persuasion is a special case of information design, which consists of a sender and a receiver.The problem of information design can be equivalently approached from various perspectives:  $\\max_{\\varphi} \\mathbb{E}_{s \\sim \\mu_0}\\left[\\mathbb{E}_{\\sigma \\sim \\varphi(\\cdot\\mid s)}\\left[r^i(s,a^*(\\sigma))\\right]\\right]$.  $\\max_{\\varphi} \\mathbb{E}_{\\mu\\sim\\tau}\\Big[\\mathbb{E}_{s\\sim\\mu}\\big[r^i(s,a^*(\\mu))\\big]\\Big]$, s.t. $\\mathbb{E}_{\\mu\\sim\\tau}(\\mu) = \\mu_0$.  In a two-signal case: The sender manipulates the receiverâ€™s posterior beliefs (each corresponding to a sent signal) to find the highest intersection point of the line segment $(\\mu_1 - \\mu_2, \\hat{v}_1 - \\hat{v}_2)$ and $x = \\mu_0$.  Denote the concavification of $\\hat{v}(\\mu) = \\mathbb{E}_{s\\sim\\mu}\\big[r^i(s,a^{*}(\\mu))\\big]$ as $f(\\mu)$. The senderâ€™s optimal expected payoff is $f(\\mu_0)$.  The sender select an optimal Bayes correlated equilibrium given an objective function.  $\\max_{\\varphi} \\mathbb{E}_{s \\sim \\mu_0}\\left[\\mathbb{E}_{\\sigma \\sim \\varphi(\\cdot\\mid s)}\\left[r^i(s,a^*(\\sigma))\\right]\\right]$, s.t. Obedience.PapersThe following part of this note is to summarize the essence of these papers:  Bayesian Persuasion (Kamenica &amp; Gentzkow 2011)  An equilibrium perspective:          Correlated Equilibrium in Games with Incomplete Information (Bergemann &amp; Morris 2011a)      Robust Predictions in Games with Incomplete Information (Bergemann &amp; Morris 2011b)      Bayes Correlated Equilibrium and the Comparison of Information Structures in Games (Bergemann &amp; Morris 2016)        Surveys:          Bayesian Persuasion and Information Design (Kamenica 2019)      Algorithmic Information Structure Design: A Survey (Dughmi 2019)      Information Design: A Unifified Perspective (Bergemann &amp; Morris 2019)      TimingConsider a persuasion between a sender and a receiver (named as Bayesian persuasion). The timing of it is:  The sender chooses a signaling scheme $\\varphi$. (Given a state $s$, $\\varphi(\\cdot \\mid s)$ is a distribution of $\\sigma$.)  The receiver observes this $\\varphi$.  The environment generates a state $s$.  A signal $\\sigma$ is sampled accroding to the commited $\\varphi(\\cdot\\mid s)$.  The receiver calculates its posterior belief $\\mu(\\cdot \\mid \\sigma)$, and then chooses an action $a$ that maximizes $\\mathbb{E}_{\\mu}(r^j)$. (Thus the term Bayesian persuasion.)  The sender and the receiver get rewards $r^i(s,a)$ and $r^j(s,a)$, respectively.  The core of information design is to find the optimal signaling scheme $\\varphi$ for the sender and to study the nature of this problem. But the senderâ€™s payoff is determined by the receiverâ€™s action, so it has to use its information advantage to influence the receiverâ€™s behavior (by designing $\\varphi$), thereby indirectly optimizing its own expected payoff.Important Examples  Recommendation Letter (Example 2.1 in Dughmi 2019).  Courtroom (The first example in Section 2.2 of Kamenica 2019).  Routing Game (Example 3.2 in Dughmi 2019) or Braessâ€™s paradox.Assumptions  The prior $\\mu_0$ of states is common knowledge.  The receiver is self-interested.          The receiver has its objective function to optimize (In my understanding it does not need to be consistent with its epected environmental reward), and the sender wants to influence the receiverâ€™s behavior.      The senderâ€™s objective function does not need to be consistent with its environmental reward, e.g., it can be social welfare $r^i+r^j$. (In this way, the sender is altruistic.)      Anyway, there is no limit to the goals of the sender and the receiver. They may be fully cooperative, mixed motived or adversarial.        The receiverâ€™s strategy is based on Bayesâ€™ rule (i.e. Bayesian rational). (Timing 5)  Commitment: The sender will honestly commit a signaling scheme to the receiver before the interaction with the receiver. (Timing 1-2)          It is this that makes Bayesian persuasion (or information design) different from other communication models, e.g. cheap talk, verifiable message, signaling games. (Kamenica 2019)      Justifications:                  Reputation          The signaling schemes are common knowledge, e.g., the sender (prosecutor) can choose investigative means (forensic tests, calling witness, etc.) to partially reveal the truth, which are public to everyone. (Courtroom)          (Justifications vary across applications. See Section 2.2 in Kamenica 2019)                      An analysis analogues to the revelation principle: The optimal scheme needs no more signals than the number of states of nature.          Proof: probability simplex + Caratheodoryâ€™s theorem. (Dughmi 2019)      Simplification and Geometric InterpretationsReformulation  The receiverâ€™s strategy:          Given a sent $\\sigma$, its posterior belief is $\\mu(\\cdot\\mid \\sigma)$.      Then it chooses $a^{*}(\\sigma) = a^{*}(\\mu(\\cdot\\mid\\sigma)) = \\arg\\max_{a}\\mathbb{E}_{s\\sim\\mu(\\cdot\\mid\\sigma)}\\left[r^j(s,a)\\right]$.      The mapping from signals to posterior beliefs is many-to-one: All the $\\sigma$ that induce the same posterior belief $\\mu(\\cdot\\mid \\sigma)$, the receiver will behave the same way (thus the payoffs of both agents).        The distribution of posterior beliefs $\\tau$          Every sent signal induces a specific posterior belief: Given a committed signaling scheme $\\varphi$ and a sent signal $\\sigma$, the receiverâ€™s posterior belief is $\\mu(\\cdot\\mid\\sigma)$. Calculated as $\\mu(s_i \\mid\\sigma) = \\frac{\\mu_0(s_i)\\cdot \\varphi(\\sigma\\mid s_i)}{\\sum\\limits_{s_j}\\mu_0(s_j)\\cdot \\varphi(\\sigma\\mid s_j)}$.      Again: The mapping from signals to posterior beliefs is many-to-one.      A distribution of signals corresponds to a distribution of posterior beliefs: Before the signal $\\sigma$ realized, the receiver can only estimate its distribution by the committed signaling scheme: $\\sigma\\sim p^1_{\\mu_0, \\varphi}(\\cdot)  = \\sum\\limits_{s}\\mu_0(s)\\cdot \\varphi(\\cdot\\mid s)$. Then $\\mu\\sim p^2_{\\mu_0, \\varphi}(\\cdot) = \\sum\\limits_{\\sigma: \\mu(\\cdot\\mid \\sigma) = \\mu}\\sum\\limits_{s} \\mu_0(s)\\cdot\\varphi(\\sigma\\mid s)$.      A $\\tau$ induced by a $\\varphi$ is denoted as $\\tau_{\\mu_0,\\varphi}$.        The senderâ€™s optimization problem          Original optimization: $\\max_{\\varphi} \\mathbb{E}_{s\\sim\\mu_0}\\Big[\\mathbb{E}_{\\sigma\\sim\\varphi(\\cdot\\mid s)}\\big[r^i(s,a^*(\\sigma))\\big]\\Big]$.      Reformulation:                  An equivalent objective function: $\\mathbb{E}_{\\mu\\sim\\tau}\\Big[\\mathbb{E}_{s\\sim\\mu}\\big[r^i(s,a^*(\\mu))\\big]\\Big]$.          The only contraint: $\\mathbb{E}_{\\mu\\sim\\tau}(\\mu) = \\mu_0$. (i.e. $\\tau = \\tau_{\\mu_0,\\varphi}$. This is named as Bayes plausible. See the next subsection.)                    Bayes plausible  The senderâ€™s optimization problem can be approached from the perspective of any arbitrary distribution of induced posterior beliefs that satisfies the Bayes plausible. Understanding this is important for grasping the concepts of geometric interpretation (concavification) in the next subsection.If an arbitrary $\\tau$ satisfies $\\mathbb{E}_{\\mu\\sim\\tau}(\\mu) = \\mu_0$, then this $\\tau$ is Bayes plausible. (Kamenica 2019)  What is necessary and sufficient:          Every $\\tau_{\\mu_0,\\varphi}$ (a $\\tau$ induced by the $\\varphi$) is Bayes plausible. (It can be proved by the law of iterated expectations. Kamenica 2019)      If a $\\tau$ is Bayes plausible, then it can be induced by a $\\varphi$. (Kamenica 2019. Proved in Kamenica &amp; Gentzkow 2011.)        A geometric interpretation: probability simplex (Figure 2 of Dughmi 2019).          A point in the probability simplex represents a distribution of singals (a convex combination of signals).      The blue point is the prior belief. The red points are posterior beliefs.      Bayes plausible = The blue point is inside the simplex of the red points.      Concavification  Does the sender benefit from persuasion? What is an optimal mechanism?This technique is best described with a two-signal example (Figure 1 of Kamenica &amp; Gentzkow 2011, Kamenica 2019):  Assume that there are $n$ states. Then a posterior belief $\\mu$ is a point in $\\mathbb{R}^{n-1}$. (2011)  The values of $x$-axis are in $\\mathbb{R}^{n-1}$. Each represents a posterior belief $\\mu$. (2011)  A $\\mu$ induces an expected payoff of the sender $\\hat{v}(\\mu) = \\mathbb{E}_{s\\sim\\mu}\\big[r^i(s,a^{*}(\\mu))\\big]$. The black line in the figure denotes this function. (2011,2019)  $\\sigma_1$ and $\\sigma_2$ induce posterior beliefs $\\mu_1(\\cdot\\mid\\sigma_1)$ and $\\mu_2(\\cdot\\mid\\sigma_2)$ respectively. These two $\\mu$ correspond to two values on the $x$-axis, and indicate two expected payoff of the sender $\\hat{v}_1$ and $\\hat{v}_2$. (2019)  $\\mu_1$ and $\\mu_2$ are almost arbitrary: The distribution of $\\mu_1$ and $\\mu_2$ should be Bayes plausible, i.e., their expectation should be $\\mu_0$.          Assume that the distribution of $\\mu$ is $\\tau = (k, 1-k)$. Then $k\\cdot\\mu_1+(1-k)\\cdot\\mu_2 = \\mu_0$. Thus if $\\mu_{i}$ is on the left side of $\\mu_0$ on the $x$-axis, then $\\mu_{1-i}$ is on the right side. Unless they both coincide with $\\mu_0$.      Also, $\\mathbb{E}_{\\mu\\sim\\tau}(\\hat{v}(\\mu)) = k\\cdot \\hat{v}_1+(1-k)\\cdot\\hat{v}_2$, which is the senderâ€™s expected payoff (obejective). It means that: When the receiverâ€™s posterior beliefs are $\\mu_1,\\mu_2$ (after receiving $\\sigma_1, \\sigma_2$ respectively), the sender will gets $\\mathbb{E}_{\\mu\\sim\\tau}(\\hat{v}(\\mu))$.      Since the coordinates are proportional, connecting $(\\mu_1, \\hat{v}_1)$ and $(\\mu_2, \\hat{v}_2)$ to get a line segment $l$, $(\\mu_0, \\mathbb{E}_{\\mu\\sim\\tau}(\\hat{v}(\\mu)))$ is on $l$. In this way, the sender needs to manipulate the receiverâ€™s posterior beliefs to find the highest intersection point.        A concavification of $\\hat{v}(\\mu)$ is the smallest concave function everywhere greater than $\\hat{v}(\\mu)$. (The red line in Figure 1 of 2011, 2019)          The concavification of $\\hat{v}(\\mu)$ evaluated at $\\mu_0$ equals $\\max\\set{y\\mid(\\mu_0, y)\\in co(\\hat{v})}$, where $co(\\hat{v})$ denotes the convex hull of the graph of $\\hat{v}$, i.e., the light blue region in Figure 1 of 2011.        In this case, a signaling scheme with more than two signals cannot improve the senderâ€™s expected payoff. (Kamenica 2019)Corollaries and propositions(Kamenica &amp; Gentzkow 2011):  Corollary 1: The sender benefits from persuasion if and only if there exists a Bayes plausible distribution $\\tau$ of posterior belief $\\mu$ such that $\\mathbb{E}_{\\mu\\sim\\tau}\\Big[\\mathbb{E}_{s\\sim\\mu}\\big[r^i(s,a^{*}(\\mu))\\big]\\Big] &gt; \\mathbb{E}_{s\\sim\\mu_0}\\big[r^i(s,a^{*}(\\mu_0))\\big]$. The senderâ€™s optimal expected payoff is $\\max_{\\varphi} \\mathbb{E}_{\\mu\\sim\\tau}\\Big[\\mathbb{E}_{s\\sim\\mu}\\big[r^i(s,a^*(\\mu))\\big]\\Big]$, s.t. $\\mathbb{E}_{\\mu\\sim\\tau}(\\mu) = \\mu_0$.  Corollary 2: The sender benefits from persuasion if and only if $f(\\mu_0)&gt;\\hat{v}(\\mu_0)$, where $\\hat{v}(\\mu) = \\mathbb{E}_{s\\sim\\mu}\\big[r^i(s,a^{*}(\\mu))\\big]$, and $f(\\mu)$ is the concavification of $\\hat{v}(\\mu)$. The senderâ€™s optimal expected payoff is $f(\\mu_0)$.  Proposition 2: If $\\hat{v}(\\mu)$ is concave, the sender does not benefit from persuasion for any prior $\\mu_0$. If $\\hat{v}(\\mu)$ is convex and not concave, the sender benefits from persuasion for every prior $\\mu_0$.  Proposition 3: If there is no information the sender would share, the sender does not benefit from persuasion.          â€œThere is information the sender would shareâ€ is defined as $\\exists \\mu$, s.t. $\\hat{v}(\\mu)&gt; \\sum\\limits_{s} \\mu(s)\\cdot r^j(s, a^{*}(\\mu_0))$.      In other words, there must exist a $\\mu$ such that, if the sender had private information that led it induces $\\mu$, it would prefer to share this information with the receiver rather than have the receiver act based on $\\mu_0$.      Otherwise, the sender does not have the motivation to share its information.        Proposition 4: If there is information the sender would share, and the receiverâ€™s preference is discrete at the prior, the sender benefits from persuasion. (sufficient)          â€œThe receiverâ€™s preference is discrete at belief $\\mu$â€ is defined as: If there is an $\\epsilon&gt;0$ s.t. $\\forall a\\ne a^{*}(\\mu)$, $\\sum\\limits_{s}\\mu(s)\\cdot r^j(s, a^{*}(\\mu)) &gt; \\sum\\limits_{s} \\mu(s)\\cdot r^j(s, a)+\\epsilon$.      In other words, the receiverâ€™s expected payoff from its preferred action $a^{*}(\\mu)$ is bounded away from its expected payoff from any other action.      Otherwise, the sender does not have the ability to influence the receiver.        Proposition 5: If $A$ is finite, the receiverâ€™s preference is discrete at the prior generically.  Proposition 7: An optimal mechanism exists.  Proposition 8: For any prior,          if $\\hat{v}(\\mu)$ is (strictly) concave, no disclosure is (uniquely) optimal;                  no disclosure: $\\mu = \\mu_0$.                    if $\\hat{v}(\\mu)$ is (strictly) convex, full disclosure is (uniquely) optimal;                  full disclosure: $\\mu(\\cdot\\mid\\sigma)$ is degenerate for all $\\sigma$ sent in equilibirum. $\\mu$ is degenerate if there is $s$ s.t. $\\mu(s) = 1$.                    if $\\hat{v}(\\mu)$ is convex and not concave, strong disclosure is (uniquely) optimal.                  strong disclosure: $\\mu$ is at the boundary of $\\Delta(s)$ for all $\\sigma$ sent in equilibirum. (?)                    (Dughmi 2019):  If reward functions are identical (i.e. $r^i = r^j$), then the senderâ€™s objective function is convex. The optimal signaling scheme is to reveal all the information.  If $(r^i+r^j)(s,a) = k, \\forall s,a$, where $k \\in \\mathbb{R}$, then the senderâ€™s objective function is concave. The optimal signaling scheme is to reveal nothing (In this case, $\\mu = \\mu_0$).An Equilibrium Perspective(Bergemann &amp; Morris 2016)Settings in my understanding  This paper studies the persuasion of a group of receivers with private channels.  A game of incomplete information can be decomposed into a basic game and an observation structure (or information structure).          The basic game defines the set of actions, the set of payoff states, the payoff functions, and the common prior over the payoff states.      The observation structure refers to the function of partial observations that the players receive in the game.        An incomplete information game $\\mathcal{G} = (G, M)$.          A basic game $G = ((A^j, r^j)_{j=1}^{J}, \\mu_0)$.                  $A = \\prod_{j} A^j$.          $r^j: S\\times A \\to \\mathbb{R}$.          $\\mu_0 \\in \\Delta_{++}(S)$.                    An observation structure $M = ((O^j)_{j=1}^{J}, q)$.                  Partial observation (type) $O = \\prod_{j} O^j$.          Emission function $q: S \\to \\Delta(O)$.          A possible (and natural) observation structure is the null observation structure in which each playerâ€™s set of ovservations (tyes) $O^j$ is a singleton, $O^j = \\set{t^j}$.                      A signaling scheme (decision rule) $\\varphi: S\\times O \\to \\Delta(A)$.          There is an omniscient mediator, who first observes the sampled $s\\sim\\mu_0$ and the sampled $o\\sim q(\\cdot\\mid s)$, and then picks actons and privately announces $a^j$ to each receiver $j$.        A policy (behavioral strategy) for receiver $j$ in $(G, M)$ is $\\pi^j: O^j \\to \\Delta(A^j)$.Obedience &amp; BCEA decision rule $\\varphi$ is obedient for $(G, M)$ if, for each $j = 1, \\ldots, J$, $o^j\\in O^j$, and $a^j\\in A^j$, we have\\[\\begin{aligned}  &amp; \\sum\\limits_{s, o^{-j}, a^{-j}} \\mu_0(s)   \\cdot q\\Big((o^j, o^{-j})\\mid s \\Big)   \\cdot \\varphi\\Big( (a^j, a^{-j})\\mid s, (o^j, o^{-j}) \\Big)  \\cdot r^j\\Big(s, (a^j, a^{-j})\\Big) \\ge \\\\  &amp; \\sum\\limits_{s, o^{-j}, a^{-j}} \\mu_0(s)   \\cdot q\\Big((o^j, o^{-j})\\mid s \\Big)   \\cdot \\varphi\\Big( (a^j, a^{-j})\\mid s, (o^j, o^{-j}) \\Big)  \\cdot r^j\\Big(s, (a^{j\\prime}, a^{-j})\\Big)\\end{aligned}\\]for all $a^{j\\prime}\\in A^j$.  The definitions of obedience in (Bergemann &amp; Morris 2011a, 2011b) is a bit different from this.A decision rule $\\varphi$ is a Bayes correlated equilibrium (BCE) of $(G,M)$ if it is obedient for $(G,M)$.Degenerated BCE  There is complete information, i.e., if $S$ is a singleton: this definition reduces to the definition of correlated equilibrium (CE) for a complete information game. (Aumann 1987)      If $M$ is the null observation function (a singleton), then this is essentially the â€œuniversal Bayesian solutionâ€ of (Forges 1993).    A decision rule $\\varphi$ is obedient for $(G)$ if, for each $j = 1, \\ldots, J$ and $a^j\\in A^j$, we have\\[\\begin{aligned}   &amp; \\sum\\limits_{s, a^{-j}} \\mu_0(s)    \\cdot \\varphi\\Big( (a^j, a^{-j})\\mid s \\Big)   \\cdot r^j\\Big(s, (a^j, a^{-j})\\Big) \\ge \\\\   &amp; \\sum\\limits_{s, a^{-j}} \\mu_0(s)    \\cdot \\varphi\\Big( (a^j, a^{-j})\\mid s \\Big)   \\cdot r^j\\Big(s, (a^{j\\prime}, a^{-j})\\Big) \\end{aligned}\\]    for all $a^{j\\prime}\\in A^j$.        If $M$ is the null observation function, and if there is only one receiver, then this definition reduces to behavior in the concavification problem of (Aumann et al. 1995) and the Bayesian persuasion of (Kamenica &amp; Gentzkow 2011).    A decision rule $\\varphi$ is obedient for $(G)$ if, for each $a\\in A$, we have\\[\\sum\\limits_{s} \\mu_0(s)  \\cdot \\varphi( a\\mid s ) \\cdot \\Big( r^j(s, a) - r^j(s, a') \\Big) \\ge 0\\]    for all $a^{\\prime}\\in A$.    Why is it called obedience? Why do the receivers follow the recommendation?(Bergemann &amp; Morris 2019)\\[\\begin{aligned}  &amp; \\Leftrightarrow  \\sum\\limits_{s} \\frac{\\mu_0(s) \\cdot \\varphi( a\\mid s )}  { \\sum\\limits_{s'}\\mu_0(s') \\cdot \\varphi( a\\mid s')}  \\cdot \\Big( r^j(s, a) - r^j(s, a') \\Big) \\ge 0 , \\forall a'\\in A.\\\\  &amp; \\Leftrightarrow  \\sum\\limits_{s} \\mu(s\\mid a)  \\cdot \\Big( r^j(s, a) - r^j(s, a') \\Big) \\ge 0 , \\forall a'\\in A.\\end{aligned}\\]BNEA strategy profile (joint policy) $\\pi$ is a Bayes Nash equilibrium (BNE) of $(G, M)$ if for each $j = 1 , \\ldots, J$, $\\sigma^j\\in\\Sigma^j$, and $a^j\\in A^j$ with $\\pi^j(a^j \\mid o^j) &gt; 0$, we have\\[\\begin{aligned}  &amp; \\sum\\limits_{s, o^{-j}, a^{-j}} \\mu_0(s)   \\cdot q\\Big((o^j, o^{-j})\\mid s \\Big)   \\cdot \\Bigg( \\prod_{j \\ne k} \\pi^k(a^k \\mid o^k) \\Bigg)  \\cdot r^j\\Big(s, (a^j, a^{-j})\\Big) \\ge \\\\  &amp; \\sum\\limits_{s, o^{-j}, a^{-j}} \\mu_0(s)   \\cdot q\\Big((o^j, o^{-j})\\mid s \\Big)   \\cdot \\Bigg( \\prod_{j \\ne k} \\pi^k(a^k \\mid o^k) \\Bigg)  \\cdot r^j\\Big(s, (a^{j\\prime}, a^{-j})\\Big)\\end{aligned}\\]for all $a^{j\\prime}\\in A^j$.  The following part has not been finished yet. One may check my writing schedule.Extensions  Multiple senders  Multiple receivers  Dynamic environment  Others:          The receiver has private information      An Ethical JustificationI do not think information design is immoral. Information is a kind of property of the sender, and it is legal for it to profit from its information. (Unless the utilized information is a public resource.)Furthermore, in those cases where the sender can improve its own expected payoff through information design, the receiverâ€™s payoff is not worse than that of the sender not reveal information at all.Nevertheless, practice use of information design should take the senderâ€™s objective function into some serious consideration.  Disclaimer: The above content is summarized from the mentioned papers. Corresponding links or references have been provided."
  },
  
  {
    "title": "MARL Tasks",
    "url": "/posts/MARL-Tasks/",
    "categories": "Artificial Intelligence, Multi-Agent Reinforcement Learning",
    "tags": "tech, multi agents, environment",
    "date": "2023-05-31 12:00:01 +0000",
    





    
    "snippet": "  This note will be consistently updated.List  StarCraft II          SMAC (StarCraft Multi-Agent Challenge). SMAC is WhiRLâ€™s environment for research in the field of collaborative multi-agent reinf...",
    "content": "  This note will be consistently updated.List  StarCraft II          SMAC (StarCraft Multi-Agent Challenge). SMAC is WhiRLâ€™s environment for research in the field of collaborative multi-agent reinforcement learning (MARL) based on Blizzardâ€™s StarCraft II RTS game.      PySC2 (StarCraft II Learning Environment). PySC2 is DeepMindâ€™s Python component of the StarCraft II Learning Environment (SC2LE).      gym-starcraft. An OpenAI Gym interface to StarCraft.        MAagent          persuit: predators pursuit preys      gather: agents rush to gather food      battle: battle between two armies      arrange: arrange agents into some characters      A demo video        Overcooked  MiniRTS          Paper      GitHub        Cards &amp; Chess          Go      Texas Holdâ€™em      Mahjong      Hanabi [code] [paper]        Communication  Melting Pot (Deep Mind)          Sequential Social Dilemma (SSD)      Harvest      Clean Up        Level-Based ForagingOvercookedThis environment is designed to test the human-ai coordination, or used as a zero-shot coordination task.Resources  [GitHub Repo: overcooked_ai]  [Online Game]  [An Environment Wrapper: PantheonRL]HanabiObjectivePlayers work collaboratively to put on a firework show by placing a series of cards in the correct order. The objective is to complete a series of five cards of the same color in ascending numerical order (1 to 5) for each color.  Cards Facing Outwards: One distinctive rule of Hanabi is that players hold their cards facing outwards, meaning they can see everyone elseâ€™s cards but their own. This facilitates a cooperative environment where players rely on each otherâ€™s hints to figure out their cards.  Communication: Communication is restricted to the formal hint-giving process to maintain the gameâ€™s difficulty and collaborative spirit.Components  Deck: Consists of 50 cards, distributed into 5 different colors (red, yellow, green, blue, white), with each color having numbers from 1 to 5 (1s x3, 2s x2, 3s x2, 4s x2, and 5s x1).  Hint tokens: 8 in total, used to give hints to other players.  Fuse tokens: 3 in total, representing the playersâ€™ â€œlivesâ€.Setup  Number of players: 2-5 players.  Hand size: Depending on the number of players, each player starts with a hand of cards (5 cards for 2-3 players, 4 cards for 4-5 players).  Initial layout: Players hold their cards facing outwards so they canâ€™t see their cards but can see othersâ€™.GameplayPlayers take turns clockwise and can choose to perform one of the following actions on their turn:      Give a hint: A player may give a hint to another player about the contents of their hand. The hint must be about either the color or the number of the cards, not both. It consumes a hint token. It must relate to at least one card in the other playerâ€™s hand, and it must give information about all the cards of the chosen characteristic.        Discard a card: A player may choose to discard a card from their hand to regain a hint token. The discarded card is placed in the discard pile and the player draws a new card from the draw pile.        Play a card: A player may choose to play a card from their hand onto the table. If the card successfully fits into a fireworks display (i.e., it is the next number in sequence of the same color), it stays on the table. Otherwise, it goes to the discard pile and a fuse token is removed.  Game Ending ConditionsThe game can end in the following ways:  Successful Completion: Players successfully play all five cards in each color.  Fuse Tokens Exhausted: Players make three mistakes and lose all fuse tokens.  Deck Exhaustion: The draw pile is exhausted. Players get one final round before the game ends.ScoringAt the end of the game, the score is calculated based on the number of cards successfully played in the fireworks displays. The highest possible score is 25 (if all fireworks are successfully completed). The score is determined by summing the highest value of cards played for each color.Strategy and Tips  Memory and Deduction: Players must rely heavily on their memory and deduction skills to remember the hints given and to figure out which cards they hold.  Non-verbal cues: Players are not allowed to give extra hints through non-verbal cues or suggestive comments. All hints must be given using hint tokens officially in oneâ€™s turn.  Hint Efficiency: Given the limited number of hint tokens, players should strive to give hints that convey the maximum amount of information."
  },
  
  {
    "title": "Personality",
    "url": "/posts/Personality/",
    "categories": "Efficiency, Psychology",
    "tags": "life, efficiency, psychology, personality",
    "date": "2023-05-19 18:40:00 +0000",
    





    
    "snippet": "ForewordOne may choose to disregard their own labels (and bear the consequences, good or bad, of doing so), believing that the labeling system is flawed. However, they cannot claim to be without la...",
    "content": "ForewordOne may choose to disregard their own labels (and bear the consequences, good or bad, of doing so), believing that the labeling system is flawed. However, they cannot claim to be without labels or cannot be defined.This post aims to briefly introduce three personality models: The Big Five, Myers-Briggs Type Indicator (MBTI), and Enneagram. The Big Five is generally well-regarded and supported by research in the psychology community, whereas the MBTI and Enneagram are viewed more critically, with concerns about their scientific validity and reliability.But I believe that these tools are all efficient communication protocols, and using them allows me to easily introduce my profile.Because if these models are common knowledge between me and my readers, then my test results will encapsulate a wealth of information about my behavioral habits and thought preferences.After understanding these models, analyzing the characters in film and literary works will also yield many interesting new perspectives.Big Five Personality TestThe Big Five personality traits system is a widely accepted framework in psychology, consisting of five broad dimensions that describe human personality:  Openness (inventiveness and curiosity vs. consistency and caution),  Conscientiousness (efficiency and organization vs. easy-going and careless),  Extraversion (outgoing and energetic vs. solitary and reserved),  Agreeableness (friendly and compassionate vs. challenging and detached), and  Neuroticism (sensitive and nervous vs. secure and confident).This model is empirically driven and is considered robust for understanding personality due to its extensive research base, cross-cultural validity, and ability to predict various life outcomes. Itâ€™s praised for capturing the complexity of human personality traits in a comprehensive yet flexible manner.My test result[My test result in 2023]: SCOAI.            Â       Abbr.      High      Low                  Extraversion      E      S      R              Neuroticism      N      L      C              Conscientiousness      C      O      U              Agreeableness      A      A      E              Openness To Experience      O      I      N                  Openness To Experience (101)      Conscientiousness (96)      Agreeableness (76)      Neuroticism (67)      Extraversion (76)                  Imagination (19)      Self-Efficacy (16)      Trust (9)      Anxiety (14)      Friendliness (11)              Artistic Interests (15)      Orderliness (18)      Morality (15)      Anger (9)      Gregariousness (9)              Emotionality (16)      Dutifulness (13)      Altruism (14)      Depression (9)      Assertiveness (15)              Adventurousness (17)      Achievement-Striving (17)      Cooperation (15)      Self-Consciousness (12)      Activity Level (16)              Intellect (20)      Self-Discipline (16)      Modesty (10)      Immoderation (12)      Excitement-Seeking (14)              Liberalism (14)      Cautiousness (16)      Sympathy (13)      Vulnerability (11)      Cheerfulness (11)      In the first row, each item has a full score of 120, while in the other rows, each item has a full score of 20.  The detailed description of each dimension can be found in [My Test Result in 2023].Myers-Briggs Type Indicator (MBTI)The Myers-Briggs Type Indicator (MBTI) is another popular personality framework, particularly in non-academic settings like business and career counseling. It categorizes people into 16 distinct personality types based on four dichotomies:  Introversion vs. Extraversion. This dimension describes how individuals draw and expend their energy. Extraverts (E) are energized by interaction with others and the external world. They tend to be sociable, outgoing, and more comfortable in groups or social situations. Introverts (I), in contrast, are energized by spending time alone or in quiet environments. They are often reflective, reserved, and more comfortable working independently or in smaller groups. This preference doesnâ€™t necessarily determine social skills or shyness; itâ€™s more about where one gets their energy from.  Sensing vs. Intuition. This dimension indicates how individuals prefer to gather information. People who prefer Sensing (S) focus on the present and concrete information gained from their senses. They are detail-oriented, practical, and prefer to deal with facts and real-world applications. Intuitive types (N), however, are more focused on possibilities and what might be. They enjoy thinking about the future, abstract concepts, and the big picture. They value imagination and innovation more than practical applications.  Thinking vs. Feeling. This dimension describes how people prefer to make decisions. Thinkers (T) make decisions based on logic and objective analysis. They value principles, consistency, and impersonal truth, often focusing on the task rather than people. Feelers (F), on the other hand, are more concerned with harmony and the well-being of others. Their decisions are influenced more by personal values and the impact on people. They are empathetic, compassionate, and tend to prioritize relationships in their decision-making process.  Judging vs. Perceiving. This dimension relates to how individuals prefer to organize their lives. Judging types (J) like structure, firm decisions, and clear rules. They prefer planning and organizing their lives and tend to be decisive, efficient, and task-focused. Perceiving types (P), conversely, prefer to remain open to new information and options. They are more spontaneous, flexible, and adaptable. Perceivers are less structured and more comfortable adapting to changes and exploring various possibilities.Furthermore, there is a fifth letter in some MBTI representations, such as â€œINTJ-T,â€ originates from the 16Personalities framework and represents an additional dimension known as â€œIdentity,â€ which is not part of the original MBTI model.  Assertive vs. Turbulent. This dimension is divided into â€œAssertiveâ€ (A) and â€œTurbulentâ€ (T) traits. Assertive individuals are typically confident, relaxed, and resistant to stress, maintaining a stable level of emotional resilience. In contrast, Turbulent individuals are more likely to be self-conscious, sensitive to stress, and driven by a desire for improvement, often experiencing wider emotional fluctuations and a strong sense of ambition. This Assertive/Turbulent aspect, blending elements from the Big Five personality traits, adds another layer to personality assessment, offering a nuanced view of oneâ€™s overall disposition and reaction to stress and change.Developed from Carl Jungâ€™s theory of psychological types, MBTI is often used for personal development, career planning, and team building. However, it has faced criticism from the academic psychology community due to concerns about its scientific validity, reliability, and lack of empirical support, as people may receive different type designations upon retaking the test.My test result[My test result in 2023]: INTJ-T (The Turbulent Architect).  Introverted, Intuitive, Thinking, Judging, and Turbulent;  Strategy: Constant Improvement;  Dominant (Jungian) Cognitive Functions: Introverted Intuition (Ni) and Extraverted Thinking (Te).Jungian cognitive functionsThe MBTI is heavily influenced by the theories of Swiss psychiatrist Carl Jung, particularly his concept of cognitive functions.Jung proposed a theory of psychological types in his 1921 work â€œPsychological Types.â€He introduced the concepts of introversion and extraversion as the main orientations of personality and added four psychological functions through which people experience the world: thinking, feeling, sensation, and intuition.These functions could operate in either an extraverted or introverted mode, leading to a total of eight distinct functions in his theory.  Cognitive functions, also referred to as psychological functions, as described by Carl Jung in his book Psychological Types, are particular mental processes within a personâ€™s psyche that are present regardless of common circumstance. This is a concept that serves as one of the foundations for his theory on personality type. In his book, he noted four main psychological functions: thinking, feeling, sensation, and intuition. He introduced them with having either an internally focused (introverted) or externally focused (extraverted) tendency which he called â€œattitudesâ€. He also categorizes the functions as either rational (thinking and feeling) or irrational (intuition and sensation).â€” Wikipedia.The MBTI was developed by Katharine Cook Briggs and her daughter Isabel Briggs Myers in the 1940s. They were influenced by Jungâ€™s theory and sought to apply it in a more practical, accessible way.While they retained Jungâ€™s basic concepts of extraversion/introversion and the four functions (thinking, feeling, sensing, and intuition), they added a new dimension: Judging vs. Perceiving. This was not explicitly part of Jungâ€™s original theory but was inferred from his descriptions of how people use their judging (thinking and feeling) and perceiving (sensing and intuition) functions.In MBTI, the combination of Extraversion vs. Introversion with the four functions results in eight possible cognitive functions (e.g., extraverted thinking, introverted feeling, etc.).These functions are used to determine the 16 MBTI types, with each type having a primary function that dominates their personality. The MBTI also considers secondary and tertiary functions in its personality descriptions, attempting to create a more holistic view of each personality type.In summary, the MBTI is an adaptation and extension of Carl Jungâ€™s theory of cognitive functions. It maintains the core of Jungâ€™s ideas but simplifies and expands them to create a more structured and easily applicable personality typology. However, itâ€™s important to note that the MBTIâ€™s interpretation and application of Jungâ€™s theories are not without criticism, particularly from academic psychologists who question the MBTIâ€™s scientific validity and reliability.Table 1 from Wikipedia.Table 2 from Wikipedia.Table 3 from Wikipedia.Introverted Intuition (Ni) is an information-gathering function, but unlike its counterpart, Extroverted Intuition, it processes information in a more internal and reflective manner. Individuals who primarily use Introverted Intuition tend to focus on the future, relying on insights and impressions rather than concrete facts. They often perceive patterns and connections that are not immediately obvious to others. Ni users are typically deep thinkers, valuing concepts, theories, and abstract ideas. They have a natural inclination towards understanding complex systems and are often driven by a vision or a sense of where things are heading. This introspective function can sometimes lead them to seem detached from the present, as they are more engaged with their internal world of ideas and possibilities.Extroverted Thinking (Te) is an extroverted function, meaning it is oriented towards the external world. Te is logical, systematic, and focused on organizing and structuring the environment. Individuals with a dominant Extroverted Thinking function are typically very good at setting goals, making plans, and implementing strategies. They prioritize efficiency, productivity, and are often drawn to positions of management or leadership due to their ability to make objective, logical decisions and to create order and consistency in their surroundings. Te users are practical and straightforward, preferring clear communication and measurable outcomes. They excel in taking complex ideas and organizing them in a way that is understandable and actionable, often thriving in environments where their skills in organization and leadership can be put to good use.Introverted Feeling (Fi) is featured in the Myers-Briggs Type Indicator (MBTI). Fi is a decision-making function that is focused inwardly, dealing with emotions and values. Individuals who primarily use Introverted Feeling process their feelings internally and place a high emphasis on personal, often subjective, values and morals. They tend to be guided by these internal values when making decisions, rather than external sources of morality or societal expectations. Fi users are often very aware of their own emotions and seek to understand and align their actions with their inner sense of integrity and authenticity. This can make them seem reserved or private, as they prefer to reflect deeply on their feelings rather than express them openly. They are often empathetic and compassionate, with a strong sense of individualism and a deep commitment to living in harmony with their personal ethical code.Extroverted Sensing (Se) is a perceiving function that focuses on the external world, emphasizing the present moment and concrete sensory experience. Individuals with a dominant Extroverted Sensing function are typically very aware of their physical environment and enjoy engaging with it. They are often pragmatic and realistic, preferring direct interaction with the world around them. Se users tend to be action-oriented, enjoying activities that involve sensory stimulation and physical engagement. They are often adept at responding to immediate situations and can be very spontaneous and adaptable. This focus on the here and now can sometimes lead to a lesser emphasis on planning for the future or reflecting on the past. Extroverted Sensing is associated with a love for aesthetics, a keen awareness of their surroundings, and a preference for hands-on experiences.EnneagramThe Enneagram is a personality system that describes nine primary personality types, each with its own set of characteristics, motivations, fears, and desires. Its origins are more spiritual and mystical, tracing back to ancient traditions. The Enneagram has become popular in various self-help and spiritual contexts and is sometimes used in counseling for personal and relational growth. Despite its popularity in these areas, the Enneagram is viewed with skepticism by the mainstream psychology community due to its lack of empirical evidence and scientific grounding. Critics point out that its categories are too vague and subjective, making it difficult to validate and study scientifically.This website provides a great introduction to the various types of the Enneagram.My test result[My test result in 2023]: Type 5 with a 4-Wing.  My test result at this website.  Fives (The Investigators) are alert, insightful, and curious. They are able to concentrate and focus on developing complex ideas and skills. Independent, innovative, and inventive, they can also become preoccupied with their thoughts and imaginary constructs. They become detached, yet high-strung and intense. They typically have problems with eccentricity, nihilism, and isolation. At their Best: visionary pioneers, often ahead of their time, and able to see the world in an entirely new way.      Basic Fear: Being useless, helpless, or incapable    Basic Desire: To be capable and competent    Enneagram Five with a Four-Wing: â€œThe Iconoclastâ€    Enneagram Five with a Six-Wing: â€œThe Problem Solverâ€    Key Motivations: Want to possess knowledge, to understand the environment, to have everything figured out as a way of defending the self from threats from the environment.  When moving in their Direction of Disintegration (stress), detached Fives suddenly become hyperactive and scattered at Seven. However, when moving in their Direction of Integration (growth), avaricious, detached Fives become more self-confident and decisive, like healthy Eights.  â€” From this webpage.TritypeThe tritype theory suggests that while individuals have a primary Enneagram type, they also exhibit significant characteristics of two other types.These three types â€“ one from each of the Enneagramâ€™s centers of intelligence (Heart/Feeling, Head/Thinking, Gut/Instinct) â€“ combine to form a personâ€™s tritype. For example, someone could have a tritype of 4-7-1, meaning their primary type is in the Heart center (type 4), but they also strongly identify with a type in the Head center (type 7) and the Gut center (type 1).My tritye is 548.  Fours (The Individualists) are self-aware, sensitive, and reserved. They are emotionally honest, creative, and personal, but can also be moody and self-conscious. Withholding themselves from others due to feeling vulnerable and defective, they can also feel disdainful and exempt from ordinary ways of living. They typically have problems with melancholy, self-indulgence, and self-pity. At their Best: inspired and highly creative, they are able to renew themselves and transform their experiences.  Basic Fear: That they have no identity or personal significanceBasic Desire: To find themselves and their significance (to create an identity)Enneagram Four with a Three-Wing: â€œThe Aristocratâ€Enneagram Four with a Five-Wing: â€œThe Bohemianâ€  Key Motivations: Want to express themselves and their individuality, to create and surround themselves with beauty, to maintain certain moods and feelings, to withdraw to protect their self-image, to take care of emotional needs before attending to anything else, to attract a â€œrescuer.â€  When moving in their Direction of Disintegration (stress), aloof Fours suddenly become over-involved and clinging at Two. However, when moving in their Direction of Integration (growth), envious, emotionally turbulent Fours become more objective and principled, like healthy Ones.  â€” From this webpage.  Eights (The Challengers) are self-confident, strong, and assertive. Protective, resourceful, straight-talking, and decisive, but can also be ego-centric and domineering. Eights feel they must control their environment, especially people, sometimes becoming confrontational and intimidating. Eights typically have problems with their tempers and with allowing themselves to be vulnerable. At their Best: self- mastering, they use their strength to improve othersâ€™ lives, becoming heroic, magnanimous, and inspiring.  Basic Fear: Of being harmed or controlled by othersBasic Desire: To protect themselves (to be in control of their own life and destiny)Enneagram Eight with a Seven-Wing: â€œThe Maverickâ€Enneagram Eight with a Nine-Wing: â€œThe Bearâ€  Key Motivations: Want to be self-reliant, to prove their strength and resist weakness, to be important in their world, to dominate the environment, and to stay in control of their situation.  When moving in their Direction of Disintegration (stress), self-confident Eights suddenly become secretive and fearful at Five. However, when moving in their Direction of Integration (growth), lustful, controlling Eights become more open-hearted and caring, like healthy Twos.  â€” From this webpage."
  },
  
  {
    "title": "For Prospective Undergraduates",
    "url": "/posts/A-Document-for-Prospective-Undergraduates/",
    "categories": "Misc Notes",
    "tags": "misc note",
    "date": "2023-05-18 12:00:00 +0000",
    





    
    "snippet": "  Welcome! ğŸ‰Preliminariesif you are wondering whether you are â€¦capable:  PyTorch  GitHub  Weights &amp; Biases  Advantage Actor Critic (A2C)  You have a relatively large amount of time.  A straight...",
    "content": "  Welcome! ğŸ‰Preliminariesif you are wondering whether you are â€¦capable:  PyTorch  GitHub  Weights &amp; Biases  Advantage Actor Critic (A2C)  You have a relatively large amount of time.  A straightforward test:Reproduce A2C to complete any RL task (development needs to take place within a GitHub repository), and utilize Weights &amp; Biases for experiment visualization and hyperparameter tuning.interested in our topic:Currently, we are focusing on information design (a subfield of computational economics and game theory) and multi-agent reinforcement learning (MARL). Our next few works will be based on:  Information Design in Multi-Agent Reinforcement Learning.    Yue Lin, Wenhao Li, Hongyuan Zha, Baoxiang Wang.    arXiv 2023.  For more specific details, please contact me so that we can discuss in person.Recommended ReadingsOur communication will be smoother if you have read some of the following papers.MARL in Sequential Social Dilemmas  MARL in Sequential Social Dilemmas  Melting PotInformation Design  Bayesian Persuasion  Bayes Correlated Equilibrium and the Comparison of Information Structures in Games  Surveys:          Bayesian Persuasion and Information Design      Algorithmic Information Structure Design: A Survey      Mechanism Design in MARL  LIO  The AI Economist: TaxationMARL Algorithms  MADDPG  COMA  QMIXMARL Communication  RIAL,DIAL  CommNet"
  },
  
  {
    "title": "RL Toolbox",
    "url": "/posts/RL-Toolbox/",
    "categories": "Artificial Intelligence, Reinforcement Learning",
    "tags": "tech, toolbox, reinforcement learning",
    "date": "2023-04-10 18:40:00 +0000",
    





    
    "snippet": "  This note will be consistently updated.PPO TricksThere are a total of 37 tricks, among which 13 are relatively core.      PPO paper    The 37 Implementation Details of Proximal Policy Optimizatio...",
    "content": "  This note will be consistently updated.PPO TricksThere are a total of 37 tricks, among which 13 are relatively core.      PPO paper    The 37 Implementation Details of Proximal Policy Optimization.    å½±å“PPOç®—æ³•æ€§èƒ½çš„10ä¸ªå…³é”®æŠ€å·§ï¼ˆé™„PPOç®—æ³•ç®€æ´Pytorchå®ç°ï¼‰ - Beamançš„æ–‡ç«  - çŸ¥ä¹  Adam Optimizer Epsilon Parameterself.actor_optim = torch.optim.Adam(self.actor.parameters(), lr=config.lr_actor, eps=1e-5)self.critic_optim = torch.optim.Adam(self.critic.parameters(), lr=config.lr_critic, eps=1e-5)Gradient Clipself.critic_optim.zero_grad()critic_loss.backward()torch.nn.utils.clip_grad_norm_(self.critic.parameters(), config.clip_range) # hereself.critic_optim.step()self.actor_optim.zero_grad()loss_actor.mean().backward()torch.nn.utils.clip_grad_norm_(self.actor.parameters(), config.clip_range) # hereself.actor_optim.step()Tanh Activation Function# A continuous actorclass Actor(torch.nn.Module):    def __init__(self):        super(Actor, self).__init__()        self.mlp = torch.nn.Sequential(            torch.nn.Linear(config.state_size, config.mlp_dim), torch.nn.Tanh(),            torch.nn.LayerNorm(config.mlp_dim),            torch.nn.Linear(config.mlp_dim, config.action_num),            torch.nn.Tanh()        )        self.log_std = torch.nn.Parameter(torch.zeros(1, config.action_num))  # Gaussian std, learnable    def forward(self, state):        mean_raw = self.mlp(state)  # [-1, 1]        mean = mean_raw * config.action_space_range  # [-max_a, max_a]        std = torch.exp(self.log_std)  # std=exp(log_std)&gt;0        distribution = torch.distributions.Normal(mean, std)        return distributionPolicy Entropydef choose_action(self, state):    state_tensor = torch.tensor(state).to(torch.float32).squeeze()    distribution = self.actor(state_tensor)    dist_entropy = distribution.entropy()    action = distribution.sample().squeeze(dim=0)    log_prob = distribution.log_prob(action)    return action.detach().numpy(), log_prob, dist_entropynegative_loss_actor = log_prob * TD_error.detach() + dist_entropy * config.entropy_coeloss_actor = - negative_loss_actorself.actor_optim.zero_grad()loss_actor.mean().backward()self.actor_optim.step()Reward ScalingIncremental meanI have a dataset with $n$ samples ${x_1, x_2, \\ldots, x_n}$. The expectation of $X$ is calculated as\\[\\mu_n = \\frac{1}{n}\\sum\\limits_{i=1}^n x_i\\]Then I get a new sample $x_{n+1}$, then the expectation of $X$ should be updated. And it can be represented by the current expectation:\\[\\mu_{n+1} = \\mu_n + \\frac{1}{n+1}\\left(x_{n+1} - \\mu_n \\right)\\]Derivation:\\[\\begin{aligned}  \\mu_{n+1} =&amp; \\frac{1}{n+1}\\sum\\limits_{i=1}^{n+1} x_i   =  \\frac{1}{n+1}\\left(x_{n+1} + \\sum\\limits_{i=1}^n x_i \\right) \\\\  =&amp;  \\frac{1}{n+1}x_{n+1} + \\frac{n}{n+1} \\sum\\limits_{i=1}^n x_i \\\\  =&amp;  \\frac{1}{n+1}x_{n+1} + \\left(1 - \\frac{1}{n+1}\\right) \\mu_n \\\\  =&amp; \\mu_n + \\frac{1}{n+1}\\left(x_{n+1} - \\mu_n \\right)\\end{aligned}\\]To reduce the impact of previous samples, the coefficient is fixed as a constant $\\alpha$:\\[\\begin{aligned}  \\mu_{n+1}   =&amp; \\mu_n + \\alpha\\left(x_{n+1} - \\mu_n \\right) \\\\  =&amp; \\alpha\\cdot x_{n+1} - \\left(1-\\alpha\\right)\\cdot\\mu_n \\end{aligned}\\]Incremental Variance\\[s_n^2 = s_{n-1}^2 + \\frac{(x_n - \\mu_{n-1})(x_n - \\mu_n)}{n}\\]\\[s_n^2 = \\frac{1}{n} \\sum_{i=1}^n (x_i - \\mu_n)^2\\]\\[\\mu_n = \\mu_{n-1} + \\frac{1}{n}(x_n - \\mu_{n-1})\\]\\[\\begin{aligned}s_n^2 =&amp; \\frac{1}{n} \\sum_{i=1}^n \\left(x_i - \\mu_{n-1} - \\frac{1}{n}(x_n - \\mu_{n-1})\\right)^2 \\\\=&amp; \\frac{1}{n} \\sum_{i=1}^{n-1}(x_i - \\mu_{n-1})^2 + \\frac{1}{n}(x_n - \\mu_{n-1})^2 - 2\\frac{1}{n}(x_n - \\mu_{n-1})\\sum_{i=1}^{n-1}(x_i - \\mu_{n-1}) + \\frac{1}{n^2}(x_n - \\mu_{n-1})^2\\sum_{i=1}^{n-1}1\\end{aligned}\\]\\[\\sum_{i=1}^{n-1}(x_i - \\mu_{n-1}) = 0\\]\\[\\begin{aligned}s_n^2 =&amp; \\frac{1}{n} \\sum_{i=1}^{n-1}(x_i - \\mu_{n-1})^2 + \\frac{1}{n}(x_n - \\mu_{n-1})^2 - \\frac{n-1}{n^2}(x_n - \\mu_{n-1})^2 \\\\=&amp; \\frac{n-1}{n}s_{n-1}^2 + \\frac{1}{n}(x_n - \\mu_{n-1})^2 - \\frac{n-1}{n^2}(x_n - \\mu_{n-1})^2 \\\\=&amp; s_{n-1}^2 + \\frac{1}{n}(x_n - \\mu_{n-1})(x_n - \\mu_n)\\end{aligned}\\]Embedding for the Q-value Critic  Check the implementation of DIAL.In my understanding, after going through the embedding, inputs with different ranges can be considered as linearly independent quantities in the same space, so they can be added directly.# From the CoLab: https://colab.research.google.com/gist/MJ10/2c0d1972f3dd1edcc3cd17c636aac8d2/dial.ipynb#scrollTo=G5e0IeqmIJJjclass CNet(nn.Module):    def __init__(self, opts):        \"\"\"        Initializes the CNet model        \"\"\"        super(CNet, self).__init__()        self.opts = opts        self.comm_size = opts['game_comm_bits']        self.init_param_range = (-0.08, 0.08)        ## Lookup tables for the state, action and previous action.        self.action_lookup = nn.Embedding(opts['game_nagents'], opts['rnn_size'])        self.state_lookup = nn.Embedding(2, opts['rnn_size'])        self.prev_action_lookup = nn.Embedding(opts['game_action_space_total'], opts['rnn_size'])        # Single layer MLP(with batch normalization for improved performance) for producing embeddings for messages.        self.message = nn.Sequential(            nn.BatchNorm1d(self.comm_size),            nn.Linear(self.comm_size, opts['rnn_size']),            nn.ReLU(inplace=True)        )        # RNN to approximate the agentâ€™s action-observation history.        self.rnn = nn.GRU(input_size=opts['rnn_size'], hidden_size=opts['rnn_size'], num_layers=2, batch_first=True)        # 2 layer MLP with batch normalization, for producing output from RNN top layer.        self.output = nn.Sequential(            nn.Linear(opts['rnn_size'], opts['rnn_size']),            nn.BatchNorm1d(opts['rnn_size']),            nn.ReLU(),            nn.Linear(opts['rnn_size'], opts['game_action_space_total'])        )    def forward(self, state, messages, hidden, prev_action, agent):        \"\"\"        Returns the q-values and hidden state for the given step parameters        \"\"\"        state = Variable(torch.LongTensor(state))        hidden = Variable(torch.FloatTensor(hidden))        prev_action = Variable(torch.LongTensor(prev_action))        agent = Variable(torch.LongTensor(agent))        # Produce embeddings for rnn from input parameters        z_a = self.action_lookup(agent)        z_o = self.state_lookup(state)        z_u = self.prev_action_lookup(prev_action)        z_m = self.message(messages.view(-1, self.comm_size))        # Add the input embeddings to calculate final RNN input.        z = z_a + z_o + z_u + z_m        z = z.unsqueeze(1)        rnn_out, h = self.rnn(z, hidden)        # Produce final CNet output q-values from GRU output.        out = self.output(rnn_out[:, -1, :].squeeze())        return h, outGumbel-Softmax  Reparameterization.  Maintain gradients from the sampled variables.  Commonly used in communication methods.What is gumbel-softmax for?If $a_t\\sim \\pi_\\theta(\\cdot \\mid s_t)$, then how to calculate $\\nabla_\\theta a_t$?What is reparameterization?This trick decouples the deterministic part and the random part of a variable.This concept can be best illustrated with the example of the Gaussian distribution.If $z\\sim \\mathcal{N}(\\mu,\\sigma^2)$, then $z = \\mu + \\sigma \\cdot \\epsilon$, where $\\epsilon\\sim \\mathcal{N}(0,1)$. In this way, $\\frac{\\partial z}{\\partial \\mu} = 1$ and $\\frac{\\partial z}{\\partial \\sigma} = \\epsilon$. Usually $\\mu$ and $\\sigma$ are estimated by a neural network, and the following gradient can be automatically calculated by deep frameworks.What does Gumbel-Softmax do?We often use neural networks to generate a probability simplex, i.e., a profile of probability where $0\\le p_i$ and $\\sum\\limits_{i} p_i = 1$. Then we will sample an $x$ based on this distribution.An example scenario is in RL, where an agent needs to choose an action $a_t$. We output a distribution $\\pi(\\cdot \\mid s_t)$ and then sample an action $a_t\\sim \\pi(\\cdot \\mid s_t)$ based on this distribution to execute.Gumbel-Softmax is used to reparameterization this kind of categorical distribution. This technique allows samples to be drawn according to the original distribution and enables gradient computation.\\[z\\sim \\arg\\max\\limits_i (\\log(p_i) + g_i),\\]where $g_i = -\\log(-\\log (u_i)), u_i\\sim U(0,1)$.The argmax is non-differentiable, it can be replaced with softmax. $i = \\arg\\max\\limits_{j} (x_j)$.\\[\\mathrm{softmax}_T (x) = \\frac{e^{x_j/T}}{\\sum_k e^{x_k/T}}.\\]If temperature $T$ is small enough, then the output of the softmax can be seen as a one-hot vector which indicates $i$.$x\\ne \\log(\\mathrm{softmax}(x))$\\[\\mathrm{softmax}(x_i) = \\frac{e^{x_i}}{\\sum_{j=1}^{n} e^{x_j}}\\]\\[\\begin{aligned}    \\log(\\mathrm{softmax}(x_i))     =&amp; \\log\\left(\\frac{e^{x_i}}{\\sum_{j=1}^{n} e^{x_j}}\\right) \\\\    =&amp; x_i - \\log\\left(\\sum_{j=1}^{n} e^{x_j}\\right)\\end{aligned}\\]import torchx = torch.rand(5)x1 = torch.nn.Softmax(dim=0)(x)x2 = torch.nn.functional.softmax(x, dim=0)x3 = torch.nn.functional.log_softmax(x, dim=0)print(x1)print(x2)print(torch.log(x1))print(x3)tensor([0.1385, 0.1978, 0.2231, 0.2861, 0.1543])tensor([0.1385, 0.1978, 0.2231, 0.2861, 0.1543])tensor([-1.9766, -1.6204, -1.4999, -1.2512, -1.8686])tensor([-1.9766, -1.6204, -1.4999, -1.2512, -1.8686])Example codeimport torchif __name__ == '__main__':    batch_size = int(1e7)    logits_distribution = [2, 3]    logits_batch = torch.tensor(logits_distribution, dtype=torch.float64) \\        .unsqueeze(dim=0).expand(batch_size, len(logits_distribution))    softmax = torch.nn.Softmax(dim=-1)    pi = softmax(logits_batch)    # -----    # The standard way.    temperature = 1    actions_sampled = torch.nn.functional.gumbel_softmax(logits_batch, tau=temperature, hard=True)    a0_num = torch.sum(actions_sampled[:, 0])    a1_num = torch.sum(actions_sampled[:, 1])    print(pi[0], a0_num, a1_num, sep=\"\\n\")    # -----    # In RL, the common epsilon-greedy is a operation on the policy space.    # To sample it, we need to edit the policy first, and then put log(pi) into the gumbel-softmax.    # See https://stackoverflow.com/questions/64980330/input-for-torch-nn-functional-gumbel-softmax    print('===============')    temperature = 1    actions_sampled = torch.nn.functional.gumbel_softmax(torch.log(pi), tau=temperature, hard=True)    a0_num = torch.sum(actions_sampled[:, 0])    a1_num = torch.sum(actions_sampled[:, 1])    print(pi[0], a0_num, a1_num, sep=\"\\n\")tensor([0.2689, 0.7311], dtype=torch.float64)tensor(2687766., dtype=torch.float64)tensor(7312234., dtype=torch.float64)===============tensor([0.2689, 0.7311], dtype=torch.float64)tensor(2690092., dtype=torch.float64)tensor(7309908., dtype=torch.float64)  Applying Gumbel-Softmax may cause NaN during training. Changing the data type of the variable to float64 seems to have avoided this issue.Computation graphCheck my note on computation graph.Social Influence  A MARL method.  An intrinsic reward.  Agent $i$ chooses the action that has the most impact on others.\\[\\begin{aligned}    r_t^i     =&amp; \\sum\\limits_{j\\ne i} D_{KL}\\left[\\pi^j(a_t^j \\mid s_t, a_t^i) \\Big\\Vert \\sum\\limits_{a_t^{i\\prime}} \\pi^j(a_t^j \\mid s_t, a_t^{i\\prime})\\cdot \\pi^i(a_t^{i\\prime}\\mid s_t) \\right] \\\\    =&amp; \\sum\\limits_{j\\ne i} D_{KL}\\left[\\pi^j(a_t^j \\mid s_t, a_t^i) \\Big\\Vert P(a_t^j\\mid s_t) \\right]\\end{aligned}\\]In the principal-agent communication:\\[r^i = D_{KL}\\left[ \\pi^j(a^j\\mid\\sigma^i) \\Big\\Vert \\sum\\limits_{\\sigma'}\\varphi^i(\\sigma^{i\\prime}\\mid s)\\cdot \\pi^j(a^j\\mid\\sigma^{i\\prime})\\right]\\]BasicsTD(0)Resampling techniques are a class of statistical methods that involve creating new samples by repeatedly drawing observations from the original data sample.Bootstrapping is a method where new â€œbootstrap samplesâ€ are created by drawing observations with replacement from the original sample.In RL, a common example is the Temporal Difference (TD) learning. This method bootstraps from the current estimate of the value function. The value function is defined as\\[V(s) = \\mathbb{E}\\left[\\sum\\limits_{t=0}^\\infty \\gamma^t \\cdot r_t | s_0 = s\\right]\\]But if the trajectory will never end, then we cannot get all the $r_t$ that we need to calculate the expectation.According to the Bellman equation, the value function can be calculated as\\[V(s) = \\mathbb{E}\\left[r_{t+1} + \\gamma V(s_{t+1}) | s_t = s\\right]\\]Now I get a new sample of $R_{t+1}$, I can use it to update $V(s_t)$, using the incremental mean trick.\\[V(s_t) \\gets V(s_t) + \\alpha\\left(x_{n+1} - V(s_t) \\right),\\]where $x_{n+1} = r_{t+1} + \\gamma V(s_{t+1}).$ The $V(s_{t+1})$ is not the ground true value, but we can used it. (Proving convergence is another thing to do.)So we can say that the value function is updated based on itself. And this method uses $V(s_{t+1})$ instead of $\\sum\\limits_{k=t}^\\infty \\gamma^{k-t}\\cdot r_{k+2}.$ And thatâ€™s what bootstrapping means."
  },
  
  {
    "title": "Code Toolbox",
    "url": "/posts/Code-Toolbox/",
    "categories": "Code",
    "tags": "tech, toolbox",
    "date": "2023-04-09 18:40:00 +0000",
    





    
    "snippet": "  This note will be consistently updated.Per-Sample Gradient  $\\mathrm{batch_size} = n$,  $\\boldsymbol x \\to \\mathrm{net}(\\boldsymbol w) \\to \\boldsymbol y \\to \\boldsymbol L \\to L_{scalar}$         ...",
    "content": "  This note will be consistently updated.Per-Sample Gradient  $\\mathrm{batch_size} = n$,  $\\boldsymbol x \\to \\mathrm{net}(\\boldsymbol w) \\to \\boldsymbol y \\to \\boldsymbol L \\to L_{scalar}$          $\\boldsymbol w \\gets \\boldsymbol w + \\frac{\\alpha}{n}\\cdot \\frac{\\partial L_{scalar}}{L_i} \\cdot \\frac{L_i}{\\partial \\boldsymbol w}$        Accomplishing it by for costs lots of time.Hook  PyTorchä¸­ï¼Œå¯ä»¥è‡ªå·±å®šä¸€ä¸ªhookå‡½æ•°ï¼Œç»™nn.Moduleç™»è®°          ç™»è®°å®Œåï¼Œnn.Moduleåœ¨forwardçš„æ—¶å€™ä¼šè§¦å‘è¿™ä¸ªhookå‡½æ•°      ä¹Ÿå¯ä»¥é€‰æ‹©è®©å…¶åœ¨backwardçš„æ—¶å€™è§¦å‘hookå‡½æ•°        hookå‡½æ•°çš„å‚æ•°æ˜¯å›ºå®šçš„ï¼š(module, grad_input, grad_output)          hookå‡½æ•°è¢«è§¦å‘åï¼Œè‡ªåŠ¨æœé›†å½“å‰è§¦å‘çŠ¶æ€ä¸‹çš„è¿™3ä¸ªå‚æ•°ï¼Œå› æ­¤å¯ä»¥ç”¨hookå®ç°æœé›†ä¸€äº›ä¸­é—´é‡      grad_inputæ˜¯åå‘ä¼ æ’­çš„é‡å¯¹moduleçš„inputçš„æ¢¯åº¦        $\\frac{\\partial L}{\\partial w} = \\sum\\limits_i \\frac{\\partial L}{\\partial L_i} \\cdot\\frac{\\partial L_i}{\\partial y_i}\\cdot\\frac{\\partial y_i}{\\partial w}$          $\\frac{\\partial L}{\\partial L_i} \\cdot\\frac{\\partial L_i}{\\partial y_i}=\\mathrm{grad_output}$      Opacus  è®©PyTorchè®­ç»ƒæ¨¡å‹æ—¶èƒ½åšå·®åˆ†éšç§çš„ä¸€ä¸ªåº“  DP-SGD (Differentially-Private Stochastic Gradient Descent)          è¦è®©Losså¯¹æ¯ä¸ªsampleçš„gradéƒ½åšä¸€ä¸ªclipï¼Œå†åŠ ä¸ªå™ªå£°      æ‰€ä»¥è¦æ±‚per-sample gradient        ä»–ä»¬ä¹Ÿæ˜¯ç”¨çš„hookæ¥åšçš„ï¼Œä½†æ˜¯æ˜¯å°è£…å¥½äº†ï¼Œå¯ä»¥ç›´æ¥ç”¨vmap  v = vectorization  æ–°å‡½æ•° = vmap(è¦åšçš„æ‰¹é‡æ“ä½œçš„å‡½æ•°ï¼Œè¾“å…¥çš„é‡æŒ‰å“ªä¸ªç»´åº¦ä½œåˆ†å‰²)  æ‰¹é‡æ“ä½œçš„ç»“æœ = æ–°å‡½æ•°(æ‰¹é‡çš„åŸå‡½æ•°çš„è¾“å…¥)  ç°åœ¨è¦æ‰¹é‡æ±‚æ¢¯åº¦ï¼Œé‚£ä¹ˆè¦ç»™vmapä¼ å…¥ä¸ªæ±‚æ¢¯åº¦çš„å‡½æ•°  vmapä¸æ”¯æŒautogradï¼Œä½†æœ‰å‡½æ•°ä»£æ›¿  å…·ä½“å†™åœ¨äº†22.9.14çš„å®éªŒè¿›å±•é‡ŒPython Profile  Used to find performance bottlenecks.  Can be easily done by clicking the button in the upper right corner, if you are using PyCharm (Professional Edition).  Check this website.Tmux  tmux ls  tmux attach-session -t 0Terminal Python Environment Initializationsource ~/.bash_profileconda activate rlbasicCheck the status of GPU or CPU(â€¦ in a terminal)  GPU: nvidia-smi  CPU: topGithubCreate a repo  Click the green button New on the GitHub repo website.  Do not check the Add a README file.  Copy the link with the .git extension.  Create a directory locally and enter it in a terminal.  git init  git remote add origin [xxx.git]  The content inside [] is a variable.  The origin is a default name that refers to the original location (i.e., the remote repositoryâ€™s URL) from which you cloned the repository. When you use the git clone [URL] command to clone a repository, Git automatically names the remote repositoryâ€™s URL as origin.Lazy commitCreate a snippet in the software Termius:git add .git commit -m \"[commit_info]\"git push origin [branch_name]Then enter your github name and your git temporary token.Download  Create a new terminal at the folder where you want to download the repo. The downloaded repo will be a subfolder, and its contents are what you see on the webpage.  git clone [repo_URL(xxx.git)] (Download.)  Enter the subfolder.The git clone will create a subfolder (named after the repo) in your current folder.Branch  git branch -a (List all the braches.)  git checkout [branch_name] (Switch to a branch.)  git checkout -b [branch_name] (Create a branch.)Get updatedWay 1  git fetch origin (Retrieve the changes from all branches.)  git merge origin/[remote_branch_name] [local_branch_name]Way 2pull = fetch + mergegit pull origin [remote_branch_name] (Update the code in your current local branch.)RandomIntegerimport torchtorch.randint(1,5,[2])\t# [1,4]çš„æ•´æ•°ï¼Œç”Ÿæˆ2ä¸ªæ•°ï¼Œå¯é‡å¤torch.randint(5,[1,2])\t# [0,4]çš„æ•´æ•°ï¼Œç»“æœæ˜¯ç±»ä¼¼è¿™æ ·çš„ï¼štensor([[2, 4]])import randomrandom.randint(-1,2) # [-1,2]çš„æ•´æ•°ï¼Œç”Ÿæˆ1ä¸ªimport randomnumbers = random.choices(range(101), k=10) # [0,100]ï¼Œç”Ÿæˆ10ä¸ªï¼Œå¯ä»¥é‡å¤print(numbers)import randomnumbers = random.sample(range(101), 10) # [0,100]çš„æ•´æ•°ï¼Œç”Ÿæˆ10ä¸ªï¼Œä¸ä¼šé‡å¤print(numbers)import randomitems = [i for i in range(1, 6)] # [1,5]çš„æ•´æ•°åˆ—è¡¨random.shuffle(items)\t#æ‰“ä¹±é¡ºåºprint(items[:k])\t\t#ä¿ç•™å‰kä¸ªï¼›kè‡ªå·±å–RealUniform distributiontorch.rand(2, 3) # size (2,3)ï¼Œæ¯ä¸€ä¸ªæ•°éƒ½æ˜¯ä»[0,1)å‡åŒ€é‡‡æ ·çš„Normal distributiontorch.randn(2, 3) # size (2,3)ï¼Œæ¯ä¸€ä¸ªæ•°éƒ½æ˜¯ä»æ­£æ€åˆ†å¸ƒN(0,1)é‡‡æ ·çš„  Check the PyTorch documentation.Customized Module Templateimport torchimport torch.nn as nnimport osclass net_base(nn.Module):    def __init__(self, n_channels, config, name, device=None):        super().__init__()        self.n_channels = n_channels        # padding for keeping the width and height of input unchanged: kernel=3, padding=1; kernel=5, padding= 2; ...        self.conv_layer = nn.Sequential(            nn.Conv2d(n_channels, config.nn.n_filters, config.nn.kernel, config.nn.stride,                      padding=int((config.nn.kernel - 1) / 2), dtype=torch.double), nn.ReLU(),        )        obs_vector = config.env.map_height * config.env.map_width * config.nn.n_filters        self.mlp = nn.Sequential(            nn.Linear(obs_vector, config.nn.hidden_width, dtype=torch.double), nn.ReLU(),            nn.Linear(config.nn.hidden_width, config.nn.hidden_width, dtype=torch.double), nn.ReLU(),        )        self.name = name        self.checkpoint_file = os.path.join(config.path.saved_models, config.main.exp_name, name)        # print(os.getcwd())        if not os.path.exists(os.path.join(config.path.saved_models, config.main.exp_name)):            os.makedirs(os.path.join(config.path.saved_models, config.main.exp_name), exist_ok=True)        self.device = device        self.to(self.device)    def forward(self, x):        return self.mlp(self.conv_layer(x).view(x.shape[0], -1))    def save_checkpoint(self):        torch.save(self.state_dict(), self.checkpoint_file)        return    def load_checkpoint(self, path=None):        if not path:            self.load_state_dict(torch.load(self.checkpoint_file, map_location=self.device))        else:            load_path = os.path.join(path, self.name)            self.load_state_dict(torch.load(load_path, map_location=self.device))Web CrawlerResources  A nice blog cuiqingcai (in Chinese).ML TricksDropout  Generated by ChatGPT 4Dropoutçš„åŸç†ï¼šDropoutæ˜¯ä¸€ç§æ­£åˆ™åŒ–æŠ€å·§ï¼Œç”¨äºé˜²æ­¢ç¥ç»ç½‘ç»œè¿‡æ‹Ÿåˆã€‚å®ƒåœ¨è®­ç»ƒæœŸé—´éšæœºåœ°â€ä¸¢å¼ƒâ€æˆ–â€œå…³é—­â€ä¸€éƒ¨åˆ†ç¥ç»å…ƒï¼Œå³å°†å®ƒä»¬çš„è¾“å‡ºè®¾ç½®ä¸º0ã€‚è¿™æ ·åšå¯ä»¥å‡å°‘ç¥ç»å…ƒä¹‹é—´çš„ç›¸äº’ä¾èµ–ï¼Œä»è€Œé¼“åŠ±æ¯ä¸ªç¥ç»å…ƒç‹¬ç«‹åœ°å­¦ä¹ ç‰¹å¾ã€‚å…·ä½“æ¥è¯´ï¼ŒDropoutæ“ä½œå¦‚ä¸‹ï¼š  å¯¹äºæ¯ä¸ªè®­ç»ƒæ ·æœ¬ï¼Œåœ¨å‰å‘ä¼ æ’­æ—¶ï¼Œæ¯ä¸ªç¥ç»å…ƒéƒ½æœ‰æ¦‚ç‡pè¢«è®¾ç½®ä¸º0ã€‚  åœ¨åå‘ä¼ æ’­æ—¶ï¼Œè¢«è®¾ç½®ä¸º0çš„ç¥ç»å…ƒä¸ä¼šæ›´æ–°å…¶æƒé‡ã€‚  åœ¨æµ‹è¯•æˆ–éªŒè¯æ—¶ï¼Œä¸ä½¿ç”¨dropoutï¼Œä½†ä¸ºäº†å¹³è¡¡å› dropoutå¯¼è‡´çš„è¾“å‡ºå˜åŒ–ï¼Œæˆ‘ä»¬å°†ç¥ç»å…ƒçš„è¾“å‡ºä¹˜ä»¥(1-p)è¿›è¡Œç¼©æ”¾ã€‚PyTorchä¸­çš„Dropoutä»£ç ç¤ºä¾‹ï¼šä»¥ä¸‹æ˜¯ä½¿ç”¨PyTorchçš„nn.Dropoutæ¨¡å—çš„ç®€å•ç¤ºä¾‹ï¼šimport torchimport torch.nn as nnclass SimpleNN(nn.Module):    def __init__(self, input_dim, hidden_dim, output_dim):        super(SimpleNN, self).__init__()                # å®šä¹‰ä¸€ä¸ªç®€å•çš„ä¸‰å±‚ç¥ç»ç½‘ç»œï¼Œå…¶ä¸­åŒ…å«ä¸€ä¸ªdropoutå±‚        self.fc1 = nn.Linear(input_dim, hidden_dim)        self.dropout = nn.Dropout(p=0.5)  # è®¾ç½®dropoutæ¦‚ç‡ä¸º0.5        self.fc2 = nn.Linear(hidden_dim, output_dim)    def forward(self, x):        x = torch.relu(self.fc1(x))        x = self.dropout(x)  # åœ¨éšè—å±‚ååº”ç”¨dropout        x = self.fc2(x)        return x# åˆ›å»ºä¸€ä¸ªç®€å•çš„æ¨¡å‹å®ä¾‹model = SimpleNN(input_dim=10, hidden_dim=20, output_dim=2)input_tensor = torch.randn(5, 10)  # åˆ›å»ºä¸€ä¸ª5x10çš„éšæœºè¾“å…¥å¼ é‡output = model(input_tensor)print(output)MaskAs indicesimport torchdata = torch.arange(5)  # tensor([0, 1, 2, 3, 4])mask = data &lt;= 2  # tensor([ True,  True,  True, False, False]); any condition is okdata[mask] = 0  # tensor([0, 0, 0, 3, 4])Retain gradientsimport torchdata_shape = 5, 3data = torch.arange(15, dtype=torch.float64).view(data_shape).requires_grad_(True)mask = data &lt;= 6  # any condition is okdata_masked = data * maskloss = data_masked.sum()loss.backward()grad1 = data_masked.gradgrad2 = data.grad'''data_masked: tensor([[0., 1., 2.],        [3., 4., 5.],        [6., 0., 0.],        [0., 0., 0.],        [0., 0., 0.]], dtype=torch.float64, grad_fn=&lt;MulBackward0&gt;)data_masked.grad: Nonedata.grad:tensor([[1., 1., 1.],        [1., 1., 1.],        [1., 0., 0.],        [0., 0., 0.],        [0., 0., 0.]], dtype=torch.float64)'''argparse      A tutorial.    Documentation.  argparse_test.py:import argparseif __name__ == '__main__':    parser = argparse.ArgumentParser()    parser.add_argument(\"positional_arg1\", help=\"A message to the user.\")    parser.add_argument(\"positional_arg2\")    parser.add_argument(\"-o1\", \"--optional_arg1\")    parser.add_argument(\"-o2\", \"--optional_arg2\", action=\"store_true\", help=\"Input -o2 will set it True.\")    parser.add_argument(\"-o3\", \"--optional_arg3\", default=\"wuwu\")    args = parser.parse_args()    print(args, args.positional_arg1, args.positional_arg2, sep=\"\\n\")Run (help):  python3 argparse_test.py -h  python3 argparse_test.py --helpResult:usage: argparse_test.py [-h] [-o1 OPTIONAL_ARG1] [-o2] [-o3 OPTIONAL_ARG3] positional_arg1 positional_arg2positional arguments:  positional_arg1       A message to the user.  positional_arg2optional arguments:  -h, --help            show this help message and exit  -o1 OPTIONAL_ARG1, --optional_arg1 OPTIONAL_ARG1  -o2, --optional_arg2  Input -o2 will set it True.  -o3 OPTIONAL_ARG3, --optional_arg3 OPTIONAL_ARG3Run:python3 argparse_test.py xixi hahaResult:Namespace(optional_arg1=None, optional_arg2=False, optional_arg3='wuwu', positional_arg1='xixi', positional_arg2='haha')xixihahaRun:python3 argparse_test.py xixi haha -o2 -o1 heiheiResult:Namespace(optional_arg1='heihei', optional_arg2=True, optional_arg3='wuwu', positional_arg1='xixi', positional_arg2='haha')xixihahaChange Dir to the Project Dir  Make sure that there is a README.md file under the project directory. This file is used as an identifier.import osdef find_project_directory(identifier_file):    current_path = \".\"    while True:        if identifier_file in os.listdir(current_path):            return os.path.abspath(current_path)        parent_path = os.path.join(current_path, \"..\")        if os.path.abspath(parent_path) == os.path.abspath(current_path):            return None        current_path = parent_pathdef cd_project_directory():    project_directory = find_project_directory(\"README.md\")    if project_directory:        os.chdir(project_directory)    else:        print(\"Project directory not found.\")    return project_directoryif __name__ == '__main__':    project_directory = cd_project_directory()    print('All done.')printSeparator lineimport shutilterminal_columns = shutil.get_terminal_size().columnsprint('=' * terminal_columns)Set Seeddef all_seed(env, seed=1):    env.seed(seed)  # env config    np.random.seed(seed)    random.seed(seed)    torch.manual_seed(seed)  # config for CPU    torch.cuda.manual_seed(seed)  # config for GPU    os.environ['PYTHONHASHSEED'] = str(seed)  # config for python scripts    # config for cudnn    torch.backends.cudnn.deterministic = True    torch.backends.cudnn.benchmark = False    torch.backends.cudnn.enabled = False"
  },
  
  {
    "title": "Paper Toolbox",
    "url": "/posts/Paper-Toolbox/",
    "categories": "Misc Notes",
    "tags": "tech, toolbox",
    "date": "2023-04-08 18:40:00 +0000",
    





    
    "snippet": "  This note will be consistently updated.Frequently Referenced PapersClassic RL milestones  Atari  Go  Poker  video games  bioinformatics  economicsMARLExpressionsCool  The canonical formulation co...",
    "content": "  This note will be consistently updated.Frequently Referenced PapersClassic RL milestones  Atari  Go  Poker  video games  bioinformatics  economicsMARLExpressionsCool  The canonical formulation considers â€¦  Domain-specific  The ego agent (for which we design the algorithm)Avoid the following usages  canâ€™t (better: cannot)  can not (correct: cannot)  whatâ€™s / itâ€™s (better: what is / it is)  whatâ€™s more (better: additionally)  traditional (better: classic, vanilla)  he/she  Firstly, â€¦ (better: First, â€¦)"
  },
  
  {
    "title": "Math Toolbox",
    "url": "/posts/Math-Toolbox/",
    "categories": "Mathematics",
    "tags": "tech, math, toolbox",
    "date": "2023-04-07 18:40:00 +0000",
    





    
    "snippet": "  This note will be consistently updated.OptimizationBasicsThe standard form for an optimization problem (the primal problem) is the following:\\[\\begin{aligned}&amp;\\min\\limits_{x} \\quad f_0(x)  \\\\...",
    "content": "  This note will be consistently updated.OptimizationBasicsThe standard form for an optimization problem (the primal problem) is the following:\\[\\begin{aligned}&amp;\\min\\limits_{x} \\quad f_0(x)  \\\\&amp;\\begin{array}{cc}\\mathrm{s.t.} \t&amp;f_i(x) \\le 0,\t&amp; i=1,2,\\ldots,m\\\\\t\t\t\t&amp;h_i(x) = 0,\t\t&amp; i=1,2,\\ldots,p\\\\\\end{array}\\end{aligned}\\]      optimization variable: $x\\in \\mathbb{R}^n$        $\\mathrm{dom}(x)=D=\\bigcap\\limits_{i=1}^m \\mathrm{dom}(f_i) \\cap \\bigcap\\limits_{i=1}^p\\mathrm{dom}(h_i)$        objective function (cost function): $f_0(x):\\mathbb{R}^n\\to \\mathbb{R}$        $x^*=\\arg\\min\\limits_{x\\in D} f_0(x),$ subjects to the constraints.        $p^*=f_0(x^*)$  ConvexConvex set:Epigraphs: ${(x,y):y\\ge f(x)}$Convex function: a function is convex if and only if its epigraph is convex, and the epigraph of a pointwise supremum is the intersection of the epigraphs. Hence, the pointwise supremum of convex func tions is convex.DualityThe Lagrangian function $L:\\mathbb{R}^n \\times \\mathbb{R}^m \\times \\mathbb{R}^p \\to \\mathbb{R}$\\(L(x,\\lambda,v)=f_0(x) + \\sum\\limits_{i=0}^m \\lambda_i f_i(x) + \\sum\\limits_{i=0}^p v_i h_i(x)\\)The Lagrange dual function $g:\\times \\mathbb{R}^m \\times \\mathbb{R}^p \\to \\mathbb{R}$\\(g(\\lambda,v)=\\inf\\limits_{x\\in D} L(x,\\lambda,v)\\)The dual function is concave even when the optimization problem is not convex, since the dual function is the pointwise infimum of a family of affine functions of $(\\lambda,v)$.Infimum = Greatest Lower Bound: the infimum of a subset $S$ partially ordered set $P$ is a greatest element in $P$ that is less than or equal to all elements of $S$, if such an element exists.The Lagrange dual problem, then, is to maximize this dual function:\\[\\begin{align*}\\max\\limits_{\\lambda,v} \\quad &amp; g(\\lambda,v) \\\\\\mathrm{s.t.} \\quad &amp; \\lambda_i \\ge 0, \\, i = 1, \\ldots, m \\\\\\end{align*}\\]Formulating a dual problem gives a lower bound for the primal problem.\\[f_0(x) \\ge L(x, \\lambda, \\nu) \\ge g(\\lambda, \\nu)\\]Solving the dual problem can provide useful insights into the primal problem, and in some circumstances, it is easier to find the solution to the primal problem through solving its dual.In certain cases (under certain conditions such as the Slater condition), the solutions to the original and dual problems will match, showcasing â€œstrong dualityâ€.Slaterâ€™s ConditionSlaterâ€™s condition is a criterion in convex optimization used to ensure strong duality. It imposes requirements on the existence of interior points concerning the constraints of the primal problem. Specifically, it requires there to be a point $x \\in \\text{relint}(D)$ (where $\\text{relint}(D)$ is the relative interior of the domain $D$) such that:\\[\\begin{align*}f_i(x) &amp; &lt; 0, &amp; i = 1, \\ldots, m \\\\h_i(x) &amp; = 0, &amp; i = 1, \\ldots, p \\\\\\end{align*}\\]Meeting Slaterâ€™s condition ensures that there is zero duality gap between the primal and dual problems, affirming strong duality.Slaterâ€™s condition ensures that there is a point that lies strictly within all inequality constraints and satisfies all equality constraints. This condition is employed to assure strong duality, i.e., the optimal value of the primal problem equals the optimal value of the dual problem.      Existence: By assuring there is a feasible point that meets all constraint conditions, Slaterâ€™s condition guarantees that both the primal and dual problems are solvable.        Gap-Free: Slaterâ€™s condition ensures a zero duality gap, i.e., there is no â€œgapâ€ between the optimal solution of the primal problem and that of the dual problem, thus assuring strong duality.  KKT ConditionsThe KKT (Karush-Kuhn-Tucker) conditions are a set of equations and inequalities necessary for finding the optimal solutions to a nonlinear constrained optimization problem. The KKT conditions comprise the following equations and inequalities:      Stationarity condition:\\(\\nabla f_0(x) + \\sum_{i=1}^m \\lambda_i \\nabla f_i(x) + \\sum_{i=1}^p v_i \\nabla h_i(x) = 0\\)        Primal feasibility:\\(\\begin{align*}f_i(x) &amp; \\leq 0, &amp; i = 1, \\ldots, m \\\\h_i(x) &amp; = 0, &amp; i = 1, \\ldots, p \\\\\\end{align*}\\)        Dual feasibility:\\(\\lambda_i \\geq 0, \\, i = 1, \\ldots, m\\)        Complementary slackness:\\(\\lambda_i f_i(x) = 0, \\, i = 1, \\ldots, m\\)  If a set of solutions $x^$, $\\lambda^$, and $v^$ satisfy these conditions, then $x^$ is a local optimal solution to the primal problem.      Primal Feasibility and Dual Feasibility: These two conditions ensure that the solution we find satisfies all constraint conditions of the primal and dual problems, respectively.        Complementary Slackness: This condition implies that for each inequality constraint, either it is tight (i.e., the equality holds) or its corresponding Lagrange multiplier is zero. This ensures that at the optimal solution, the solutions to the primal and dual problems are â€œaligned,â€ thereby affirming strong duality.        Stationarity Condition: This condition, by setting the gradient of the Lagrangian to zero, provides us with a system to solve for the potential optimal solutions.  Notation &amp; OperatorsMisc  $[[n]] = \\set{1, \\ldots, n}$.  $A := B$ means $A$ is defined as $B$.  $(f\\circ g)(x) = f(g(x))$. Function composition. \\circ.  $A^\\intercal$. vector/matrix transpose. \\intercal.  Norm $\\Vert x \\Vert$. \\Vert x \\Vert.Greek alphabet  Check here.  $\\alpha$ (\\alpha). /ËˆÃ¦lfÉ™/.  $\\beta$ (\\beta). UK: /ËˆbiËtÉ™/, US: /ËˆbeÉªtÉ™/.  $\\gamma$ (\\gamma), $\\Gamma$ (\\Gamma), $\\varGamma$ (\\varGamma). /ËˆÉ¡Ã¦mÉ™/.  $\\delta$ (\\delta), $\\Delta$ (\\Delta), $\\varDelta$ (\\varDelta). /ËˆdÉ›ltÉ™/.  $\\epsilon$ (\\epsilon), $\\varepsilon$ (\\varepsilon). /ËˆÉ›psÉªlÉ’n, É›pËˆsaÉªlÉ™n/.  $\\zeta$ (\\zeta). UK: /ËˆziËtÉ™/,[1] US: /ËˆzeÉªtÉ™/.  $\\eta$ (\\eta). /ËˆiËtÉ™, ËˆeÉªtÉ™/.  $\\theta$ (\\theta), $\\Theta$ (\\Theta), $\\vartheta$ (\\vartheta), $\\varTheta$ (\\varTheta). UK: /ËˆÎ¸iËtÉ™/, US: /ËˆÎ¸eÉªtÉ™/.  $\\iota$ (\\iota). /aÉªËˆoÊŠtÉ™/.  $\\kappa$ (\\kappa), $\\varkappa$ (\\varkappa). /ËˆkÃ¦pÉ™/.  $\\lambda$ (\\lambda), $\\Lambda$ (\\Lambda), $\\varLambda$ (\\varLambda). /ËˆlÃ¦mdÉ™/.  $\\mu$ (\\mu). /Ëˆm(j)uË/.  $\\nu$ (\\nu). /ËˆnjuË/.  $\\xi$ (\\xi), $\\Xi$ (\\Xi), $\\varXi$ (\\varXi). /zaÉª, ksaÉª/.  $o$ (o), $O$ (O). Omicron, /ËˆoÊŠmÉªkrÉ’n, ËˆÉ’mÉªkrÉ’n, oÊŠËˆmaÉªkrÉ’n/.  $\\pi$ (\\pi), $\\Pi$ (\\Pi), $\\varpi$ (\\varpi), $\\varPi$ (\\varPi). /ËˆpaÉª/.  $\\rho$ (\\rho), $\\varrho$ (\\varrho). /ËˆroÊŠ/.  $\\sigma$ (\\sigma), $\\Sigma$ (\\Sigma), $\\varsigma$ (\\varsigma), $\\varSigma$ (\\varSigma). /ËˆsÉªÉ¡mÉ™/.  $\\tau$ (\\tau). /ËˆtÉ”Ë, ËˆtaÊŠ/.  $\\upsilon$ (\\upsilon), $\\Upsilon$ (\\Upsilon), $\\varUpsilon$ (\\varUpsilon). /ËˆÊŒpsÉªËŒlÉ’n, Ëˆ(j)uËp-, -lÉ™n/.  $\\phi$ (\\phi), $\\Phi$ (\\Phi),  $\\varphi$ (\\varphi), $\\varPhi$ (\\varPhi). /faÉª/.  $\\chi$ (\\chi). /ËˆkaÉª, ËˆxiË/.  $\\psi$ (\\psi), $\\Psi$ (\\Psi), $\\varPsi$ (\\varPsi). /Ëˆ(p)saÉª, Ëˆ(p)siË/.  $\\omega$ (\\omega), $\\Omega$ (\\Omega), $\\varOmega$ (\\varOmega). /oÊŠËˆmiËÉ¡É™, oÊŠËˆmÉ›É¡É™, oÊŠËˆmeÉªÉ¡É™, É™ËˆmiËÉ¡É™/.var indicates that this quantity is a variable.Fonts  Adapted from this site.  \\mathnormal{} is the normal math italic font. It is the default font.  \\mathbb{}.          $\\mathbb{E}[X]$, the expectation of $X$.      $\\mathbb{R}$, the set of real numbers.        \\mathcal{} is the special calligraphic font for uppercase letters only.          The standard normal distribution $\\mathcal{N}(0,1)$.        \\mathbf{} gives upright Roman boldface letters.          $\\mathbf{v}$, a vector.      $\\mathbf{A}$, a matrix.        \\mathrm{} is the normal upright Roman font.          $\\mathrm{e}$, the constant â€œeâ€.      $\\mathrm{sin}$, the sine function.      $\\mathrm{softmax}$, an operators.      $\\mathrm{m}$, the unit â€œmeterâ€.      $\\mathrm{cov}(X, Y)$, the covariance of $X$ and $Y$.        \\mathtt{} gives upright letters from the typewriter type font.  It is typically used to represent computer code, variable names, function names, and other elements that need to be displayed in a fixed-width font.          \\mathtt{print(\"Hello, World!\")}      $\\mathtt{print(â€œHello, World!â€)}$        \\mathit{} gives text italic letters.  \\mathsf{} gives upright sans serif letters.Multiple variables  Addition and subtraction of matrices are element-wise.\\[\\begin{bmatrix}1 &amp; 2 \\\\3 &amp; 4 \\end{bmatrix}+\\begin{bmatrix}5 &amp; 6 \\\\7 &amp; 8 \\end{bmatrix}=\\begin{bmatrix}6 &amp; 8 \\\\10 &amp; 12 \\end{bmatrix}\\]  Multiplication of a matrix with a scalar:\\[2 \\cdot\\begin{bmatrix}1 &amp; 2 \\\\3 &amp; 4 \\end{bmatrix}=\\begin{bmatrix}2 &amp; 4 \\\\6 &amp; 8 \\end{bmatrix}\\]  Multiplication of matrices.          $AB$ or $A\\cdot B$.      $A$ is of $m\\times n$ shape, and $B$ is of $n\\times k$.      \\[\\begin{bmatrix}1 &amp; 2 \\\\3 &amp; 4 \\end{bmatrix}\\cdot\\begin{bmatrix}5 &amp; 6 \\\\7 &amp; 8 \\end{bmatrix}=\\begin{bmatrix}1\\times 5+2\\times 7 &amp; 1\\times 6+2\\times 8 \\\\3\\times 5+4\\times 7 &amp; 3\\times 9+7\\times 8\\end{bmatrix}\\]  Hadamard (element-wise) product: $\\odot$ (\\odot) or $\\circ$ (\\circ).\\[\\begin{bmatrix}1 &amp; 2 \\\\3 &amp; 4 \\end{bmatrix}\\odot\\begin{bmatrix}5 &amp; 6 \\\\7 &amp; 8 \\end{bmatrix}=\\begin{bmatrix}5 &amp; 12 \\\\21 &amp; 28 \\end{bmatrix}\\]  Dot product of vectors. Or inner product, scalar product.          $\\mathbf{u} \\cdot \\mathbf{v} = \\sum\\limits_{i}^n u_i\\cdot v_i$        Outer product of vectors.          $\\mathbf{u} = [u_1,\\ldots, u_m]^\\intercal$      $\\mathbf{v} = [v_1,\\ldots, v_n]^\\intercal$      $\\mathbf{A} = \\mathbf{u} \\otimes \\mathbf{v} = \\mathbf{u} \\mathbf{v}^\\intercal$.      $\\mathbf{A}$ is $m\\times n$.      \\[\\mathbf{u} \\otimes \\mathbf{v}=\\begin{bmatrix}   u_1\\cdot v_1 &amp; u_1\\cdot v_2 &amp; \\ldots &amp; u_1\\cdot v_n \\\\   u_2\\cdot v_1 &amp; u_2\\cdot v_2 &amp; \\ldots &amp; u_2\\cdot v_n \\\\   \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\   u_m\\cdot v_1 &amp; u_m\\cdot v_2 &amp; \\ldots &amp; u_m\\cdot v_n \\end{bmatrix}\\]  Outer product of tensors.Given two tensors $\\mathbf{u}$ and $\\mathbf{v}$ with dimensions $(k_1, k_2, \\ldots, k_m)$ and $l_1,l_2, \\ldots, l_n$, their outer product is a tensor with dimensions $(k_1, k_2, \\ldots, k_m, l_1,l_2, \\ldots, l_n)$. and entries\\[(\\mathbf{u} \\otimes \\mathbf{v})_{i_1,\\ldots,i_m, j_1,\\ldots, j_m} = u_{i_1,\\ldots,i_m}\\cdot v_{j_1, \\ldots, j_n}\\]InequalityLog-sum inequality\\[\\sum\\limits_{i=1}^n a_i \\cdot \\log \\frac{a_i}{b_i} \\ge \\left( \\sum\\limits_{i=1}^n a_i \\right) \\cdot\\log \\frac{\\sum\\limits_{i=1}^n a_i}{\\sum\\limits_{i=1}^n b_i}\\]Jensenâ€™s inequality\\[\\varphi\\left(\\mathbb{E}[X]\\right) \\le \\mathbb{E}[\\varphi(X)],\\]where $X$ is a random variable and $\\varphi$ is a convex function.Jensen gap: $\\mathbb{E}[\\varphi(X)] - \\varphi\\left(\\mathbb{E}[X]\\right)$.\\[\\varphi\\left(\\frac{\\sum a_i\\cdot x_i}{\\sum a_i}\\right) \\le \\frac{\\sum a_i \\cdot \\varphi(x_i)}{\\sum a_i}\\]Equality holds iif. $x_1 = \\ldots =x_n$ or $\\varphi$ is linear.ProbabilityExpectation\\[\\mathbb{E}[X] = \\sum\\limits_{i}p_i\\cdot x_i\\]\\[\\mathbb{E}[X] = \\int_{-\\infty}^{\\infty} x\\cdot f(x) \\, dx\\]Linearity:\\[\\mathbb{E}[X+Y] = \\mathbb{E}[X]+\\mathbb{E}[Y],\\]\\[\\mathbb{E}[aX] = a\\mathbb{E}[X].\\]If $X$ and $Y$ are independent:\\[\\mathbb{E}[XY] = \\mathbb{E}[X]\\cdot \\mathbb{E}[Y].\\]If $X$ and $Y$ are dependent:\\[\\mathbb{E}[XY] \\ne \\mathbb{E}[X]\\cdot \\mathbb{E}[Y].\\]If $X = c$, where $c\\in \\mathbb{R}$, then $\\mathbb{E}[X] = c$. Thus\\[\\mathbb{E}[\\mathbb{E}[X]] = \\mathbb{E}[X].\\]Variance\\[\\begin{aligned}   \\mathbb{V}\\left[X \\right] =&amp; \\mathbb{E}\\left[ (X - \\mathbb{E}[X])^2 \\right] \\\\   =&amp; \\mathbb{E}\\left[ X^2 - 2\\,X\\cdot \\mathbb{E}[X] +\\mathbb{E}[X]^2 \\right]\\\\   =&amp; \\mathbb{E}\\left[ X^2 - 2\\,\\mathbb{E}[X]\\cdot \\mathbb{E}[X] +\\mathbb{E}[X]^2 \\right]\\\\   =&amp; \\mathbb{E}\\left[X^2\\right] - \\mathbb{E}\\left[X\\right]^2\\end{aligned}\\]\\[\\mathbb{V}\\left[X \\right] = \\sum\\limits_{i} p_i\\cdot (x_i - \\mathbb{E}[X])^2\\]\\[\\mathbb{V}\\left[X \\right] = \\int_{\\mathbb{R}} f(x) \\cdot (x - \\mathbb{E}[X])^2 \\, dx\\]\\[\\mathbb{V}\\left[X \\right] = \\int_{\\mathbb{R}} x^2\\cdot f(x)\\, dx - \\mathbb{E}\\left[X\\right]^2\\]\\[\\mathbb{V}\\left[X \\right] = \\mathrm{Cov}(X,X)\\]\\[\\mathbb{V}[X] \\ge 0\\]\\[\\mathbb{V}[X+a] = \\mathbb{V}[X]\\]\\[\\mathbb{V}[aX] = a^2\\mathbb{V}[X]\\]\\[\\mathbb{V}[aX+bY] = a^2\\, \\mathbb{V}[X] + b^2\\, \\mathbb{V}[Y] + 2ab\\, \\mathrm{Cov}[X]\\]If there is a set of random variables $\\set{X_1,\\ldots X_N}$,then\\[\\begin{aligned}   \\mathbb{V}\\left[\\sum\\limits_i X_i\\right] =&amp; \\sum\\limits_{i,j} \\mathrm{Cov}(X_i, X_j) \\\\   =&amp; \\sum\\limits_{i}\\mathbb{V} [X_i] +\\sum\\limits_{i\\ne j}\\mathrm{Cov} (X_i, X_j).\\end{aligned}\\]Covariance\\[\\mathrm{Cov}(X,Y) = \\mathbb{E}\\left[ X - \\mathbb{E}[X] \\right] \\cdot \\mathbb{E}\\left[ Y - \\mathbb{E}[Y] \\right]\\]MomentInformation TheorySelf-information\\(I(x) = -\\log p(x)\\)ä¿¡æ¯æºå‘æ¶ˆæ¯ï¼Œä¿¡æ¯æ˜¯æ¶ˆæ¯ä¸­çš„è¯­ä¹‰ï¼Œæ˜¯æŠ½è±¡çš„ï¼Œä¿¡å·æ˜¯æ¶ˆæ¯çš„ç‰©ç†è¡¨ç¤ºï¼›ä¿¡æ¯æºå‘ä»€ä¹ˆæ¶ˆæ¯æ˜¯ä¸ç¡®å®šçš„ï¼ˆç¡®å®šçš„è¯å°±æ²¡æœ‰é€šä¿¡çš„å¿…è¦äº†ï¼‰ï¼Œæ‰€ä»¥æ¶ˆæ¯æœ‰ä¸€ä¸ªæ ·æœ¬ç©ºé—´ï¼Œå’Œä¸€ä¸ªå¯¹åº”çš„æ¦‚ç‡åˆ†å¸ƒï¼›ä¿¡æ¯æè¿°äº‹ä»¶çš„ä¸ç¡®å®šæ€§ã€‚ä¸€ä¸ªäº‹ä»¶å‘ç”Ÿï¼Œä¼šå¸¦æ¥ä¿¡æ¯ï¼›äº‹ä»¶å‘ç”Ÿçš„æ¦‚ç‡è¶Šå°ï¼Œåˆ™å…¶å‡ºç°åæ‰€å¸¦æ¥çš„ä¿¡æ¯è¶Šå¤§ã€‚äº‹ä»¶çš„ä¿¡æ¯=äº‹ä»¶çš„å‘ç”Ÿæ¦‚ç‡çš„æŸä¸ªå‡½æ•°ï¼Œ$I(a_i) = f[P(a_i)]$ï¼Œè¿™ä¸ªå«è‡ªä¿¡æ¯ï¼Œè‡ªä¿¡æ¯åº¦é‡äº†éšæœºäº‹ä»¶ä¿¡æ¯é‡çš„å¤§å°ï¼Œthe amount of informationï¼›æ ¹æ®ä¿¡æ¯çš„ç‰¹æ€§æ‰¾å‡ºè¿™ä¸ªå‡½æ•°ï¼š  äº‹ä»¶$x_i$å‘ç”Ÿçš„æ¦‚ç‡è¶Šå°ï¼Œå…¶å‘ç”Ÿåå¸¦æ¥çš„ä¿¡æ¯è¶Šå¤§  äº‹ä»¶$x_i$å‘ç”Ÿæ¦‚ç‡ä¸º1ï¼Œåˆ™å…¶å‘ç”Ÿä¸å¸¦æ¥ä¿¡æ¯  äº‹ä»¶$x_i$å‘ç”Ÿæ¦‚ç‡ä¸º0ï¼Œåˆ™å…¶å‘ç”Ÿå¸¦æ¥æ— ç©·å¤§çš„ä¿¡æ¯  ä¿¡æ¯æ˜¯å…³äºäº‹ä»¶å‘ç”Ÿæ¦‚ç‡çš„é€’å‡å‡½æ•°  ä¸¤ä¸ªäº‹ä»¶éƒ½å‘ç”Ÿçš„æ¦‚ç‡ä¸ºä¸¤ä¸ªäº‹ä»¶çš„å‘ç”Ÿæ¦‚ç‡çš„ä¹˜ç§¯ï¼Œä¸¤ä¸ªäº‹ä»¶éƒ½å‘ç”Ÿçš„ä¿¡æ¯ä¸ºè¿™ä¸¤ä¸ªäº‹ä»¶å‘ç”Ÿçš„ä¿¡æ¯çš„å’Œç„¶åä¸€äº›è¯æ˜ï¼Œè‡ªä¿¡æ¯çš„å‡½æ•°ä¸º$-\\log$ï¼Œå³ $I(x_i) = -\\log P(x_i)$æˆ‘çœ‹åˆ°æœ‰è¯´è¿™ä¸ªä¹Ÿå«log-perplexityï¼Œå›°æƒ‘ç¨‹åº¦ã€‚  æ¦‚ç‡è¶Šå¤§ï¼Œè¶Šæœ‰å¯èƒ½å‘ç”Ÿï¼Œè¶Šç¬¦åˆä¹ æƒ¯ï¼Œå›°æƒ‘ç¨‹åº¦ä½ï¼Œæ¯”å¦‚â€Mary had a little lamb.â€  æ¦‚ç‡è¶Šå°ï¼Œè¶Šä¸å¯èƒ½å‘ç”Ÿï¼Œè¶Šå¥‡æ€ªï¼Œå›°æƒ‘ç¨‹åº¦é«˜ï¼Œæ¯”å¦‚â€Correct horse battery stapler.â€Entropy\\(\\begin{aligned}   H(X) =&amp; \\mathbb{E}\\_{X}\\left[I(x)\\right]\\\\   =&amp; -\\sum\\limits_{x} p(x) \\cdot \\log p(x)\\end{aligned}\\)  ä¿¡æºçš„æ ·æœ¬ç©ºé—´æ˜¯$X$ï¼Œä¹Ÿå°±æ˜¯èƒ½å‘é€çš„ä¿¡å·ï¼ˆäº‹ä»¶çš„é›†åˆï¼‰  ä¿¡æºçš„æ¦‚ç‡ç©ºé—´æ˜¯$\\set{X, P(X)}$  äº‹ä»¶$x_i$ä»¥$P(x_i)$çš„æ¦‚ç‡å‘ç”Ÿï¼Œå‘ç”Ÿåå¸¦æ¥$I(x_i)$çš„ä¿¡æ¯  ç†µ = ä¿¡æºèƒ½å¸¦æ¥çš„å¹³å‡è‡ªä¿¡æ¯  ç†µä¸€å®šæ˜¯æ­£çš„ï¼ˆæ¦‚ç‡å¤§äº0å°äº1çœ‹çœ‹å°±çŸ¥é“ï¼‰  ç†µæ˜¯å‡¸å‡½æ•°ï¼Œç”¨Jensenä¸ç­‰å¼è¯æ˜  å‡åŒ€åˆ†å¸ƒçš„æ—¶å€™ï¼Œç†µæœ€å¤§ä¸€ä¸ªäº‹ä»¶çš„è‡ªä¿¡æ¯è¡¨ç¤ºäº†è¯¥äº‹ä»¶å‘ç”Ÿçš„ä¸ç¡®å®šæ€§ï¼Œè¯¥äº‹ä»¶å‘ç”Ÿçš„æ¦‚ç‡è¶Šå°ï¼Œåˆ™å…¶å‘ç”Ÿçš„ä¸ç¡®å®šæ€§è¶Šå¤§ï¼Œåˆ™å…¶å‘ç”Ÿå¸¦æ¥çš„ä¿¡æ¯è¶Šå¤§ï¼Œåˆ™å…¶è‡ªä¿¡æ¯è¶Šå¤§ã€‚ä¿¡æ¯ç†µæ˜¯è‡ªä¿¡æ¯çš„æœŸæœ›ï¼Œè¡¨ç¤ºä¸€ä¸ªæ¶ˆæ¯çš„æ¦‚ç‡åˆ†å¸ƒç¡®å®šæ—¶è¿™ä¸ªä¿¡æºèƒ½å¸¦æ¥çš„ä¿¡æ¯çš„å¤šå°‘ï¼Œä¹Ÿæ˜¯å¹³å‡æ¯ä¸ªä¿¡æºç¬¦å·ï¼ˆå‘é€ä¸€æ¬¡ä¿¡æ¯ï¼Œå‡ºç°ä¸€æ¬¡å‘ä¿¡æ¯äº‹ä»¶ï¼‰æ‰€æºå¸¦çš„ä¿¡æ¯é‡ã€‚The uniform distribution has the max entropyè¦è¯æ˜åœ¨ç»™å®šæ¡ä»¶ä¸‹å‡åŒ€åˆ†å¸ƒå…·æœ‰æœ€å¤§ç†µï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨æ‹‰æ ¼æœ—æ—¥ä¹˜æ•°æ³•æ¥æ‰¾åˆ°æ¦‚ç‡åˆ†å¸ƒçš„æœ€ä¼˜è§£ã€‚å‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ªç¦»æ•£çš„æ¦‚ç‡åˆ†å¸ƒ$p(x)$ï¼Œå…¶ä¸­$x$å¯ä»¥å–$n$ä¸ªä¸åŒçš„å€¼ï¼Œä¼˜åŒ–é—®é¢˜æ˜¯\\[\\begin{aligned}\\max\\limits_{p}\\quad &amp;H(p) = - \\sum_{i=1}^{n} p(x_i) \\log p(x_i) \\\\\\textrm{s.t.}\\quad &amp;\\sum_{i=1}^{n} p(x_i) = 1, \\\\\\quad &amp;p(x_i) &gt; 0, \\forall i.\\end{aligned}\\]é¦–å…ˆæ„é€ æ‹‰æ ¼æœ—æ—¥å‡½æ•°å¦‚ä¸‹ï¼š\\[L(p, \\lambda) = -\\sum_{i=1}^{n} p(x_i) \\log p(x_i) - \\lambda \\left( \\sum_{i=1}^{n} p(x_i) - 1 \\right),\\]å…¶ä¸­ $\\lambda$ æ˜¯æ‹‰æ ¼æœ—æ—¥ä¹˜æ•°ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†å¯¹$L$åˆ†åˆ«å¯¹$p(x_i)$å’Œ$\\lambda$æ±‚åå¯¼ï¼Œå¹¶å°†å…¶è®¾ä¸º0ä»¥æ‰¾åˆ°é©»ç‚¹ã€‚å¾—åˆ°ï¼š\\[\\frac{\\partial L}{\\partial p(x_i)} = -\\log p(x_i) -1 - \\lambda = 0, \\quad \\forall i,\\]\\[\\frac{\\partial L}{\\partial \\lambda} = - \\sum_{i=1}^{n} p(x_i) + 1 = 0.\\]ä»ç¬¬ä¸€ä¸ªåå¯¼æ•°æ–¹ç¨‹ä¸­æˆ‘ä»¬å¯ä»¥è§£å‡ºï¼š\\[-\\log p(x_i) -1 - \\lambda = 0 \\implies \\log p(x_i) = -\\lambda - 1 \\implies p(x_i) = e^{-1-\\lambda}.\\]æ¥ç€ï¼Œæˆ‘ä»¬å¯ä»¥å°†è¿™ä¸ªè§£ä»£å›çº¦æŸæ¡ä»¶\\[\\sum_{i=1}^{n} p(x_i) = 1 \\implies \\sum_{i=1}^{n} e^{-1-\\lambda} = 1 \\implies n e^{-1-\\lambda} = 1 \\implies e^{-1-\\lambda} = \\frac{1}{n}.\\]ç°åœ¨æˆ‘ä»¬æ‰¾åˆ°äº†$\\lambda$çš„å€¼ï¼š\\[-\\lambda -1 = \\log \\frac{1}{n} \\implies \\lambda = -\\log \\frac{1}{n} + 1.\\]ç„¶åæˆ‘ä»¬å¯ä»¥æ‰¾åˆ°$p(x_i)$çš„è§£ï¼š\\[p(x_i) = e^{-1-\\lambda} \\implies p(x_i) = e^{-1 - (-\\log \\frac{1}{n} + 1)} = \\frac{1}{n}.\\]æˆ‘ä»¬éªŒè¯è¿™ç¡®å®æ˜¯ä¸€ä¸ªæœ€å¤§ç‚¹ï¼Œé€šè¿‡è¯æ˜HessiançŸ©é˜µæ˜¯è´Ÿå®šçš„ã€‚æœ€ç»ˆæˆ‘ä»¬å¾—åˆ°æœ€ä¼˜è§£æ˜¯å‡åŒ€åˆ†å¸ƒï¼š\\[p(x_i) = \\frac{1}{n}, \\quad \\forall i.\\]The more random the signal is, the less informative it will beç†µå’ŒKLæ•£åº¦æ˜¯ä¿¡æ¯è®ºä¸­çš„ä¸¤ä¸ªæ ¸å¿ƒæ¦‚å¿µã€‚ç†µæ˜¯ç”¨æ¥è¡¡é‡ä¸€ä¸ªéšæœºå˜é‡çš„ä¸ç¡®å®šæ€§çš„é‡ï¼Œè€ŒKLæ•£åº¦ç”¨æ¥è¡¡é‡ä¸¤ä¸ªæ¦‚ç‡åˆ†å¸ƒä¹‹é—´çš„å·®å¼‚ã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨KLæ•£åº¦æ¥è¯æ˜ä¸€ä¸ªä¿¡å·çš„åˆ†å¸ƒè¶Šéšæœºï¼Œåˆ™å…¶ç†µè¶Šé«˜ã€‚ä»¥ä¸‹æ˜¯è¯æ˜æ­¥éª¤ï¼š      ä¸ºäº†è¯æ˜ä¸€ä¸ªåˆ†å¸ƒè¶Šéšæœºå…¶ç†µè¶Šé«˜ï¼Œæˆ‘ä»¬å¯ä»¥å‡è®¾æœ‰ä¸€ä¸ªåˆ†å¸ƒ $P$ ä¸ä¸€ä¸ªå®Œå…¨å‡åŒ€åˆ†å¸ƒ $U$ï¼Œå…¶ä¸­ $U$ çš„æ¯ä¸ªçŠ¶æ€çš„æ¦‚ç‡éƒ½æ˜¯ $\\frac{1}{n}$ã€‚        ç„¶åæˆ‘ä»¬è®¡ç®— $P$ ä¸ $U$ ä¹‹é—´çš„ KL æ•£åº¦ï¼š\\(D_{\\text{KL}}(P||U) = \\sum_{i=1}^{n} p(x_i) \\log \\frac{p(x_i)}{\\frac{1}{n}} = \\sum_{i=1}^{n} p(x_i) \\log (n p(x_i)) - \\log n \\sum_{i=1}^{n} p(x_i)\\)        æˆ‘ä»¬å¯ä»¥å‘ç°ï¼š\\(D_{\\text{KL}}(P||U) = H(U) - H(P) + \\log n\\)        ç”±äºKLæ•£åº¦æ€»æ˜¯éè´Ÿçš„ï¼Œæˆ‘ä»¬æœ‰ï¼š\\(H(U) - H(P) + \\log n \\geq 0 \\quad \\Rightarrow \\quad H(P) \\leq H(U) + \\log n\\)        ç”±äºå‡åŒ€åˆ†å¸ƒçš„ç†µæ˜¯æœ€å¤§çš„ï¼Œæ‰€ä»¥æˆ‘ä»¬å¯ä»¥å¾—å‡ºç»“è®ºï¼šä¸€ä¸ªåˆ†å¸ƒè¶Šéšæœºï¼Œå…¶ç†µå°±è¶Šé«˜ã€‚  Joint entropy\\(H(X, Y) = \\sum\\limits_{i,j} P(x_i,y_j)\\cdot [- \\log P(x_i,y_j)]\\)æŠŠç©ºé—´æ”¹æˆäºŒç»´çš„ï¼Œ$x_i \\in X$ å˜æˆ $(x_i,y_j)\\in X\\times Y$Conditional entropy (channel equivocation)å‡è®¾æˆ‘ä»¬åŸæœ¬å¯¹$X$çš„è®¤è¯†æ˜¯å…ˆéªŒæ¦‚ç‡$P$ï¼Œç®—çš„è¿™ä¸ªç†µä¹Ÿå«å…ˆéªŒç†µ\\[H(X) = \\sum\\limits_{i} P(x_i)\\cdot [- \\log P(x_i)]\\]æ¥æ”¶åˆ°ä¸€ä¸ªä¿¡å·$y_j$ï¼Œæˆ‘ä»¬å°±æœ‰äº†åéªŒæ¦‚ç‡ï¼Œå¯¹$X$çš„è®¤è¯†å°±æ”¹å˜äº†ï¼Œå¯ä»¥ç®—ä¸€ä¸ªåéªŒç†µ\\[H(X\\mid y_j) =  \\sum\\limits_{i} P(x_i\\mid y_j)\\cdot [- \\log P(x_i\\mid y_j)]\\]å¯ä»¥å¯¹éšæœºå˜é‡$Y$æ±‚æœŸæœ›ï¼ŒæŠŠåéªŒç†µå˜æˆæ¡ä»¶ç†µï¼Œä¹Ÿå«ä¿¡é“ç–‘ä¹‰åº¦æˆ–è€…æŸå¤±ç†µ\\[\\begin{aligned}   H(X\\mid Y)   =&amp; \\sum\\limits_{j} P(y_j) \\sum\\limits_{i} P(x_i\\mid y_j)\\cdot [- \\log P(x_i\\mid y_j)] \\\\   =&amp; \\sum\\limits_{i,j} P(x_i,y_j)\\cdot [- \\log P(x_i\\mid y_j)] \\end{aligned}\\]  å¦‚æœå‘çš„ä¿¡å·$y_j$èƒ½å”¯ä¸€ç¡®å®šä¸€ä¸ª$x_i$ï¼Œé‚£ä¹ˆ$P(x_i\\mid y_j)=1$ï¼Œ$H(X\\mid Y)=0$  å¦‚æœå‘çš„ä¿¡å·$y_j$å’Œ$x_i$æ— å…³/ç‹¬ç«‹ï¼Œåˆ™ï¿¼$P(x_i\\mid y_j) = P(x_i)$ï¼Œåˆ™ï¼š\\[H(X\\mid Y)= \\sum\\limits_{i,j} P(x_i)\\cdot P(y_j)\\cdot [- \\log P(x_i)] = H(X)\\]ç»“æœå’Œå…ˆéªŒç†µä¸€æ ·ï¼Œæˆ‘ä»¬å¯¹$X$çš„è®¤è¯†æ²¡æœ‰åœ¨æ”¶åˆ°$Y$åè€Œæ”¹å˜Conditioning never increases entropyæ¡ä»¶å¢ç›Šæ€§ï¼Œç­‰å·åªæœ‰$X$å’Œ$Y$ç‹¬ç«‹æ—¶å–åˆ°\\[H(X\\mid Y) \\le H(X)\\]Chain rule for entropy\\[H(X,Y) = H(X) + H(Y\\mid X)\\]\\[\\begin{aligned}&amp; H(X) + H(Y\\mid X) \\\\=&amp; \\sum\\limits_{i} P(x_i)\\cdot [- \\log P(x_i)]+ \\sum\\limits_{i,j} P(x_i,y_j)\\cdot [- \\log P(y_i\\mid x_j)]  \\\\=&amp; \\sum\\limits_{i,j} P(x_i,y_j) \\cdot [- \\log P(x_i)]+ \\sum\\limits_{i,j} P(x_i,y_j)\\cdot [- \\log P(y_i\\mid x_j)]  \\\\=&amp; \\sum\\limits_{i,j} P(x_i,y_j)\\cdot [- \\log P(x_i,y_j)] \\\\=&amp; H(X,Y)\\end{aligned}\\]Mutual informationå¹³å‡äº’ä¿¡æ¯ï¼šæ¥æ”¶åˆ°ä¿¡å·$Y$æ¶ˆé™¤æ‰çš„ä¸ç¡®å®šæ€§ï¼Œä¹Ÿæ˜¯è·å¾—çš„ä¿¡æ¯ï¼Œä¹Ÿæ˜¯$X$å’Œ$Y$çš„ç›¸å…³æ€§\\[\\begin{aligned}I(X;Y) &amp;= H(X) - H(X\\mid Y)\\\\&amp;=\\sum\\limits_{i} P(x_i)\\cdot [- \\log P(x_i)] - \\sum\\limits_{i,j} P(x_i,y_j)\\cdot [- \\log P(x_i\\mid y_j)] \\\\&amp;= \\sum\\limits_{i,j} P(x_i,y_j)\\cdot [\\log P(x_i\\mid y_j) - \\log P(x_i)] \\\\&amp;= \\sum\\limits_{i,j} P(x_i,y_j)\\cdot \\log \\frac{P(x_i,y_j)}{P(x_i)\\cdot P(y_j)} \\\\&amp;= D_{KL} (P_{(X,Y)} \\mid \\mid  P_X \\otimes P_Y)\\end{aligned}\\]  å¯ä»¥çœ‹åˆ°æ˜¯å¯¹ç§°çš„  å¯ä»¥å†™æˆKLæ•£åº¦  äº’ä¿¡æ¯å°±æ˜¯å…ˆéªŒæ¦‚ç‡çš„ç†µå‡å»åéªŒæ¦‚ç‡çš„ç†µï¼Œæ‰€ä»¥å…ˆéªŒå’ŒåéªŒå·®è·è¶Šå¤§ï¼Œè¶Šç›¸å…³KL divergence\\(\\begin{aligned}   D_{KL} \\left[p(X) \\Vert q(X)\\right]   =&amp; \\sum\\limits_{x\\in X} - p(x)\\cdot \\log q(x)   -p(x)\\cdot \\log p(x) \\\\   =&amp; \\sum\\limits_{x\\in X} p(x)\\cdot \\log \\frac{p(x)}{q(x)}\\end{aligned}\\)#            å‘é€çš„ä¿¡å·æ˜¯å“ªä¸ª/äº‹ä»¶ï¼š$X$      $a_1$      $a_2$                  ä¿¡æº1å‘é€ä¿¡å·çš„æ¦‚ç‡/äº‹ä»¶å‘ç”Ÿçš„æ¦‚ç‡1ï¼š$P_1(X)$      0.99      0.01              ä¿¡æº2å‘é€ä¿¡å·çš„æ¦‚ç‡/äº‹ä»¶å‘ç”Ÿçš„æ¦‚ç‡2ï¼š$P_2(X)$      0.2      0.8      ä¸åŒçš„ä¿¡æºå‘ä¸åŒçš„æ¶ˆæ¯å¸¦æ¥çš„è‡ªä¿¡æ¯ä¸åŒï¼Œå› ä¸ºä»–ä»¬å‘ä¿¡æ¯çš„æ¦‚ç‡ä¸ä¸€æ ·ã€‚ï¼ˆç›¸åŒäº‹ä»¶åœ¨ä¸åŒæ¦‚ç‡åˆ†å¸ƒä¸‹å‘ç”Ÿï¼Œå¸¦æ¥çš„è‡ªä¿¡æ¯ä¸åŒï¼Œå› ä¸ºäº‹ä»¶åœ¨ä¸åŒæ¦‚ç‡åˆ†å¸ƒä¸‹çš„æ¦‚ç‡ä¸ä¸€æ ·ï¼‰KLæ•£åº¦å°±æ˜¯æè¿°è¿™ç§å·®å¼‚ï¼Œè¡¡é‡ä¸¤ä¸ªä¿¡æºå‘ä¿¡å·å¸¦æ¥ä¿¡æ¯å·®çš„æœŸæœ›ï¼Œä¹Ÿæ˜¯ä¸¤ä¸ªæ¦‚ç‡åˆ†å¸ƒçš„å·®å¼‚ç¨‹åº¦å‡å¦‚çŸ¥é“è¿™ä¸ªè¡¨ï¼Œé‚£ä¹ˆä¿¡æº1å‘$a_1$ä¿¡å·/$a_1$äº‹ä»¶å‘ç”Ÿï¼Œå¸¦æ¥çš„ä¿¡æ¯/æ¶ˆé™¤çš„ä¸ç¡®å®šæ€§/è‡ªä¿¡æ¯æ˜¯$-\\log P_1(a_1)$ï¼Œè€Œä¿¡æº2å‘$a_1$ä¿¡æ¯ï¼Œè‡ªä¿¡æ¯æ˜¯$-\\log P_2(a_1)$            KLæ•£åº¦ï¼š$D_{KL}(P      Â       Q) = \\mathbb{E}{x\\sim P}\\left[-\\log Q(x) - \\left(-\\log P(x)\\right) \\right] = \\mathbb{E}{x\\sim P} \\left[\\log\\frac{P(x)}{Q(x)}\\right]$                  ä¸Šè¿°ä¾‹å­ä¸­ï¼š$D_{KL}(P_1      Â       P_2) = P_1(a_1)\\cdot \\log \\frac{P(a_1)}{Q(a_1)} + P_1(a_2)\\cdot \\log \\frac{P(a_2)}{Q(a_2)}$                  åœ¨MLä¸­ï¼Œä¸€èˆ¬$D_{KL}(P      Â       Q)$æ˜¯ä»¥$P$åˆ†å¸ƒä¸ºçœŸå®åˆ†å¸ƒ/ç›®æ ‡åˆ†å¸ƒï¼Œè¡¡é‡$Q$åˆ†å¸ƒç¦»$P$åˆ†å¸ƒå·®å¤šå°‘ã€‚äº‹ä»¶å‘ç”Ÿçš„æ¦‚ç‡æ˜¯æŒ‰$P$å‘ç”Ÿçš„ï¼Œæ‰€ä»¥ä»¥$P$åˆ†å¸ƒçš„æ¦‚ç‡ä½œä¸ºæ±‚æœŸæœ›çš„åˆ†å¸ƒã€‚åœ¨çœŸå®åˆ†å¸ƒ/ç›®æ ‡åˆ†å¸ƒ$P$ä¸‹ï¼Œäº‹ä»¶æŒ‰è¿™æ ·çš„åˆ†å¸ƒ$P$å‘ç”Ÿï¼Œå‘ç”Ÿåä¼šå¸¦æ¥xxxçš„è‡ªä¿¡æ¯ï¼Œè€Œ$Q$åˆ™å¸¦æ¥äº†xxxçš„è‡ªä¿¡æ¯ï¼Œæ‰€ä»¥æ˜¯ç›¸å‡ç„¶åå¯¹$P$æ±‚åˆ†å¸ƒ                  KLæ•£åº¦è™½ç„¶æ˜¯ä»£è¡¨å·®å¼‚ï¼Œä½†ä¸æ˜¯çœŸæ­£æ„ä¹‰ä¸Šçš„è·ç¦»ï¼Œå› ä¸ºå®ƒä¸æ˜¯å¯¹ç§°çš„ï¼š$D_{KL}(P      Â       Q)\\ne D_{KL}(Q      Â       P)$      åœ¨MLä¸­ä¼˜åŒ–æ—¶ï¼Œæ˜¯ä»¥æœ€å°åŒ–KLæ•£åº¦ä½œä¸ºä¼˜åŒ–ç›®æ ‡ï¼Œè®©æ¨¡å‹è¾“å‡ºçš„$Q$åˆ†å¸ƒæ¥è¿‘ç›®æ ‡åˆ†å¸ƒ$P$ã€‚é‚£ä¹ˆå…¶å®å¯ä»¥å‘ç°ï¼Œå¼å­ä¸­çš„ä¸€éƒ¨åˆ†æ˜¯ä¸ç”¨ä¼˜åŒ–çš„ï¼Œæ‰€ä»¥å¼•å‡ºäº†äº¤å‰ç†µï¼š$H(P,Q)=-\\mathbb{E}_{x\\sim P}\\log Q(x)$ã€‚â€œæœ€å°åŒ–KLæ•£åº¦â€ç­‰åŒäºâ€œæœ€å°åŒ–äº¤å‰ç†µâ€AlgebraLearning resources  Linear Algebra Done Right (Book).  Videos by Gilbert Strang (reposted on bilibili).  The Matrix Cookbook (Book).Inverse  A square matrix $A$ is invertible  = $A$ has n pivots  = $A$ is not singular  = the columns/rows of $A$ are independent  = the columns/rows are linearly independent  = elimination can be completed: $PA=LDU$, with all n pivots  = the nullspace of these vectors are ${\\mathbf 0}$  = the determinant of $A$ is not 0  = the rank of $A$ is n  = 0 is not an eigenvalue of $A$  = $A^TA$ is positive definiteRankrank of a matrix = the number of independent columns/rows of this matrix = the number of pivots of this matrixæ–¹ç¨‹ç»„æ±‚è§£ -&gt; å†™æˆçŸ©é˜µå½¢å¼ -&gt; çŸ¥é“ä»€ä¹ˆæ˜¯çº¿æ€§ç»„åˆ -&gt; çŸ¥é“ä»€ä¹ˆæ˜¯ç”Ÿæˆå­ç©ºé—´ -&gt; çŸ©é˜µçš„ç§©æ˜¯ç”Ÿæˆå­ç©ºé—´çš„ç»´åº¦æ–¹ç¨‹ç»„æ±‚è§£ -&gt; å†™æˆçŸ©é˜µå½¢å¼ï¼š\\(a_{11}x_1+a_{12}x_2+a_{13}x_3=b_1\\\\ a_{21}x_1+a_{22}x_2+a_{23}x_3=b_2\\\\ a_{31}x_1+a_{32}x_2+a_{33}x_3=b_3\\)å¯ä»¥å†™æˆ$\\mathbf{A}\\mathbf{x}=\\mathbf{b}$ï¼ŒçŸ©é˜µå’Œå‘é‡çš„ä¹˜æ³•å°±æ˜¯è¿™ä¹ˆå®šä¹‰çš„ï¼Œæ–¹ä¾¿è¡¨è¾¾çŸ¥é“ä»€ä¹ˆæ˜¯çº¿æ€§ç»„åˆ -&gt; çŸ¥é“ä»€ä¹ˆæ˜¯ç”Ÿæˆå­ç©ºé—´ï¼š\\[\\begin{bmatrix} a_{11} \\\\ a_{21} \\\\ a_{31}  \\end{bmatrix} x_1 + \\begin{bmatrix} a_{12} \\\\ a_{22} \\\\ a_{32}  \\end{bmatrix} x_2 + \\begin{bmatrix} a_{13} \\\\ a_{23} \\\\ a_{33}  \\end{bmatrix} x_3  = \\begin{bmatrix} b_{1} \\\\ b_{2} \\\\ b_{3}  \\end{bmatrix}\\]çŸ©é˜µæ‹†æˆåˆ—å‘é‡ï¼Œä¸€ä¸ªåˆ—å‘é‡ä»£è¡¨ä¸€ä¸ªæ–¹å‘ï¼Œæ¯ä¸ªåˆ—å‘é‡çš„ä¹˜ç§¯ä»£è¡¨æ²¿ç€è¿™ä¸ªåˆ—å‘é‡çš„æ–¹å‘èµ°å¤šè¿œã€‚è§£æ–¹ç¨‹ç»„çš„é—®é¢˜å˜æˆï¼šåœ¨ç©ºé—´ä¸­ç»™å‡ºå‡ ä¸ªæ–¹å‘ï¼Œä¹Ÿç»™å‡ºäº†ç»ˆç‚¹å’Œèµ·ç‚¹ï¼ˆç›¸å¯¹ä½ç½®ï¼‰ï¼Œè¦åˆ†åˆ«æ²¿ç€è¿™äº›æ–¹å‘èµ°å¤šè¿œï¼Œå¯ä»¥ä»èµ·ç‚¹åˆ°è¾¾ç»ˆç‚¹ï¼Ÿçœ‹å‡ ä¸ªæƒ…å†µï¼Œæ¬ å®šæ–¹ç¨‹ç»„çš„ç³»æ•°çŸ©é˜µä¸ºçŸ®çŸ©é˜µï¼Œå³ç³»æ•°çŸ©é˜µ$\\mathbf{A}\\in \\mathbb{R}^{m\\times n}$çš„è¡Œæ•°$m$å°äºåˆ—æ•°$n$ï¼Œæ–¹ç¨‹ç»„æœ‰æ— ç©·å¤šè§£ï¼Œæ¯”å¦‚ï¼š\\[\\begin{bmatrix} a_{11} \\\\ a_{21} \\end{bmatrix} x_1 + \\begin{bmatrix} a_{12} \\\\ a_{22} \\end{bmatrix} x_2 + \\begin{bmatrix} a_{13} \\\\ a_{23}  \\end{bmatrix} x_3  = \\begin{bmatrix} b_{1} \\\\ b_{2}  \\end{bmatrix}\\]å¦‚æœè¿™ä¸‰ä¸ªåˆ—å‘é‡äº’ç›¸ä¸æˆæ¯”ä¾‹ï¼Œé‚£ä¹ˆä»–ä»¬æ˜¯çº¿æ€§æ— å…³çš„ï¼Œç”±äºæ¯ä¸ªåˆ—å‘é‡æ˜¯ä¸¤ä¸ªå…ƒç´ ï¼Œå¯ä»¥åœ¨ä¸€ä¸ªå¹³é¢ä¸­ç”»å‡ºæ¥ã€‚æˆ‘ä»¬çŸ¥é“åœ¨å¹³é¢ä¸­ä»»æ„ä¸¤ä¸ªä¸å…±çº¿çš„å‘é‡çš„çº¿æ€§ç»„åˆï¼Œå°±å¯ä»¥è¡¨ç¤ºè¿™ä¸ªå¹³é¢ä¸Šçš„ä»»æ„ä¸€ä¸ªå‘é‡äº†ï¼Œé‚£ä¹ˆç°åœ¨æœ‰3ä¸ªï¼Œå¤šäº†ï¼Œæƒ³æ€ä¹ˆèµ°å°±æ€ä¹ˆèµ°äº†å¦ä¸€ç§æƒ…å†µï¼Œç¨€ç–çŸ©é˜µçš„åˆ—æ•°å°äºè¡Œæ•°ï¼Œæ¯”å¦‚ï¼š\\[\\begin{bmatrix} a_{11} \\\\ a_{21} \\\\ a_{31}  \\end{bmatrix} x_1 + \\begin{bmatrix} a_{12} \\\\ a_{22} \\\\ a_{32}  \\end{bmatrix} x_2  = \\begin{bmatrix} b_{1} \\\\ b_{2} \\\\ b_{3}  \\end{bmatrix}\\]ç»™çš„æ–¹å‘è¿˜æœ‰ç›®æ ‡ç‚¹éƒ½æ˜¯åœ¨ä¸‰ç»´ç©ºé—´ä¸­çš„ï¼Œä½†æ˜¯åªç»™äº†ä¸¤ä¸ªæ–¹å‘ï¼Œæ€ä¹ˆç»„åˆé‚£ä¹Ÿåªèƒ½æ˜¯åœ¨ä¸€ä¸ªå¹³é¢é‡ŒæŠ˜è…¾ï¼Œå¦‚æœæ°å¥½$b$è¿™ä¸ªç›®æ ‡ç‚¹åœ¨è¿™ä¸ªå¹³é¢ä¸Šï¼Œé‚£ä¹ˆå¯ä»¥å®Œæˆä»»åŠ¡ï¼Œå¦åˆ™æ˜¯ä¸èƒ½å®Œæˆçš„ï¼Œæ–¹ç¨‹ç»„ä¹Ÿå°±æ— è§£æ¯ä¸€ç§$(x_1,x_2)$çš„å–å€¼éƒ½å¯¹åº”äº†ä¸€ç§$\\mathbf{A}$çš„åˆ—å‘é‡çš„çº¿æ€§ç»„åˆ (linear combination)ï¼Œçº¿æ€§ç»„åˆæ˜¯å¯¹å‘é‡çš„æ“ä½œï¼ŒçŸ©é˜µçš„åˆ—å‘é‡çš„çº¿æ€§ç»„åˆ = è¿™ä¸ªçŸ©é˜µçš„åˆ—ç©ºé—´ï¼ˆcolumn spaceï¼‰ æˆ–è€…æ˜¯è¿™ä¸ªçŸ©é˜µçš„å€¼åŸŸï¼ˆrangeï¼‰$\\mathbf{A}\\mathbf{x}=\\mathbf{b}$ï¼Œå¦‚æœ$\\mathbf{b}$ä¸åœ¨$\\mathbf{A}$çš„åˆ—ç©ºé—´é‡Œï¼Œé‚£ä¹ˆè¿™ä¸ªé—®é¢˜å°±æ— è§£äº†ã€‚æ‰€ä»¥å¦‚æœè¦æœ‰è§£ï¼Œé‚£ä¹ˆé¦–å…ˆç»´åº¦å¾—å¯¹å¾—ä¸Šï¼Œå³$\\mathbf{A}$çš„åˆ—ç©ºé—´çš„ç»´åº¦è¦å¤§äºç­‰äº$\\mathbf{b}$çš„ç»´åº¦####ä¸€ç»„å‘é‡çš„ç”Ÿæˆå­ç©ºé—´ï¼ˆspanï¼‰å°±æ˜¯å…³äºå®ƒçš„çº¿æ€§ç»„åˆèƒ½åˆ°è¾¾çš„ç‚¹ï¼Œä¸€ä¸ªçŸ©é˜µçš„åˆ—ç©ºé—´ï¼ˆåˆ—å‘é‡çš„ç”Ÿæˆå­ç©ºé—´ï¼‰çš„ç»´åº¦å°±æ˜¯ç§©ï¼ˆrankï¼‰Linear IndependencyIf $\\exists k\\in \\mathbb{R}$, s.t. $\\mathbf{x_1} = k\\cdot\\mathbf{x_2}$, then $x_1$ and $x_2$ are  linear dependent.å¦‚æœä¸€ä¸ªçŸ©é˜µä¸­æœ‰ä¸¤ä¸ªåˆ—å‘é‡æ˜¯çº¿æ€§ç›¸å…³çš„ï¼Œé‚£ä¹ˆå…¶ä¸­ä¸€ä¸ªè¢«åˆ æ‰äº†ä¹Ÿä¸ä¼šæ”¹å˜è¿™ä¸ªçŸ©é˜µçš„åˆ—ç©ºé—´EigenvalueDeterminantSingularCramerâ€™s ruleAdjugate matrixSetMapping      å•å°„ (Injective)ï¼š ä¸€ä¸ªå‡½æ•°$f: A \\to B$æ˜¯å•å°„çš„ï¼Œå½“ä¸”ä»…å½“å¯¹äº$A$ä¸­çš„ä»»æ„ä¸¤ä¸ªä¸åŒçš„å…ƒç´ $a_1$å’Œ$a_2$ï¼ˆå³$a_1 \\ne a_2$ï¼‰ï¼Œæˆ‘ä»¬éƒ½æœ‰$f(a_1) \\neq f(a_2)$ã€‚ç®€å•æ¥è¯´ï¼Œä¸åŒçš„è¾“å…¥è¢«æ˜ å°„åˆ°ä¸åŒçš„è¾“å‡ºã€‚        æ»¡å°„ (Surjective)ï¼š ä¸€ä¸ªå‡½æ•°$f: A \\to B$æ˜¯æ»¡å°„çš„ï¼Œå½“ä¸”ä»…å½“å¯¹äºé›†åˆ$B$ä¸­çš„æ¯ä¸€ä¸ªå…ƒç´ $b$ï¼Œéƒ½å­˜åœ¨é›†åˆ$A$ä¸­çš„æŸä¸ªå…ƒç´ $a$ä½¿å¾—$f(a) = b$ã€‚ç®€å•æ¥è¯´ï¼Œ$B$ä¸­çš„æ¯ä¸€ä¸ªå…ƒç´ éƒ½æ˜¯$f$çš„æŸä¸ªè¾“å‡ºã€‚        åŒå°„ (Bijective)ï¼š ä¸€ä¸ªå‡½æ•°$f: A \\to B$æ˜¯åŒå°„çš„ï¼Œå½“ä¸”ä»…å½“å®ƒæ—¢æ˜¯å•å°„çš„åˆæ˜¯æ»¡å°„çš„ã€‚è¿™æ„å‘³ç€$f$ä¸º$A$ä¸­çš„æ¯ä¸€ä¸ªå…ƒç´ ä¸$B$ä¸­çš„æŸä¸€ä¸ªå…ƒç´ å»ºç«‹äº†ä¸€ä¸€å¯¹åº”çš„å…³ç³»ï¼Œè€Œä¸”æ¯ä¸€ä¸ªå…ƒç´ åœ¨è¿™ç§å¯¹åº”ä¸­éƒ½æ˜¯ç‹¬ä¸€æ— äºŒçš„ã€‚å¦‚æœå­˜åœ¨è¿™æ ·çš„åŒå°„å‡½æ•°ï¼Œåˆ™æˆ‘ä»¬å¯ä»¥è¯´é›†åˆ$A$å’Œ$B$å…·æœ‰ç›¸åŒçš„åŠ¿ï¼ˆcardinalityï¼‰ã€‚  Isomorphic åŒæ„æ˜ å°„é¦–å…ˆå®šä¹‰â€œåŒæ„æ˜ å°„â€æ˜¯ä¸€ä¸ªåŒå°„ï¼Œå®ƒè¿˜ä¿æŒæŸäº›åŸºæœ¬çš„ä»£æ•°æˆ–å…¶ä»–ç»“æ„ç‰¹æ€§ã€‚æˆ‘ä»¬å¯ä»¥æ ¹æ®å…·ä½“çš„ç»“æ„æ¥å®šä¹‰å®ƒã€‚è¿™æ ·çš„æ˜ å°„é€šå¸¸è¢«è¡¨ç¤ºä¸º $f: A \\rightarrow B$ï¼Œå…¶ä¸­ A å’Œ B æ˜¯æˆ‘ä»¬æ‰€è€ƒè™‘çš„é›†åˆã€‚åœ¨ä¸åŒçš„æ•°å­¦ç»“æ„ä¸­çš„åŒæ„  ç¾¤åŒæ„ï¼šåœ¨ç¾¤è®ºä¸­ï¼Œå¦‚æœå­˜åœ¨ä¸€ä¸ªæ˜ å°„ $f: G \\rightarrow H$ ä½¿å¾—å¯¹æ‰€æœ‰çš„ $g_1, g_2 \\in G$ï¼Œéƒ½æœ‰ $f(g_1 \\cdot g_2) = f(g_1) \\cdot f(g_2)$ï¼Œå…¶ä¸­â€œ$\\cdot$â€è¡¨ç¤ºç¾¤çš„æ“ä½œï¼Œåˆ™ç§°$G$å’Œ$H$æ˜¯åŒæ„çš„ã€‚  ç¯åŒæ„ï¼šåœ¨ç¯è®ºä¸­ï¼Œä¸€ä¸ªç¯åŒæ„æ˜¯ä¸€ä¸ªä¿æŒåŠ æ³•å’Œä¹˜æ³•æ“ä½œçš„åŒå°„ã€‚å³å¦‚æœå­˜åœ¨ä¸€ä¸ªæ˜ å°„ $f: R \\rightarrow S$ ä½¿å¾—å¯¹æ‰€æœ‰çš„ $r_1, r_2 \\in R$ï¼Œéƒ½æœ‰ $f(r_1 + r_2) = f(r_1) + f(r_2)$ å’Œ $f(r_1 \\cdot r_2) = f(r_1) \\cdot f(r_2)$ï¼Œåˆ™ç§° R å’Œ S æ˜¯åŒæ„çš„ã€‚  å‘é‡ç©ºé—´åŒæ„ï¼šåœ¨çº¿æ€§ä»£æ•°ä¸­ï¼Œä¸€ä¸ªçº¿æ€§å˜æ¢å¯ä»¥æ˜¯ä¸€ä¸ªå‘é‡ç©ºé—´åˆ°å¦ä¸€ä¸ªå‘é‡ç©ºé—´çš„åŒæ„ï¼Œå¦‚æœå®ƒæ˜¯åŒå°„ä¸”ä¿æŒå‘é‡åŠ æ³•å’Œæ ‡é‡ä¹˜æ³•ã€‚æ€§è´¨å’Œç»“æœ  å”¯ä¸€æ€§ï¼šå¦‚æœä¸¤ä¸ªç»“æ„æ˜¯åŒæ„çš„ï¼Œé‚£ä¹ˆå®ƒä»¬åœ¨ç»“æ„ä¸Šæ˜¯â€œç›¸åŒâ€çš„ï¼Œè¿™æ„å‘³ç€å®ƒä»¬çš„æ€§è´¨æ˜¯ç›¸åŒçš„ã€‚å› æ­¤ï¼Œä¸€ä¸ªç»“æ„ä¸­çš„å®šç†ä¹Ÿå°†é€‚ç”¨äºå¦ä¸€ä¸ªç»“æ„ã€‚  åå¯¹ç§°æ€§å’Œä¼ é€’æ€§ï¼šåŒæ„å…·æœ‰åå¯¹ç§°æ€§å’Œä¼ é€’æ€§ã€‚è¿™æ„å‘³ç€å¦‚æœ $A$ æ˜¯ä¸ $B$ åŒæ„çš„ï¼Œé‚£ä¹ˆ $B$ ä¹Ÿæ˜¯ä¸ $A$ åŒæ„çš„ï¼›å¦‚æœ $A$ æ˜¯ä¸ $B$ åŒæ„çš„ï¼Œå¹¶ä¸” $B$ æ˜¯ä¸ $C$ åŒæ„çš„ï¼Œé‚£ä¹ˆ $A$ æ˜¯ä¸ $C$ åŒæ„çš„ã€‚ä¾‹å­  æ•´æ•°ä¸å¶æ•°çš„é›†åˆï¼šæˆ‘ä»¬å¯ä»¥æ„å»ºä¸€ä¸ªæ˜ å°„ $f: \\mathbb{Z} \\rightarrow 2\\mathbb{Z}$ï¼Œé€šè¿‡ $f(n) = 2n$ æ¥å®šä¹‰ã€‚è¿™é‡Œ $2\\mathbb{Z}$ æ˜¯å¶æ•°çš„é›†åˆã€‚è¿™ä¸ªæ˜ å°„æ˜¯ä¸€ä¸ªåŒæ„ï¼Œå› ä¸ºå®ƒæ˜¯åŒå°„ä¸”ä¿æŒåŠ æ³•è¿ç®—ã€‚TopologyLogicCalculusCombinatorics  Permutation: Permutation refers to the arrangement of a certain number of elements from a set in a specific order. The number of permutations of $k$ elements taken from a set of $n$ elements is denoted as $P(n, k)$, and it is calculated using the formula: \\(P(n, k) = \\frac{n!}{(n-k)!}\\)  Combination: Combination refers to the selection of a certain number of elements from a set without considering the order. The number of combinations of $k$ elements taken from a set of $n$ elements is denoted as $C(n, k)$, and it is calculated using the formula: \\(C(n, k) = \\binom{n}{k} = \\frac{n!}{k!(n-k)!}\\)"
  },
  
  {
    "title": "English Toolbox",
    "url": "/posts/English-Toolbox/",
    "categories": "Misc Notes",
    "tags": "tech, English, toolbox",
    "date": "2023-04-06 18:40:01 +0000",
    





    
    "snippet": "  This note will be consistently updated.5 Principles and 7 Actions  This part is summarized from this talk.Principles1: Focus on langurage content that is relevant to youInformation that helps ach...",
    "content": "  This note will be consistently updated.5 Principles and 7 Actions  This part is summarized from this talk.Principles1: Focus on langurage content that is relevant to youInformation that helps achieve personal goals has relevance. If the content is relevant to you, then you will pay attention to it.This brings us to tools. We master tools by using tools, and we learn tools the fastest when they are relevant to us (e.g. a crisis). Because it was relevant, meaningful and important.2: Use your new language as a tool to communicate from day 1As a kid does.3: When you first understand the message, you will unconsciously acquire the languageComprehension is key. Comprehensible input approach is better than grammar focus language teaching.Language learning is not about accumulating a lot of knowldge. Itâ€™s about physiological training.Because we have filters in our brain that filter in the sounds that we are familiar with and they filter out the sounds of languages that weâ€™re not. And if you canâ€™t hear it, you wonâ€™t understand it. If you canâ€™t understand it, youâ€™re not going to learn it.4: Talking takes musclePractice coordinating your face muscle in a way that you make sounds that other people will understand. When you face hurts, you are doing it right.5: Psycho-physiological state mattersIf you are sad, angry, worried, upset, you are not going to learn. If you are happy, relaxed, in an Alpha brain state, curious, you are going to learn really quickly.You must learn to tolerate ambiguity.If you are one of those people who needs to understand 100 percent every word you are hearing, you will go nuts. Because you will be incredibly upset all the time, because you are not perfect.If you are comfortable with getting some, not getting some, just pay attention to what you do understand, you will be fine, relaxed, and you will be learning quickly.Actions1: Listen a lotIt is brain soaking.you put yourself in a context where you are hearing tons and tons and tons of a language, and it does not matter if you understand it or not. You are listening to the rhythms, to the patterns that repeat, you are lisening to things that stand out.2: Focus on getting the meaning first (before the words)E.g. by observing the body language.3: Start mixingIf you have got 10 verbs, 10 nouns and 10 adjectives, you can say 1000 different things.Language is a creative process.4: Focus on the coreAny language is high frequency content. In English 1000 words covers 85 percent of anything you are ever going to say in daily communication. And 3000 words gives you 98 percent of anything you are going to say in daily conversation. You got 3000 words, you are speaking the language. The rest is icing on the cake.Start with your tool box:  Week 1          What is this?      How do you say?      I donâ€™t understandâ€¦      Repeat that pelase      what does that mean?        Week 2-3: Pronounds, common verbs, adjectives. Communicating like a baby          you      that      me      give      hot        Week 4: Glue words          And      But      Even though      therefore      5: Get yourself a language parentWhen a child is speaking, it will be using simple words, simple combinations. Sometimes quite strange, other people from outside the family donâ€™t understand it. But the parents do.And so the kid has a safe environment, gets confidence. The parents talk to the children with body language, and with simple language they know the child understands. So you have a comprehensible input environment that is safe.So you get yourself a language parent, who is somebody interested in you as a person who will communicate with you, essentially as an equal but pay attention to hel you understand the language. Spouses are not very good at this.Language parent rules:  Works to understand what you are saying.  Does not correct mistakes.  Confirms understanding by using correct language.  Use words the learner knows.6: Copy the faceYou got to get the muscles working right, so you can sound in a way that people will understand you. There is a couple of things you do.You hear how it feels, and feel how it sounds, which means you have a feedback loop operating in your face. If you can look at a native speaker, and just observe how they use their face, and let your unconscious mind absorb the rules, then you are going to be able to pick it up.7: Direct connect to mental imagesDonâ€™t translate it to your familiar language. Everything you know is an image inside your mind, its feelings. You go into that imagery and all of that memory and you come out with another pathway. Same box, different path.Latin expressions (Loanwords)(Organized from this note.)  ad hoc          For this; Formed or done for a particular purpose only.      An ad hoc committee was set up to oversee the matter.        et cetera, etcetera, etc., et cet, &amp;c., or &amp;c          And the rest; And so on; And more.      We urgently need to buy medical equipment, drugs, et cetera.      Et cetera is used when listing groups of nouns and adjectives.        et al. or et alia          And others.      Mike and Carol had six kids named Greg, Marcia, Peter, et al.      Et al. is used to reference other people not specifically named in your list.        per          For each.      This petrol station charges $5.00 per gallon.        per annum or p.a.          For each year.      The population is increasing by about 2% per annum.        per capita          For each person.      The countryâ€™s annual income is $5000 per capita.        per se          In itself/themselves; Intrinsically.      These facts per se are not important.        re          About; Concerning; Regarding.      Re: Unpaid Invoice.        status quo          Existing state of affairs.      Monarchies naturally wish to maintain the status quo.        versus, vs or v.          Against.      What are the enefits of organic versus inorganic foods?        vice versa          The other way round.      My phone serves me, and not vice versa.      GrammarGrammarBook.comTOEFLResources  ETS  KMF  Test ResourcesOverview  Sections and time scheduleReadingSpeaking  16 min.  4 questions.          Independent Speaking      Integrated Speaking                  Campus Announcement          General to Specific          Academic Lecture                    Tips  Q1          15s prep. 45s speaking.        Q2          Q2 is about a campus envent. The reading material is a proposal or an announcement. The listening material is usually a conversation about the reading material.      30s prep. 60s speaking.      Reading summary in around 20s: Summarize the subject in a single sentence, followed by a separate sentence for each key point.      Speak while wathcing the timer.        Q3          Q3 requires me to introduce a scientific concept. The reading material is a brief introduction exerpt from the textbook. The listening material is a part of a lecture containing explanation with examples.      30s prep. 60s speaking.      Independent SpeakingTemplate (15s prep, 45s answer)Well, personally, I agree/disagree with the statement that [just read the statement].I feel this way for several reasons.First of all, â€¦For example, â€¦ (and a personal example).Additionally, â€¦To be more specific, â€¦ (and a few more details).Arguments  Expand horizons          I think it can broaden my horizons because      I think it can enrich citizenâ€™s cultrual life              Learn          I can learn practical skills, in addition to academic knowledge      I can learn lots of vital skills and knowledge      e.g., presentation skills, communication skills, organization skills        Money, finance, economy          Because it can help me become financially independent      I can save money by doing so.      It can generate more revenues in order to enhance cityâ€™s image        Relax, enjoy, have fun          I love sports games and outdoor activities. Because they are fun.      I can make new friends, breathe in the fresh air      I can have a good balance between work and leisure.      It can improve quality of life. It can reduce stress.        Emotion          A good friend/mentor can tell me how to distinguish right from wrong.      A good friend will help me establish my value and belief.      I can have lots of emotional support from my friends. They stand by me. They are always there for me when I need help.        Health                  Efficiency  Communication          It can help me make friends with people from different backgrounds.        Trait, characteristic  InterestIntegrated WritingTemplateAâ€™s points are denoted by $X_1, X_2, X_3$,And Bâ€™s are denoted by $Y_1, Y_2, Y_3$.Tips:  Identify key points.          Focus on the lecture.      Summarize the passage.        Remember that all the points should be paraphrased.          Vocabulary.      Use synonyms.        225 words.  20 min.In the passage, the author mentions  3 benefits coming from posing high taxes on cigarettes and unhealthy food.  3 potential solutions to the increasing salinity of the Salton Sea in California.However, in the lecture, the speaker casts doubts on them.Initially, the author believes that $X_1$. In contrast, the speaker argues that $Y_1$. (Then focus on the speakerâ€™s argument.)Furthermore, the author claims that $X_2$. However, the speaker disproves it by pointing out that $Y_2$. (Then focus on the speakerâ€™s argument.)Additionally, the author puts forward that $X_3$. Instead, the speaker challenges the authorâ€™s opinion and offers a different perspective that $Y_3$. (Then focus on the speakerâ€™s argument.)Academic discussion10 minutes."
  },
  
  {
    "title": "Certification of Rank",
    "url": "/posts/Certification-Rank/",
    "categories": "Misc Notes",
    "tags": "misc note, certification of rank",
    "date": "2023-04-04 12:00:00 +0000",
    





    
    "snippet": "",
    "content": ""
  },
  
  {
    "title": "Swinging Search and Crawling Control",
    "url": "/posts/SSCC/",
    "categories": "Robotics",
    "tags": "tech, my paper, redundant manipulator, reinforcement learning",
    "date": "2023-04-03 12:00:01 +0000",
    





    
    "snippet": "  Please be aware that the video accompanying this article may take some time to load, depending on the speed of your internet connection to GitHub.A snake-inspired path planning algorithm based on...",
    "content": "  Please be aware that the video accompanying this article may take some time to load, depending on the speed of your internet connection to GitHub.A snake-inspired path planning algorithm based on reinforcement learning and self-motion for hyper-redundant manipulatorsSSCC = Swinging Search (self-motion for path planning) and Crawling Control (crawling into the pipe like a snake)  Self-motion is a special kind of motion for redundant manipulators. Given a fixed base and an end target, there can be more than one possible solution configuration due to its redundancy. Searching configurations (for a collision-free path) within these constraints (fixing both ends of a manipulator) can be likened to a rope swinging in the air.VideosCrawling Control: a 16 DoF manipulator crawling into a simple pipe.Sources  PDF  Videos  Codes  Self-Motion demo"
  },
  
  {
    "title": "RHex-T3 (A Mobile Robot, with Hybrid Leg Design)",
    "url": "/posts/RHex-T3/",
    "categories": "Robotics",
    "tags": "tech, my paper, robotic mechanism design, mobile robot, kinematics",
    "date": "2023-04-03 12:00:00 +0000",
    





    
    "snippet": "  Please be aware that the videos accompanying this article may take some time to load, depending on the speed of your internet connection to GitHub.Innovative design and simulation of a transforma...",
    "content": "  Please be aware that the videos accompanying this article may take some time to load, depending on the speed of your internet connection to GitHub.Innovative design and simulation of a transformable robot with flexibility and versatility, RHex-T3RHex-T3 = RHex - To The Top (for it can climb ladders)  RHex is a highly successful robot with excellent properties. The robot has six legs that are shaped like semicircles, with a motor installed at the end of each arc. We have redesigned its legs to perform a wide range of functions.Sources  PDF  VideosA substantial amount of code I wrote (kinematics, simulation, physical control) cannot be made public, because the lab I used to work will continue to conduct related research.VideosThis work is self-explanatory, as the videos that follow are easy to understand.OverviewExploded ViewTransmission SystemWorkspaceWorkspaceHook ModeLeg ModeRHex ModeMode SwitchingWheel Mode"
  },
  
  {
    "title": "Markdown Syntax",
    "url": "/posts/markdown-template/",
    "categories": "Efficiency, Tools",
    "tags": "tech, efficiency, tools, markdown",
    "date": "2023-04-03 07:48:44 +0000",
    





    
    "snippet": "  Adapted from this postPrompts  green tip  blue info  yellow warning  red danger&gt; green tip{: .prompt-tip }&gt; blue info{: .prompt-info }&gt; yellow warning{: .prompt-warning }&gt; red danger{...",
    "content": "  Adapted from this postPrompts  green tip  blue info  yellow warning  red danger&gt; green tip{: .prompt-tip }&gt; blue info{: .prompt-info }&gt; yellow warning{: .prompt-warning }&gt; red danger{: .prompt-danger }My common usage  This note will be consistently updated.  This section is generated by ChatGPT 4.  This section is adapted from the answer generated by ChatGPT 4.  The following part has not been finished yet.Titles# H1## H2### H3ListOrdered List1. 1112. 2223. 333Unordered ListThe indentation determines the structure, and the choice of * or - or + is insignificant.- 111    - 222        - 333TODO ListThe indentation determines the structure, and the choice of * or - or + is insignificant.- [ ] Job    - [x] Step 1    - [x] Step 2    - [ ] Step 3DescriptionSun: the star around which the earth orbitsEmphasis*Italian***Bold**Hyper Link[name](url)Block Quote&gt; blahblahTable            Company      Contact      Country                  Alfreds Futterkiste      Maria Anders      Germany              Island Trading      Helen Bennett      UK              Magazzini Alimentari Riuniti      Giovanni Rovelli      Italy      FootnoteClick the hook will locate the footnote1, and here is another footnote2.Filepath`/path/to/the/file.extend`{: .filepath}.Mathematics(front matter: math: True)\\(\\sum_{n=1}^\\infty 1/n^2 = \\frac{\\pi^2}{6}\\)$$ \\sum_{n=1}^\\infty 1/n^2 = \\frac{\\pi^2}{6} $$Image &amp; VideoDefault (with caption)captionLeft AlignedFloat to LeftPraesent maximus aliquam sapien. Sed vel neque in dolor pulvinar auctor. Maecenas pharetra, sem sit amet interdum posuere, tellus lacus eleifend magna, ac lobortis felis ipsum id sapien. Proin ornare rutrum metus, ac convallis diam volutpat sit amet. Phasellus volutpat, elit sit amet tincidunt mollis, felis mi scelerisque mauris, ut facilisis leo magna accumsan sapien. In rutrum vehicula nisl eget tempor. Nullam maximus ullamcorper libero non maximus. Integer ultricies velit id convallis varius. Praesent eu nisl eu urna finibus ultrices id nec ex. Mauris ac mattis quam. Fusce aliquam est nec sapien bibendum, vitae malesuada ligula condimentum.Float to RightPraesent maximus aliquam sapien. Sed vel neque in dolor pulvinar auctor. Maecenas pharetra, sem sit amet interdum posuere, tellus lacus eleifend magna, ac lobortis felis ipsum id sapien. Proin ornare rutrum metus, ac convallis diam volutpat sit amet. Phasellus volutpat, elit sit amet tincidunt mollis, felis mi scelerisque mauris, ut facilisis leo magna accumsan sapien. In rutrum vehicula nisl eget tempor. Nullam maximus ullamcorper libero non maximus. Integer ultricies velit id convallis varius. Praesent eu nisl eu urna finibus ultrices id nec ex. Mauris ac mattis quam. Fusce aliquam est nec sapien bibendum, vitae malesuada ligula condimentum.Video            The footnote sourceÂ &#8617;              The 2nd footnote sourceÂ &#8617;      "
  },
  
  {
    "title": "Writing Buffer",
    "url": "/posts/Writing-Buffer/",
    "categories": "Misc Notes",
    "tags": "misc note, writing buffer",
    "date": "2023-04-01 18:40:00 +0000",
    





    
    "snippet": "  This post is used for storing temporary content, which will be classified later. I donâ€™t want my thought process to be interrupted by format-related issues while composing.Eigenvalueå¦ä¸€ä¸ªä»‹ç»IDçš„æ€è·¯bat...",
    "content": "  This post is used for storing temporary content, which will be classified later. I donâ€™t want my thought process to be interrupted by format-related issues while composing.Eigenvalueå¦ä¸€ä¸ªä»‹ç»IDçš„æ€è·¯battle of sexesmixed strategy Nash equilibriumç”šè‡³æ¯”ä»»ä½•ä¸€ä¸ªpure strategy Nash equilibriumè¿˜å·®å¯ä»¥å¼•å…¥ä¸€ä¸ªtrusted authorityï¼Œæ‰”ç¡¬å¸ï¼Œè¿™æ ·ç»“æœå°±ä¼šå¥½å¾ˆå¤šä¸€ä¸ªè§’åº¦æ˜¯ï¼šIDå¯ä»¥è®¤ä¸ºæ˜¯è®©trusted authorityéª—äºº"
  },
  
  {
    "title": "Writing Schedule",
    "url": "/posts/Schedule/",
    "categories": "Misc Notes",
    "tags": "misc note, writing schedule",
    "date": "2023-04-01 18:40:00 +0000",
    





    
    "snippet": "Writing ScheduleGenerally, I will finish my blogs in the following order:  Zero-Determinant Strategy;  Sequential Social Dilemma  Fictitious Play and Zero-Shot Coordination;  Sequence-to-Sequence M...",
    "content": "Writing ScheduleGenerally, I will finish my blogs in the following order:  Zero-Determinant Strategy;  Sequential Social Dilemma  Fictitious Play and Zero-Shot Coordination;  Sequence-to-Sequence Models;  Details on the Analysis of Policy Gradient Methods;  MARL Basics;  Information Design: Almost finished;Depending on the circumstances, there may be additional blogs added to the queue along the way. The order of completion is not fixed.If I come up with something and have spare time, I will add content to the corresponding blog at any time.Learned a Bit but Forgotten ğŸ¥¹  Differential Privacy  Meta-Learning  Inverse RL  Generative Models          diffusion      GAN      Variational Autoencoders (VAEs)      Energy-Based Models (EBMs)        Evolutionary RL  CV basics"
  }
  
]

