---
title: Policy Distillation
date: 2023-11-16 02:40:00 +0800
categories: [Artificial Intelligence, Reinforcement Learning]
tags: [Tech, AI, RL]
math: True
---


## Introduction
[[Paper]](https://arxiv.org/abs/1511.06295): Policy Distillation

The following statements from the paper are key to understand this technique:
1. Distillation is a method to transfer knowledge from a teacher model $T$ to a student model $S$.
2. Goals:
   1. It is "used to extract the policy of a reinforcement learning agent and train a new network that performs at the expert level while being dramatically smaller and more efficient."
   2. It is "used to consolidate multiple task-specific policies into a single policy."


> The following part has not been finished yet.
{: .prompt-warning }

## Single-Game Policy Distillation


## Multi-Task Policy Distillation