---
title: Recurrent Neural Network (RNN)
date: 2023-06-15 14:40:00 +0800
categories: [Machine Learning Basics]
tags: [index]
math: True
---

## Index of an Awesome Website
> Check this awesome website: [DIVE INTO DEEP LEARNING](https://d2l.ai/chapter_preface/index.html)
{: .prompt-info }

- [RNN = Recurrent Neural Network](https://d2l.ai/chapter_recurrent-neural-networks/rnn.html)
  - [BPTT = Backpropagation Through Time](https://d2l.ai/chapter_recurrent-neural-networks/bptt.html)
- [Modern Recurrent Neural Networks (Index)](https://d2l.ai/chapter_recurrent-modern/index.html)
  - [LSTM = Long Short-Term Memory](https://d2l.ai/chapter_recurrent-modern/lstm.html)
  - [GRU = Gated Recurrent Units](https://d2l.ai/chapter_recurrent-modern/gru.html)
  - [Encoder-Decoder](https://d2l.ai/chapter_recurrent-modern/encoder-decoder.html)
  - [Seq2Seq](https://d2l.ai/chapter_recurrent-modern/seq2seq.html)
- [Attention and Transformers](https://d2l.ai/chapter_attention-mechanisms-and-transformers/index.html)


## RNN

$$
\begin{aligned}
    \begin{cases}
      h_t = f(x_t, h_{t-1}, w_h) \\
      o_t = g(h_t, w_o) \\
    \end{cases}
\end{aligned}
$$

![RNN](https://d2l.ai/_images/rnn-bptt.svg){: width="500" height="500" }
_An illustration of RNN from the book "DIVE INTO DEEL LEARNING". Boxes represent variables (not shaded) or parameters (shaded) and circles represent operators._

```python
# Adapted from https://pytorch.org/docs/stable/generated/torch.nn.RNN.html
import torch
rnn = torch.nn.RNN(10, 20, 2)   # input_size, hidden_size, num_layers
input = torch.randn(5, 3, 10)   # sequence_length, batch_size, input_size
h0 = torch.randn(2, 3, 20)      # num_layers, batch_size, hidden_size
output, hn = rnn(input, h0)
print(output.size(), hn.size())

# torch.Size([5, 3, 20]) torch.Size([2, 3, 20])
# output size: sequence_length, batch_size, hidden_size

# If parameter batch_first=True,
# then the first parameter should be the batch_size.
```

## BPTT
$$
L = \frac{1}{T}\sum\limits_{t} l(o_t,y_t)
$$  
$$
\begin{aligned}
    \frac{\partial L}{\partial w_o} =& \frac{1}{T}\sum\limits_{t} \frac{\partial l(y_t,o_t)}{\partial w_o} \\
    =& \frac{1}{T}\sum\limits_{t}
    \frac{\partial l(y_t,o_t)}{\partial o_t}\cdot
     \frac{\partial o_t}{\partial w_o}
\end{aligned}
$$  
$$
\begin{aligned}
    \frac{\partial L}{\partial w_h} =& \frac{1}{T}\sum\limits_{t} \frac{\partial l(y_t,o_t)}{\partial w_h} \\
    =& \frac{1}{T}\sum\limits_{t} \frac{\partial l(y_t,o_t)}{\partial o_t} \cdot
     \frac{\partial o_t}{\partial h_t} \cdot
      \textcolor{red}{\frac{\partial h_t}{\partial w_h}}
\end{aligned}
$$  

$$
\begin{aligned}
    \textcolor{red}{\frac{\partial h_t}{\partial w_h}} =&
    \frac{\partial f(x_t, h_{t-1}, w_h)}{\partial w_h} + \frac{\partial f(x_t, h_{t-1}, w_h)}{\partial h_{t-1}} \cdot 
    \textcolor{red}{\frac{\partial h_{t-1}}{\partial w_h}}
\end{aligned}
$$

$$
\begin{cases}
      z_0 = a_0 = 0 \\
      z_k = a_k + b_k \cdot z_{k-1} \\
\end{cases}
$$

$$
\begin{aligned}
    z_k = a_k + \sum\limits_{i=0}^{k-1}
        \left(\prod\limits_{j=i+1}^k b_j \right) \cdot a_i
\end{aligned}
$$

## LSTM
- To learn long-term dependencies (owing to vanishing and exploding gradients).
- In my understanding, there are two kinds of hidden states, $\mathrm{c}$ and $\mathrm{h}$. And $\mathrm{c}$ is renamed as the **memory cell internal state**.

Each layer is like:

$$
\begin{aligned}
  \begin{cases}
      i_t =& \mathrm{Sigmoid}\left(\mathrm{Linear}(x_t) + \mathrm{Linear}(h_{t-1})\right) \\
      f_t =& \mathrm{Sigmoid}\left(\mathrm{Linear}(x_t) + \mathrm{Linear}(h_{t-1})\right) \\
      g_t =& \mathrm{tanh}\left(\mathrm{Linear}(x_t) + \mathrm{Linear}(h_{t-1})\right) \\
      o_t =& \mathrm{Sigmoid}\left(\mathrm{Linear}(x_t) + \mathrm{Linear}(h_{t-1})\right) \\
      c_t =& f_t \odot c_{t-1} + i_t \odot g_t \\
      h_t =& o_t \odot \mathrm{tanh}(c_t)
  \end{cases}
\end{aligned}
$$

> Adapted from [the pytorch document](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html).
{: .prompt-info }

- $h_t$: the hidden state.
- $c_t$: the cell state.
- $i_t$: the input gate.
- $f_t$: the forget gate.
- $g_t$: the cell gate.
- $o_t$: the output gate.
- All values of the three gates are in the range of $(0, 1)$ because of the sigmoid function.
- $g_t$ is the vanilla part of an RNN, and it indicates the information that we currently get.
- $i_t$ controls how much we cares about the current information.
- $c$ is an addtional hidden state channel, and it also indicates the memory.
- $f_t$ controls how much we cares about the memory.
- $c_t$ and $h_t$ do not impact the curren output $o_t$.


![LSTM](https://d2l.ai/_images/lstm-3.svg){: width="500" height="500" }
_An illustration of LSTM from the book "DIVE INTO DEEL LEARNING"._

```python
# Adapted from https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html
import torch
rnn = torch.nn.LSTM(10, 20, 2)    # input_size, hidden_size, num_layers
input = torch.randn(5, 3, 10)     # sequence_length, batch_size, input_size
h0 = torch.randn(2, 3, 20)        # num_layers, batch_size, hidden_size
c0 = torch.randn(2, 3, 20)        # num_layers, batch_size, hidden_size
output, (hn, cn) = rnn(input, (h0, c0))
print(output.size(), hn.size(), cn.size())

# torch.Size([5, 3, 20]) torch.Size([2, 3, 20]) torch.Size([2, 3, 20])
# output size: sequence_length, batch_size, hidden_size

# If parameter batch_first=True,
# then the first parameter should be the batch_size.
```

## GRU

Each layer is like:

$$
\begin{aligned}
  \begin{cases}
      r_t =& \mathrm{Sigmoid}\left(\mathrm{Linear}(x_t) + \mathrm{Linear}(h_{t-1})\right) \\
      z_t =& \mathrm{Sigmoid}\left(\mathrm{Linear}(x_t) + \mathrm{Linear}(h_{t-1})\right) \\
      n_t =& \mathrm{tanh}\left(\mathrm{Linear}(x_t) + r_t \odot \mathrm{Linear}(h_{t-1})\right) \\
      h_t =& (1-z_t)\odot n_t + z \odot h_{t-1}
  \end{cases}
\end{aligned}
$$

- $h_t$: the hidden state. It can be used as the output.
- $r_t$: the reset gate, controls how much we cares about the memory. It is a bit like the forget gate $f_t$ in LSTM
- $z_t$: the update gate, controls how much we cares about the current information. It is a bit like the input gate $i_t$ in LSTM.
- $n_t$: the candidate hidden state, or the new gate.
  - If the reset gate $r_t$ is close to $1$, then it is like the vanilla RNN.
  - If the reset gate $r_t$ is close to $0$, then the new gate $n_t$ is the result of an MLP of $x_t$.

![GRU](https://d2l.ai/_images/gru-3.svg){: width="500" height="500" }
_An illustration of GRU from the book "DIVE INTO DEEL LEARNING"._

```python
# Adapted from https://pytorch.org/docs/stable/generated/torch.nn.GRU.html
import torch
rnn = torch.nn.GRU(10, 20, 2)   # input_size, hidden_size, num_layers
input = torch.randn(5, 3, 10)   # sequence_length, batch_size, input_size
h0 = torch.randn(2, 3, 20)      # num_layers, batch_size, hidden_size
output, hn = rnn(input, h0)
print(output.size(), hn.size())

# torch.Size([5, 3, 20]) torch.Size([2, 3, 20])
# output size: sequence_length, batch_size, hidden_size

# If parameter batch_first=True,
# then the first parameter should be the batch_size.
```

## Variable Number of Inputs

- Truncation and Padding
  - [DIVE INTO DEEP LEARNING 10.5.3](https://d2l.ai/chapter_recurrent-modern/machine-translation-and-dataset.html#loading-sequences-of-fixed-length)
- Relation Network
  - [A blog](https://medium.com/@andre.holzner/learning-a-function-with-a-variable-number-of-inputs-with-pytorch-c487e35d4dba)
  - [ICLR 2017](https://arxiv.org/pdf/1702.05068.pdf)
- Embedding?